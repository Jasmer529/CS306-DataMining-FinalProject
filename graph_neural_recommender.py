"""
åŸºäºå›¾ç¥ç»ç½‘ç»œçš„æ¨èç³»ç»Ÿ
ä½¿ç”¨ç”¨æˆ·-å•†å“äºŒéƒ¨å›¾å’ŒGCNè¿›è¡Œæ¨è
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from scipy.sparse import coo_matrix
import networkx as nx
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

class GraphConvolution(nn.Module):
    """å›¾å·ç§¯å±‚"""
    
    def __init__(self, input_dim, output_dim, dropout=0.1):
        super(GraphConvolution, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, adj):
        """
        å‰å‘ä¼ æ’­
        :param x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
        :param adj: é‚»æ¥çŸ©é˜µ [num_nodes, num_nodes]
        """
        # ç‰¹å¾å˜æ¢
        h = self.linear(x)
        # å›¾å·ç§¯ï¼šèšåˆé‚»å±…ä¿¡æ¯
        output = torch.spmm(adj, h)
        output = self.dropout(output)
        return F.relu(output)

class GraphNeuralRecommender(nn.Module):
    """å›¾ç¥ç»ç½‘ç»œæ¨èç³»ç»Ÿ"""
    
    def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=32, num_layers=2, dropout=0.1):
        super(GraphNeuralRecommender, self).__init__()
        
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # ç”¨æˆ·å’Œå•†å“åµŒå…¥
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # å›¾å·ç§¯å±‚
        self.gc_layers = nn.ModuleList()
        input_dim = embedding_dim
        
        for i in range(num_layers):
            output_dim = hidden_dim if i < num_layers - 1 else embedding_dim
            self.gc_layers.append(GraphConvolution(input_dim, output_dim, dropout))
            input_dim = output_dim
        
        # é¢„æµ‹å±‚
        self.predictor = nn.Sequential(
            nn.Linear(embedding_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.xavier_uniform_(self.user_embedding.weight)
        nn.init.xavier_uniform_(self.item_embedding.weight)
    
    def forward(self, user_ids, item_ids, adj_matrix):
        """
        å‰å‘ä¼ æ’­
        :param user_ids: ç”¨æˆ·IDåˆ—è¡¨
        :param item_ids: å•†å“IDåˆ—è¡¨  
        :param adj_matrix: å›¾é‚»æ¥çŸ©é˜µ
        """
        # è·å–åˆå§‹åµŒå…¥
        user_emb = self.user_embedding.weight  # [num_users, embedding_dim]
        item_emb = self.item_embedding.weight  # [num_items, embedding_dim]
        
        # æ‹¼æ¥ç”¨æˆ·å’Œå•†å“åµŒå…¥å½¢æˆå›¾èŠ‚ç‚¹ç‰¹å¾
        node_features = torch.cat([user_emb, item_emb], dim=0)  # [num_users + num_items, embedding_dim]
        
        # å›¾å·ç§¯ä¼ æ’­
        h = node_features
        for gc_layer in self.gc_layers:
            h = gc_layer(h, adj_matrix)
        
        # åˆ†ç¦»ç”¨æˆ·å’Œå•†å“çš„æœ€ç»ˆåµŒå…¥
        final_user_emb = h[:self.num_users]  # [num_users, embedding_dim]
        final_item_emb = h[self.num_users:]  # [num_items, embedding_dim]
        
        # è·å–ç‰¹å®šç”¨æˆ·å’Œå•†å“çš„åµŒå…¥
        batch_user_emb = final_user_emb[user_ids]  # [batch_size, embedding_dim]
        batch_item_emb = final_item_emb[item_ids]  # [batch_size, embedding_dim]
        
        # ç‰¹å¾æ‹¼æ¥
        combined_features = torch.cat([batch_user_emb, batch_item_emb], dim=1)
        
        # é¢„æµ‹è¯„åˆ†
        scores = self.predictor(combined_features).squeeze()
        
        return scores, final_user_emb, final_item_emb

class GNNRecommenderSystem:
    """GNNæ¨èç³»ç»Ÿå®Œæ•´å®ç°"""
    
    def __init__(self, embedding_dim=64, hidden_dim=32, num_layers=2, learning_rate=0.001):
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.learning_rate = learning_rate
        
        self.model = None
        self.user_to_idx = {}
        self.item_to_idx = {}
        self.idx_to_user = {}
        self.idx_to_item = {}
        self.adj_matrix = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ”§ åˆå§‹åŒ–GNNæ¨èç³»ç»Ÿ (è®¾å¤‡: {self.device})")
    
    def prepare_data(self, data):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡GNNè®­ç»ƒæ•°æ®...")
        
        # åˆ›å»ºç”¨æˆ·å’Œå•†å“çš„æ˜ å°„
        unique_users = data['user_id'].unique()
        unique_items = data['item_id'].unique()
        
        self.user_to_idx = {user: idx for idx, user in enumerate(unique_users)}
        self.item_to_idx = {item: idx for idx, item in enumerate(unique_items)}
        self.idx_to_user = {idx: user for user, idx in self.user_to_idx.items()}
        self.idx_to_item = {idx: item for item, idx in self.item_to_idx.items()}
        
        self.num_users = len(unique_users)
        self.num_items = len(unique_items)
        
        print(f"   ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"   å•†å“æ•°é‡: {self.num_items}")
        
        # åˆ›å»ºé‚»æ¥çŸ©é˜µ
        self._create_adjacency_matrix(data)
        
        # å‡†å¤‡è®­ç»ƒæ ·æœ¬
        train_data = self._prepare_training_samples(data)
        
        return train_data
    
    def _create_adjacency_matrix(self, data):
        """åˆ›å»ºç”¨æˆ·-å•†å“äºŒéƒ¨å›¾çš„é‚»æ¥çŸ©é˜µ"""
        print("ğŸ•¸ï¸ æ„å»ºç”¨æˆ·-å•†å“äºŒéƒ¨å›¾...")
        
        total_nodes = self.num_users + self.num_items
        
        # åˆ›å»ºè¾¹åˆ—è¡¨
        rows, cols = [], []
        
        for _, row in data.iterrows():
            user_idx = self.user_to_idx[row['user_id']]
            item_idx = self.item_to_idx[row['item_id']] + self.num_users  # å•†å“ç´¢å¼•åç§»
            
            # æ·»åŠ ç”¨æˆ·-å•†å“è¾¹ï¼ˆæ— å‘å›¾ï¼‰
            rows.extend([user_idx, item_idx])
            cols.extend([item_idx, user_idx])
        
        # åˆ›å»ºç¨€ç–é‚»æ¥çŸ©é˜µ
        adj_coo = coo_matrix(
            (np.ones(len(rows)), (rows, cols)), 
            shape=(total_nodes, total_nodes)
        )
        
        # å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
        adj_coo = self._normalize_adjacency(adj_coo)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        indices = torch.LongTensor(np.vstack((adj_coo.row, adj_coo.col)))
        values = torch.FloatTensor(adj_coo.data)
        shape = adj_coo.shape
        
        self.adj_matrix = torch.sparse.FloatTensor(indices, values, shape).to(self.device)
        
        print(f"   å›¾èŠ‚ç‚¹æ•°: {total_nodes}")
        print(f"   å›¾è¾¹æ•°: {len(adj_coo.data)}")
    
    def _normalize_adjacency(self, adj):
        """å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ"""
        # è®¡ç®—åº¦çŸ©é˜µ
        rowsum = np.array(adj.sum(1)).flatten()
        d_inv_sqrt = np.power(rowsum, -0.5)
        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
        d_mat_inv_sqrt = coo_matrix((d_inv_sqrt, (range(len(d_inv_sqrt)), range(len(d_inv_sqrt)))), shape=adj.shape)
        
        # å¯¹ç§°å½’ä¸€åŒ–: D^(-1/2) * A * D^(-1/2)
        normalized_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)
        
        return normalized_adj.tocoo()
    
    def _prepare_training_samples(self, data):
        """å‡†å¤‡è®­ç»ƒæ ·æœ¬"""
        # æ­£æ ·æœ¬ï¼šç”¨æˆ·å®é™…äº¤äº’çš„å•†å“
        positive_samples = []
        for _, row in data.iterrows():
            user_idx = self.user_to_idx[row['user_id']]
            item_idx = self.item_to_idx[row['item_id']]
            
            # æ ¹æ®è¡Œä¸ºç±»å‹è®¾ç½®æ ‡ç­¾æƒé‡
            if row['behavior_type'] == 'buy':
                label = 1.0
            elif row['behavior_type'] == 'cart':
                label = 0.8
            elif row['behavior_type'] == 'fav':
                label = 0.6
            else:  # pv
                label = 0.4
            
            positive_samples.append((user_idx, item_idx, label))
        
        # è´Ÿæ ·æœ¬ï¼šéšæœºé€‰æ‹©ç”¨æˆ·æœªäº¤äº’çš„å•†å“
        negative_samples = []
        user_items = defaultdict(set)
        
        # è®°å½•æ¯ä¸ªç”¨æˆ·äº¤äº’è¿‡çš„å•†å“
        for _, row in data.iterrows():
            user_id = row['user_id']
            item_id = row['item_id']
            user_items[user_id].add(item_id)
        
        # ä¸ºæ¯ä¸ªæ­£æ ·æœ¬ç”Ÿæˆä¸€ä¸ªè´Ÿæ ·æœ¬
        all_items = set(self.item_to_idx.keys())
        for user_idx, item_idx, _ in positive_samples:
            user_id = self.idx_to_user[user_idx]
            uninteracted_items = all_items - user_items[user_id]
            
            if uninteracted_items:
                neg_item_id = np.random.choice(list(uninteracted_items))
                neg_item_idx = self.item_to_idx[neg_item_id]
                negative_samples.append((user_idx, neg_item_idx, 0.0))
        
        # åˆå¹¶æ­£è´Ÿæ ·æœ¬
        all_samples = positive_samples + negative_samples
        np.random.shuffle(all_samples)
        
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(all_samples)} (æ­£æ ·æœ¬: {len(positive_samples)}, è´Ÿæ ·æœ¬: {len(negative_samples)})")
        
        return all_samples
    
    def train(self, data, epochs=50, batch_size=1024, validation_split=0.1):
        """è®­ç»ƒGNNæ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒGNNæ¨¡å‹ (epochs={epochs}, batch_size={batch_size})")
        
        # å‡†å¤‡æ•°æ®
        train_samples = self.prepare_data(data)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = GraphNeuralRecommender(
            num_users=self.num_users,
            num_items=self.num_items,
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        criterion = nn.BCELoss()
        
        # æ•°æ®åˆ†å‰²
        split_idx = int(len(train_samples) * (1 - validation_split))
        train_data = train_samples[:split_idx]
        val_data = train_samples[split_idx:]
        
        losses = []
        val_losses = []
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            epoch_loss = 0.0
            num_batches = len(train_data) // batch_size + 1
            
            for i in range(0, len(train_data), batch_size):
                batch = train_data[i:i+batch_size]
                
                user_ids = torch.LongTensor([sample[0] for sample in batch]).to(self.device)
                item_ids = torch.LongTensor([sample[1] for sample in batch]).to(self.device)
                labels = torch.FloatTensor([sample[2] for sample in batch]).to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                scores, _, _ = self.model(user_ids, item_ids, self.adj_matrix)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(scores, labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_train_loss = epoch_loss / num_batches
            losses.append(avg_train_loss)
            
            # éªŒè¯é˜¶æ®µ
            if val_data:
                self.model.eval()
                val_loss = 0.0
                val_batches = 0
                
                with torch.no_grad():
                    for i in range(0, len(val_data), batch_size):
                        batch = val_data[i:i+batch_size]
                        
                        user_ids = torch.LongTensor([sample[0] for sample in batch]).to(self.device)
                        item_ids = torch.LongTensor([sample[1] for sample in batch]).to(self.device)
                        labels = torch.FloatTensor([sample[2] for sample in batch]).to(self.device)
                        
                        scores, _, _ = self.model(user_ids, item_ids, self.adj_matrix)
                        loss = criterion(scores, labels)
                        
                        val_loss += loss.item()
                        val_batches += 1
                
                avg_val_loss = val_loss / val_batches
                val_losses.append(avg_val_loss)
                
                if (epoch + 1) % 10 == 0:
                    print(f"   Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
            else:
                if (epoch + 1) % 10 == 0:
                    print(f"   Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}")
        
        print("âœ… GNNæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return losses, val_losses
    
    def recommend_for_user(self, user_id, top_k=10, exclude_seen=True):
        """ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆæ¨è"""
        if self.model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return []
        
        if user_id not in self.user_to_idx:
            print(f"âŒ ç”¨æˆ· {user_id} ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­")
            return []
        
        self.model.eval()
        
        with torch.no_grad():
            user_idx = self.user_to_idx[user_id]
            
            # ä¸ºè¯¥ç”¨æˆ·å¯¹æ‰€æœ‰å•†å“è¿›è¡Œè¯„åˆ†
            user_ids = torch.LongTensor([user_idx] * self.num_items).to(self.device)
            item_ids = torch.LongTensor(list(range(self.num_items))).to(self.device)
            
            scores, _, _ = self.model(user_ids, item_ids, self.adj_matrix)
            scores = scores.cpu().numpy()
            
            # æ’åºå¹¶é€‰æ‹©Top-K
            item_scores = [(self.idx_to_item[i], scores[i]) for i in range(self.num_items)]
            item_scores.sort(key=lambda x: x[1], reverse=True)
            
            # å¯é€‰ï¼šæ’é™¤ç”¨æˆ·å·²ç»äº¤äº’è¿‡çš„å•†å“
            if exclude_seen:
                # è¿™é‡Œéœ€è¦é¢å¤–çš„é€»è¾‘æ¥æ’é™¤å·²äº¤äº’å•†å“
                pass
            
            recommendations = [item_id for item_id, score in item_scores[:top_k]]
            
        return recommendations
    
    def get_embeddings(self):
        """è·å–å­¦ä¹ åˆ°çš„ç”¨æˆ·å’Œå•†å“åµŒå…¥"""
        if self.model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None, None
        
        self.model.eval()
        
        with torch.no_grad():
            # ä½¿ç”¨æ‰€æœ‰ç”¨æˆ·å’Œå•†å“çš„ç´¢å¼•
            all_user_ids = torch.LongTensor(list(range(self.num_users))).to(self.device)
            all_item_ids = torch.LongTensor(list(range(self.num_items))).to(self.device)
            
            # é€šè¿‡æ¨¡å‹è·å–æœ€ç»ˆåµŒå…¥
            _, user_embeddings, item_embeddings = self.model(
                all_user_ids, all_item_ids, self.adj_matrix
            )
            
            user_embeddings = user_embeddings.cpu().numpy()
            item_embeddings = item_embeddings.cpu().numpy()
        
        return user_embeddings, item_embeddings
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¿å­˜")
            return
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'user_to_idx': self.user_to_idx,
            'item_to_idx': self.item_to_idx,
            'idx_to_user': self.idx_to_user,
            'idx_to_item': self.idx_to_item,
            'num_users': self.num_users,
            'num_items': self.num_items,
            'config': {
                'embedding_dim': self.embedding_dim,
                'hidden_dim': self.hidden_dim,
                'num_layers': self.num_layers
            }
        }, path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        
        # æ¢å¤é…ç½®
        self.user_to_idx = checkpoint['user_to_idx']
        self.item_to_idx = checkpoint['item_to_idx']
        self.idx_to_user = checkpoint['idx_to_user']
        self.idx_to_item = checkpoint['idx_to_item']
        self.num_users = checkpoint['num_users']
        self.num_items = checkpoint['num_items']
        
        config = checkpoint['config']
        self.embedding_dim = config['embedding_dim']
        self.hidden_dim = config['hidden_dim']
        self.num_layers = config['num_layers']
        
        # é‡å»ºæ¨¡å‹
        self.model = GraphNeuralRecommender(
            num_users=self.num_users,
            num_items=self.num_items,
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers
        ).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"ï¿½ï¿½ æ¨¡å‹å·²ä» {path} åŠ è½½") 