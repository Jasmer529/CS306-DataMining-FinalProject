"""
åŸºäºTransformerçš„åºåˆ—æ¨èæ¨¡å‹
å®ç°SASRec (Self-Attentive Sequential Recommendation)
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import math
from collections import defaultdict
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_len=500):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class SASRecModel(nn.Module):
    """SASRec Transformeræ¨èæ¨¡å‹"""
    
    def __init__(self, n_items, hidden_size=64, num_layers=2, num_heads=2, 
                 max_seq_len=50, dropout=0.2):
        super().__init__()
        
        self.n_items = n_items
        self.hidden_size = hidden_size
        self.max_seq_len = max_seq_len
        
        # ç‰©å“åµŒå…¥å±‚
        self.item_embedding = nn.Embedding(n_items + 1, hidden_size, padding_idx=0)
        self.pos_encoding = PositionalEncoding(hidden_size, max_seq_len)
        
        # Transformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=dropout,
            activation='relu',
            batch_first=True
        )
        
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_size, n_items)
        self.dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–å‚æ•°
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
    
    def forward(self, input_seq, mask=None):
        """å‰å‘ä¼ æ’­"""
        # è·å–åºåˆ—é•¿åº¦
        seq_len = input_seq.size(1)
        
        # ç‰©å“åµŒå…¥
        embeddings = self.item_embedding(input_seq)  # [batch_size, seq_len, hidden_size]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        embeddings = self.pos_encoding(embeddings.transpose(0, 1)).transpose(0, 1)
        embeddings = self.dropout(embeddings)
        
        # åˆ›å»ºå› æœæ³¨æ„åŠ›æ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        if mask is None:
            mask = self._generate_square_subsequent_mask(seq_len).to(input_seq.device)
        
        # Transformerç¼–ç 
        hidden_states = self.transformer_encoder(embeddings, mask=mask)
        
        # è¾“å‡ºå±‚
        logits = self.output_layer(hidden_states)  # [batch_size, seq_len, n_items]
        
        return logits
    
    def _generate_square_subsequent_mask(self, sz):
        """ç”Ÿæˆå› æœæ³¨æ„åŠ›æ©ç """
        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)
        return mask

class SequenceDataset(Dataset):
    """åºåˆ—æ¨èæ•°æ®é›†"""
    
    def __init__(self, sequences, max_seq_len=50):
        self.sequences = sequences
        self.max_seq_len = max_seq_len
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        
        # æˆªå–æˆ–å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦
        if len(sequence) > self.max_seq_len:
            sequence = sequence[-self.max_seq_len:]
        
        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡åºåˆ—
        input_seq = sequence[:-1]
        target_seq = sequence[1:]
        
        # å¡«å……åˆ°æœ€å¤§é•¿åº¦
        input_seq = input_seq + [0] * (self.max_seq_len - 1 - len(input_seq))
        target_seq = target_seq + [0] * (self.max_seq_len - 1 - len(target_seq))
        
        return torch.tensor(input_seq), torch.tensor(target_seq)

class TransformerRecommender:
    """Transformeråºåˆ—æ¨èç³»ç»Ÿ"""
    
    def __init__(self, hidden_size=64, num_layers=2, num_heads=2, 
                 max_seq_len=50, learning_rate=0.001):
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.max_seq_len = max_seq_len
        self.learning_rate = learning_rate
        
        self.model = None
        self.item_to_idx = None
        self.idx_to_item = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def prepare_sequences(self, data, min_seq_len=3):
        """å‡†å¤‡ç”¨æˆ·åºåˆ—æ•°æ®"""
        print("ğŸ” å‡†å¤‡ç”¨æˆ·è¡Œä¸ºåºåˆ—...")
        
        # æŒ‰æ—¶é—´æ’åº
        data_sorted = data.sort_values(['user_id', 'datetime'])
        
        # åˆ›å»ºç‰©å“ç´¢å¼•æ˜ å°„
        unique_items = data_sorted['item_id'].unique()
        self.item_to_idx = {item: idx + 1 for idx, item in enumerate(unique_items)}  # 0ä¿ç•™ç»™padding
        self.idx_to_item = {idx: item for item, idx in self.item_to_idx.items()}
        
        # æ„å»ºç”¨æˆ·åºåˆ—
        user_sequences = []
        for user_id, user_data in data_sorted.groupby('user_id'):
            # å°†ç‰©å“IDè½¬æ¢ä¸ºç´¢å¼•
            item_sequence = [self.item_to_idx[item] for item in user_data['item_id']]
            
            # è¿‡æ»¤çŸ­åºåˆ—
            if len(item_sequence) >= min_seq_len:
                user_sequences.append(item_sequence)
        
        print(f"âœ… åºåˆ—å‡†å¤‡å®Œæˆ: {len(user_sequences)} ä¸ªç”¨æˆ·åºåˆ—")
        print(f"   ç‰©å“æ•°é‡: {len(unique_items)}")
        print(f"   å¹³å‡åºåˆ—é•¿åº¦: {np.mean([len(seq) for seq in user_sequences]):.1f}")
        
        return user_sequences
    
    def train(self, data, epochs=50, batch_size=256, min_seq_len=3):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒTransformeræ¨èæ¨¡å‹...")
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        user_sequences = self.prepare_sequences(data, min_seq_len)
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = SequenceDataset(user_sequences, self.max_seq_len)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        n_items = len(self.item_to_idx)
        self.model = SASRecModel(
            n_items=n_items,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            max_seq_len=self.max_seq_len
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        
        # è®­ç»ƒå¾ªç¯
        losses = []
        self.model.train()
        
        for epoch in range(epochs):
            epoch_loss = 0
            batch_count = 0
            
            for input_seq, target_seq in dataloader:
                input_seq = input_seq.to(self.device)
                target_seq = target_seq.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                logits = self.model(input_seq)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(logits.view(-1, logits.size(-1)), target_seq.view(-1))
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_loss = epoch_loss / batch_count
            losses.append(avg_loss)
            
            if epoch % 10 == 0:
                print(f"  Epoch {epoch}: Loss = {avg_loss:.4f}")
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        return losses
    
    def predict_next_items(self, user_sequence, top_k=10):
        """é¢„æµ‹ç”¨æˆ·åºåˆ—çš„ä¸‹ä¸€ä¸ªç‰©å“"""
        if self.model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return []
        
        self.model.eval()
        
        # è½¬æ¢ç‰©å“IDä¸ºç´¢å¼•
        try:
            sequence_indices = [self.item_to_idx.get(item, 0) for item in user_sequence]
        except:
            return []
        
        # æˆªå–æˆ–å¡«å……åºåˆ—
        if len(sequence_indices) > self.max_seq_len - 1:
            sequence_indices = sequence_indices[-(self.max_seq_len - 1):]
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        padded_sequence = sequence_indices + [0] * (self.max_seq_len - 1 - len(sequence_indices))
        input_tensor = torch.tensor([padded_sequence]).to(self.device)
        
        with torch.no_grad():
            # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
            logits = self.model(input_tensor)
            last_logits = logits[0, len(sequence_indices) - 1, :]  # æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
            
            # è·å–top-ké¢„æµ‹
            _, top_indices = torch.topk(last_logits, top_k)
            
            # è½¬æ¢å›ç‰©å“ID
            recommendations = []
            for idx in top_indices.cpu().numpy():
                if idx > 0 and idx in self.idx_to_item:  # æ’é™¤paddingç´¢å¼•
                    item_id = self.idx_to_item[idx]
                    if item_id not in user_sequence:  # é¿å…æ¨èå·²äº¤äº’ç‰©å“
                        recommendations.append(item_id)
            
            return recommendations[:top_k]
    
    def recommend_for_user(self, data, user_id, top_k=10):
        """ä¸ºæŒ‡å®šç”¨æˆ·æ¨èç‰©å“"""
        # è·å–ç”¨æˆ·å†å²åºåˆ—
        user_data = data[data['user_id'] == user_id].sort_values('datetime')
        
        if len(user_data) == 0:
            return []
        
        user_sequence = user_data['item_id'].tolist()
        return self.predict_next_items(user_sequence, top_k)
    
    def evaluate_model(self, data, test_ratio=0.2):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        user_sequences = self.prepare_sequences(data)
        
        hit_count = 0
        total_count = 0
        
        for sequence in user_sequences:
            if len(sequence) < 4:  # éœ€è¦è¶³å¤Ÿé•¿çš„åºåˆ—è¿›è¡Œæµ‹è¯•
                continue
            
            # ä½¿ç”¨å‰80%ä½œä¸ºè¾“å…¥ï¼Œå20%ä½œä¸ºæµ‹è¯•ç›®æ ‡
            split_point = int(len(sequence) * (1 - test_ratio))
            input_seq = [self.idx_to_item.get(idx, 0) for idx in sequence[:split_point]]
            target_items = set([self.idx_to_item.get(idx, 0) for idx in sequence[split_point:]])
            
            # è·å–æ¨èç»“æœ
            recommendations = self.predict_next_items(input_seq, top_k=10)
            
            # è®¡ç®—å‘½ä¸­ç‡
            if any(item in target_items for item in recommendations):
                hit_count += 1
            total_count += 1
        
        hit_rate = hit_count / total_count if total_count > 0 else 0
        print(f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ:")
        print(f"   æµ‹è¯•åºåˆ—æ•°: {total_count}")
        print(f"   å‘½ä¸­ç‡ (Hit Rate): {hit_rate:.3f}")
        
        return hit_rate
    
    def visualize_attention_weights(self, user_sequence, save_path=None):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
        if self.model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦ä¿®æ”¹æ¨¡å‹ä»¥è¿”å›æ³¨æ„åŠ›æƒé‡
        print("ğŸ’¡ æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–åŠŸèƒ½éœ€è¦æ¨¡å‹ä¿®æ”¹ä»¥æš´éœ²æ³¨æ„åŠ›å±‚")
        print("   å»ºè®®åœ¨æ¨¡å‹forwardæ–¹æ³•ä¸­è¿”å›æ³¨æ„åŠ›æƒé‡")
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
            return
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'item_to_idx': self.item_to_idx,
            'idx_to_item': self.idx_to_item,
            'model_params': {
                'hidden_size': self.hidden_size,
                'num_layers': self.num_layers,
                'num_heads': self.num_heads,
                'max_seq_len': self.max_seq_len
            }
        }
        
        torch.save(checkpoint, path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        
        # æ¢å¤æ˜ å°„å…³ç³»
        self.item_to_idx = checkpoint['item_to_idx']
        self.idx_to_item = checkpoint['idx_to_item']
        
        # é‡å»ºæ¨¡å‹
        params = checkpoint['model_params']
        n_items = len(self.item_to_idx)
        
        self.model = SASRecModel(
            n_items=n_items,
            hidden_size=params['hidden_size'],
            num_layers=params['num_layers'],
            num_heads=params['num_heads'],
            max_seq_len=params['max_seq_len']
        ).to(self.device)
        
        # åŠ è½½æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æ¨¡å‹å·²ä» {path} åŠ è½½")

def demo_transformer_recommender():
    """Transformeræ¨èæ¼”ç¤º"""
    print("ğŸš€ Transformeråºåˆ—æ¨èæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_users, n_items = 1000, 500
    
    data_list = []
    for user_id in range(1, n_users + 1):
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ—¶é—´åºåˆ—è¡Œä¸º
        n_actions = np.random.randint(5, 50)
        items = np.random.choice(range(1, n_items + 1), size=n_actions, replace=True)
        
        for i, item_id in enumerate(items):
            timestamp = pd.Timestamp('2023-01-01') + pd.Timedelta(days=i)
            data_list.append({
                'user_id': user_id,
                'item_id': item_id,
                'datetime': timestamp,
                'behavior_type': 'pv'
            })
    
    demo_data = pd.DataFrame(data_list)
    print(f"ğŸ“Š ç¤ºä¾‹æ•°æ®: {len(demo_data)} æ¡è¡Œä¸ºè®°å½•")
    
    # åˆå§‹åŒ–Transformeræ¨èå™¨
    recommender = TransformerRecommender(
        hidden_size=32,  # ä¸ºäº†æ¼”ç¤ºä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
        num_layers=2,
        num_heads=2,
        max_seq_len=20
    )
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ” è®­ç»ƒTransformeræ¨¡å‹...")
    losses = recommender.train(demo_data, epochs=20, batch_size=64)
    
    # ä¸ºç”¨æˆ·æ¨è
    test_user = 1
    recommendations = recommender.recommend_for_user(demo_data, test_user, top_k=5)
    print(f"\nğŸ“‹ ä¸ºç”¨æˆ· {test_user} æ¨èçš„ç‰©å“: {recommendations}")
    
    # è¯„ä¼°æ¨¡å‹
    hit_rate = recommender.evaluate_model(demo_data)
    
    # å¯è§†åŒ–æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('Transformeræ¨¡å‹è®­ç»ƒæŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    print("\nâœ… Transformeræ¨èæ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    demo_transformer_recommender() 