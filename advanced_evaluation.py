"""
é«˜çº§æ¨èç³»ç»Ÿè¯„ä¼°æ¨¡å—
åŒ…å«å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®æ€§ã€å¤šæ ·æ€§ã€æ–°é¢–æ€§ã€è¦†ç›–ç‡ç­‰
"""

import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
from collections import defaultdict
import math
import warnings
warnings.filterwarnings('ignore')

class AdvancedRecommenderEvaluator:
    """æ¨èç³»ç»Ÿé«˜çº§è¯„ä¼°å™¨"""
    
    def __init__(self, data):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        :param data: åŸå§‹æ•°æ®DataFrame
        """
        self.data = data
        self.user_item_matrix = None
        self.item_popularity = None
        self.prepare_evaluation_data()
    
    def prepare_evaluation_data(self):
        """å‡†å¤‡è¯„ä¼°æ•°æ®"""
        # åˆ›å»ºç”¨æˆ·-å•†å“äº¤äº’çŸ©é˜µ
        self.user_item_matrix = self.data.pivot_table(
            index='user_id', 
            columns='item_id', 
            values='behavior_type',
            aggfunc='count',
            fill_value=0
        )
        
        # è®¡ç®—å•†å“æµè¡Œåº¦
        self.item_popularity = self.data['item_id'].value_counts().to_dict()
        
        print(f"ğŸ“Š è¯„ä¼°æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   ç”¨æˆ·æ•°é‡: {len(self.user_item_matrix)}")
        print(f"   å•†å“æ•°é‡: {len(self.user_item_matrix.columns)}")
    
    def split_data(self, test_ratio=0.2, random_state=42):
        """
        åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
        :param test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        :param random_state: éšæœºç§å­
        :return: train_data, test_data
        """
        # æŒ‰æ—¶é—´æ’åº
        data_sorted = self.data.sort_values('datetime')
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ†å‰²æ•°æ®
        train_list = []
        test_list = []
        
        for user_id in self.data['user_id'].unique():
            user_data = data_sorted[data_sorted['user_id'] == user_id]
            n_test = max(1, int(len(user_data) * test_ratio))
            
            train_data = user_data.iloc[:-n_test]
            test_data = user_data.iloc[-n_test:]
            
            train_list.append(train_data)
            test_list.append(test_data)
        
        train_data = pd.concat(train_list, ignore_index=True)
        test_data = pd.concat(test_list, ignore_index=True)
        
        return train_data, test_data
    
    def precision_at_k(self, actual, predicted, k=10):
        """è®¡ç®—Precision@K"""
        if not predicted:
            return 0.0
        
        predicted_k = predicted[:k]
        relevant = len(set(predicted_k) & set(actual))
        return relevant / min(k, len(predicted_k))
    
    def recall_at_k(self, actual, predicted, k=10):
        """è®¡ç®—Recall@K"""
        if not actual:
            return 0.0
        
        predicted_k = predicted[:k]
        relevant = len(set(predicted_k) & set(actual))
        return relevant / len(actual)
    
    def ndcg_at_k(self, actual, predicted, k=10):
        """è®¡ç®—NDCG@K (Normalized Discounted Cumulative Gain)"""
        if not predicted:
            return 0.0
        
        predicted_k = predicted[:k]
        
        # è®¡ç®—DCG
        dcg = 0.0
        for i, item in enumerate(predicted_k):
            if item in actual:
                dcg += 1.0 / math.log2(i + 2)
        
        # è®¡ç®—IDCG
        idcg = 0.0
        for i in range(min(len(actual), k)):
            idcg += 1.0 / math.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def diversity_score(self, recommendations, item_features=None):
        """
        è®¡ç®—æ¨èåˆ—è¡¨çš„å¤šæ ·æ€§
        ä½¿ç”¨é¡¹ç›®é—´çš„å¹³å‡è·ç¦»
        """
        if len(recommendations) < 2:
            return 0.0
        
        if item_features is not None:
            # åŸºäºç‰¹å¾çš„å¤šæ ·æ€§
            diversity = 0.0
            count = 0
            for i in range(len(recommendations)):
                for j in range(i+1, len(recommendations)):
                    item_i = recommendations[i]
                    item_j = recommendations[j]
                    if item_i in item_features and item_j in item_features:
                        # è®¡ç®—ä½™å¼¦è·ç¦»
                        features_i = np.array(item_features[item_i])
                        features_j = np.array(item_features[item_j])
                        similarity = np.dot(features_i, features_j) / (
                            np.linalg.norm(features_i) * np.linalg.norm(features_j)
                        )
                        diversity += 1 - similarity
                        count += 1
            
            return diversity / count if count > 0 else 0.0
        else:
            # åŸºäºå“ç±»çš„å¤šæ ·æ€§
            categories = []
            for item in recommendations:
                item_data = self.data[self.data['item_id'] == item]
                if not item_data.empty and 'category_id' in item_data.columns:
                    categories.append(item_data['category_id'].iloc[0])
            
            if not categories:
                return 0.0
            
            unique_categories = len(set(categories))
            return unique_categories / len(categories)
    
    def novelty_score(self, recommendations):
        """
        è®¡ç®—æ¨èåˆ—è¡¨çš„æ–°é¢–æ€§
        åŸºäºå•†å“æµè¡Œåº¦çš„è´Ÿå¯¹æ•°
        """
        if not recommendations:
            return 0.0
        
        total_novelty = 0.0
        total_items = len(self.data)
        
        for item in recommendations:
            if item in self.item_popularity:
                popularity = self.item_popularity[item]
                novelty = -math.log2(popularity / total_items)
                total_novelty += novelty
        
        return total_novelty / len(recommendations)
    
    def coverage_score(self, all_recommendations):
        """
        è®¡ç®—æ¨èç³»ç»Ÿçš„è¦†ç›–ç‡
        :param all_recommendations: æ‰€æœ‰ç”¨æˆ·çš„æ¨èåˆ—è¡¨
        """
        recommended_items = set()
        for recommendations in all_recommendations:
            recommended_items.update(recommendations)
        
        total_items = len(self.user_item_matrix.columns)
        return len(recommended_items) / total_items
    
    def evaluate_algorithm(self, algorithm, test_data, algorithm_name="Unknown", k=10):
        """
        å…¨é¢è¯„ä¼°å•ä¸ªç®—æ³•
        :param algorithm: æ¨èç®—æ³•å¯¹è±¡
        :param test_data: æµ‹è¯•æ•°æ®
        :param algorithm_name: ç®—æ³•åç§°
        :param k: Top-Kæ¨èæ•°é‡
        :return: è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nğŸ“Š è¯„ä¼°ç®—æ³•: {algorithm_name}")
        
        # è·å–æµ‹è¯•ç”¨æˆ·
        test_users = test_data['user_id'].unique()
        
        metrics = {
            'precision': [],
            'recall': [],
            'ndcg': [],
            'diversity': [],
            'novelty': []
        }
        
        all_recommendations = []
        successful_count = 0
        
        for user_id in test_users:
            try:
                # è·å–ç”¨æˆ·çš„çœŸå®äº¤äº’é¡¹ç›®
                actual_items = test_data[test_data['user_id'] == user_id]['item_id'].tolist()
                
                # ç”Ÿæˆæ¨è
                if hasattr(algorithm, 'recommend'):
                    recommendations = algorithm.recommend(user_id, top_k=k)
                elif hasattr(algorithm, 'recommend_for_user'):
                    recommendations = algorithm.recommend_for_user(
                        self.data[self.data['user_id'] != user_id], user_id, top_k=k
                    )
                else:
                    continue
                
                if not recommendations:
                    continue
                
                # è®¡ç®—æŒ‡æ ‡
                precision = self.precision_at_k(actual_items, recommendations, k)
                recall = self.recall_at_k(actual_items, recommendations, k)
                ndcg = self.ndcg_at_k(actual_items, recommendations, k)
                diversity = self.diversity_score(recommendations)
                novelty = self.novelty_score(recommendations)
                
                metrics['precision'].append(precision)
                metrics['recall'].append(recall)
                metrics['ndcg'].append(ndcg)
                metrics['diversity'].append(diversity)
                metrics['novelty'].append(novelty)
                
                all_recommendations.append(recommendations)
                successful_count += 1
                
            except Exception as e:
                continue
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        results = {}
        for metric, values in metrics.items():
            if values:
                results[f'{metric}_mean'] = np.mean(values)
                results[f'{metric}_std'] = np.std(values)
            else:
                results[f'{metric}_mean'] = 0.0
                results[f'{metric}_std'] = 0.0
        
        # è®¡ç®—è¦†ç›–ç‡
        results['coverage'] = self.coverage_score(all_recommendations)
        results['success_rate'] = successful_count / len(test_users)
        
        print(f"   âœ… æˆåŠŸè¯„ä¼° {successful_count}/{len(test_users)} ä¸ªç”¨æˆ·")
        print(f"   ğŸ“ˆ Precision@{k}: {results['precision_mean']:.4f} Â± {results['precision_std']:.4f}")
        print(f"   ğŸ“ˆ Recall@{k}: {results['recall_mean']:.4f} Â± {results['recall_std']:.4f}")
        print(f"   ğŸ“ˆ NDCG@{k}: {results['ndcg_mean']:.4f} Â± {results['ndcg_std']:.4f}")
        print(f"   ğŸ¯ Diversity: {results['diversity_mean']:.4f} Â± {results['diversity_std']:.4f}")
        print(f"   ğŸ†• Novelty: {results['novelty_mean']:.4f} Â± {results['novelty_std']:.4f}")
        print(f"   ğŸ“Š Coverage: {results['coverage']:.4f}")
        
        return results
    
    def compare_algorithms(self, algorithms, test_data, k=10):
        """
        æ¯”è¾ƒå¤šä¸ªç®—æ³•çš„æ€§èƒ½
        :param algorithms: ç®—æ³•å­—å…¸ {name: algorithm}
        :param test_data: æµ‹è¯•æ•°æ®
        :param k: Top-Kæ¨èæ•°é‡
        :return: æ¯”è¾ƒç»“æœDataFrame
        """
        print("\n" + "="*60)
        print("ğŸ” å¼€å§‹ç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
        print("="*60)
        
        comparison_results = {}
        
        for alg_name, algorithm in algorithms.items():
            results = self.evaluate_algorithm(algorithm, test_data, alg_name, k)
            comparison_results[alg_name] = results
        
        # è½¬æ¢ä¸ºDataFrame
        comparison_df = pd.DataFrame(comparison_results).T
        
        # æŒ‰ä¸»è¦æŒ‡æ ‡æ’åº
        comparison_df['f1_score'] = 2 * (comparison_df['precision_mean'] * comparison_df['recall_mean']) / (
            comparison_df['precision_mean'] + comparison_df['recall_mean'] + 1e-8
        )
        
        comparison_df = comparison_df.sort_values('f1_score', ascending=False)
        
        print("\nğŸ† ç®—æ³•æ€§èƒ½æ’å:")
        print(comparison_df[['precision_mean', 'recall_mean', 'ndcg_mean', 'diversity_mean', 'coverage', 'f1_score']].round(4))
        
        return comparison_df
    
    def generate_evaluation_report(self, comparison_results, output_path='evaluation_report.md'):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# æ¨èç³»ç»Ÿç®—æ³•è¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"**è¯„ä¼°æ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## è¯„ä¼°æŒ‡æ ‡è¯´æ˜\n\n")
            f.write("- **Precision@K**: æ¨èåˆ—è¡¨ä¸­ç›¸å…³é¡¹ç›®çš„æ¯”ä¾‹\n")
            f.write("- **Recall@K**: ç”¨æˆ·ç›¸å…³é¡¹ç›®ä¸­è¢«æ¨èçš„æ¯”ä¾‹\n")
            f.write("- **NDCG@K**: è€ƒè™‘æ’åºè´¨é‡çš„è¯„ä¼°æŒ‡æ ‡\n")
            f.write("- **Diversity**: æ¨èåˆ—è¡¨çš„å¤šæ ·æ€§\n")
            f.write("- **Novelty**: æ¨èé¡¹ç›®çš„æ–°é¢–æ€§\n")
            f.write("- **Coverage**: æ¨èç³»ç»Ÿè¦†ç›–çš„å•†å“æ¯”ä¾‹\n\n")
            
            f.write("## ç®—æ³•æ€§èƒ½å¯¹æ¯”\n\n")
            f.write(comparison_results.round(4).to_markdown())
            
            f.write("\n\n## æ€§èƒ½åˆ†æ\n\n")
            best_algorithm = comparison_results.index[0]
            f.write(f"**æœ€ä½³ç®—æ³•**: {best_algorithm}\n\n")
            f.write("**å„æŒ‡æ ‡æœ€ä¼˜ç®—æ³•**:\n")
            
            metrics = ['precision_mean', 'recall_mean', 'ndcg_mean', 'diversity_mean', 'coverage']
            for metric in metrics:
                best_alg = comparison_results[metric].idxmax()
                best_value = comparison_results.loc[best_alg, metric]
                f.write(f"- {metric}: {best_alg} ({best_value:.4f})\n")
        
        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}") 