"""
å¢å¼ºç‰ˆç”µå•†ç”¨æˆ·è¡Œä¸ºæ¨èç³»ç»Ÿ
é›†æˆæ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼šå¤šç»´åº¦è¯„ä¼°ã€å›¾ç¥ç»ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ ç­‰
"""

import pandas as pd
import numpy as np
import warnings
from datetime import datetime
import os
import sys
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥æ‰€æœ‰æ¨èæ¨¡å—
from user_analysis_enhancement import UserAnalysisEnhancer
from collaborative_filtering import CollaborativeFiltering
from transformer_recommender import TransformerRecommender
from advanced_evaluation import AdvancedRecommenderEvaluator
from graph_neural_recommender import GNNRecommenderSystem
from reinforcement_learning_recommender import RLRecommenderSystem

warnings.filterwarnings('ignore')

class EnhancedECommerceRecommenderSystem:
    """å¢å¼ºç‰ˆç”µå•†æ¨èç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, data_path=None):
        """
        åˆå§‹åŒ–å¢å¼ºæ¨èç³»ç»Ÿ
        :param data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data_path = data_path
        self.data = None
        self.train_data = None
        self.test_data = None
        
        # åˆ†ææ¨¡å—
        self.user_analyzer = None
        self.evaluator = None
        
        # æ¨èç®—æ³•
        self.algorithms = {}
        self.evaluation_results = {}
        
        print("ğŸš€ å¢å¼ºç‰ˆç”µå•†ç”¨æˆ·è¡Œä¸ºæ¨èç³»ç»Ÿ")
        print("=" * 70)
        print("ğŸ”¥ é›†æˆå‰æ²¿æŠ€æœ¯:")
        print("   ğŸ“Š å¤šç»´åº¦è¯„ä¼° (Precision@K, NDCG, Diversity, Novelty)")
        print("   ğŸ•¸ï¸  å›¾ç¥ç»ç½‘ç»œæ¨è (GCN)")
        print("   ğŸ¯ å¼ºåŒ–å­¦ä¹ æ¨è (DQN)")
        print("   ğŸ¤– Transformeråºåˆ—æ¨è")
        print("   ğŸ‘¥ ååŒè¿‡æ»¤ç®—æ³•")
        print("=" * 70)
        
    def load_data(self, data_path=None, sample_size=None):
        """
        åŠ è½½æ•°æ®
        :param data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        :param sample_size: é‡‡æ ·å¤§å°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        """
        if data_path:
            self.data_path = data_path
            
        if not self.data_path:
            print("âŒ è¯·æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„")
            return False
            
        try:
            print(f"ğŸ“ æ­£åœ¨åŠ è½½æ•°æ®: {self.data_path}")
            
            # è¯»å–æ•°æ®
            if sample_size:
                print(f"ğŸ“Š é‡‡æ · {sample_size} æ¡è®°å½•è¿›è¡Œå¿«é€Ÿæµ‹è¯•...")
                self.data = pd.read_csv(self.data_path, nrows=sample_size)
            else:
                self.data = pd.read_csv(self.data_path)
            
            # æ•°æ®é¢„å¤„ç†
            self._preprocess_data()
            
            # æ•°æ®åˆ†å‰²
            self._split_data()
            
            # åˆå§‹åŒ–è¯„ä¼°å™¨
            self.evaluator = AdvancedRecommenderEvaluator(self.train_data)
            
            print(f"âœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å®Œæˆ!")
            self._print_data_summary()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        # æ—¶é—´æˆ³å¤„ç†
        if 'datetime' not in self.data.columns and 'timestamp' in self.data.columns:
            self.data['datetime'] = pd.to_datetime(self.data['timestamp'], unit='s')
        elif 'datetime' in self.data.columns:
            self.data['datetime'] = pd.to_datetime(self.data['datetime'])
        
        # æ·»åŠ æ—¶é—´ç‰¹å¾
        if 'datetime' in self.data.columns:
            self.data['date'] = self.data['datetime'].dt.date
            self.data['hour'] = self.data['datetime'].dt.hour
            self.data['day_of_week'] = self.data['datetime'].dt.day_of_week
        
        # æ•°æ®æ¸…æ´—
        self.data = self.data.dropna(subset=['user_id', 'item_id', 'behavior_type'])
        
        # è¿‡æ»¤ä½é¢‘ç”¨æˆ·å’Œå•†å“
        user_counts = self.data['user_id'].value_counts()
        item_counts = self.data['item_id'].value_counts()
        
        # ä¿ç•™è‡³å°‘æœ‰5æ¬¡äº¤äº’çš„ç”¨æˆ·å’Œå•†å“
        valid_users = user_counts[user_counts >= 5].index
        valid_items = item_counts[item_counts >= 5].index
        
        self.data = self.data[
            (self.data['user_id'].isin(valid_users)) & 
            (self.data['item_id'].isin(valid_items))
        ]
        
        print(f"ğŸ“Š æ•°æ®æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(self.data)} æ¡è®°å½•")
    
    def _split_data(self, test_ratio=0.2):
        """åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print(f"ğŸ”€ åˆ†å‰²æ•°æ® (æµ‹è¯•é›†æ¯”ä¾‹: {test_ratio})")
        
        # æ—¶é—´åˆ†å‰²
        self.data = self.data.sort_values('datetime')
        split_point = int(len(self.data) * (1 - test_ratio))
        
        self.train_data = self.data.iloc[:split_point].copy()
        self.test_data = self.data.iloc[split_point:].copy()
        
        print(f"   è®­ç»ƒé›†: {len(self.train_data)} æ¡è®°å½•")
        print(f"   æµ‹è¯•é›†: {len(self.test_data)} æ¡è®°å½•")
    
    def _print_data_summary(self):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        print(f"\nğŸ“‹ æ•°æ®æ‘˜è¦:")
        print(f"   ğŸ“Š æ€»è®°å½•æ•°: {len(self.data):,}")
        print(f"   ğŸ‘¥ ç”¨æˆ·æ•°é‡: {self.data['user_id'].nunique():,}")
        print(f"   ğŸ›ï¸ å•†å“æ•°é‡: {self.data['item_id'].nunique():,}")
        
        if 'category_id' in self.data.columns:
            print(f"   ğŸ·ï¸ å“ç±»æ•°é‡: {self.data['category_id'].nunique():,}")
            
        behavior_counts = self.data['behavior_type'].value_counts()
        print(f"   ğŸ¯ è¡Œä¸ºåˆ†å¸ƒ:")
        for behavior, count in behavior_counts.items():
            percentage = count / len(self.data) * 100
            print(f"      {behavior}: {count:,} ({percentage:.1f}%)")
    
    def analyze_users(self):
        """ç”¨æˆ·è¡Œä¸ºåˆ†æ"""
        if self.train_data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return
            
        print("\n" + "="*60)
        print("ğŸ‘¥ ç”¨æˆ·è¡Œä¸ºæ·±åº¦åˆ†æ")
        print("="*60)
        
        # åˆå§‹åŒ–ç”¨æˆ·åˆ†æå™¨
        self.user_analyzer = UserAnalysisEnhancer(self.train_data)
        
        # æ‰§è¡Œåˆ†æ
        print("\nğŸ” è¿›è¡ŒRFMåˆ†æ...")
        rfm_data = self.user_analyzer.calculate_rfm_features()
        rfm_segments = self.user_analyzer.rfm_segmentation()
        
        print("\nğŸ” è®¡ç®—é«˜çº§ç”¨æˆ·ç‰¹å¾...")
        user_features = self.user_analyzer.calculate_advanced_features()
        
        print("\nğŸ” è¿›è¡ŒK-meansèšç±»...")
        clusters = self.user_analyzer.kmeans_clustering()
        
        print("\nğŸ” ç”Ÿæˆç”¨æˆ·ç”»åƒ...")
        profiles = self.user_analyzer.generate_user_profiles()
        
        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        self.user_analyzer.save_analysis_results('enhanced_analysis_results')
        
        print("âœ… ç”¨æˆ·åˆ†æå®Œæˆ!")
        return True
    
    def train_all_algorithms(self):
        """è®­ç»ƒæ‰€æœ‰æ¨èç®—æ³•"""
        if self.train_data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return
            
        print("\n" + "="*60)
        print("ğŸ¤– è®­ç»ƒæ‰€æœ‰æ¨èç®—æ³•")
        print("="*60)
        
        # 1. ååŒè¿‡æ»¤ç®—æ³•
        print("\nğŸ” è®­ç»ƒååŒè¿‡æ»¤ç®—æ³•...")
        self._train_collaborative_filtering()
        
        # 2. Transformeråºåˆ—æ¨è
        print("\nğŸ” è®­ç»ƒTransformeråºåˆ—æ¨è...")
        self._train_transformer_recommender()
        
        # 3. å›¾ç¥ç»ç½‘ç»œæ¨è
        print("\nğŸ” è®­ç»ƒå›¾ç¥ç»ç½‘ç»œæ¨è...")
        self._train_gnn_recommender()
        
        # 4. å¼ºåŒ–å­¦ä¹ æ¨è
        print("\nğŸ” è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¨è...")
        self._train_rl_recommender()
        
        print(f"\nâœ… æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆ! å…±è®­ç»ƒ {len(self.algorithms)} ä¸ªç®—æ³•")
        return True
    
    def _train_collaborative_filtering(self):
        """è®­ç»ƒååŒè¿‡æ»¤ç®—æ³•"""
        try:
            # åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤
            user_cf = CollaborativeFiltering('user_based')
            user_cf.prepare_data(self.train_data)
            user_cf.calculate_similarity()
            self.algorithms['UserCF'] = user_cf
            print("   âœ… åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤è®­ç»ƒå®Œæˆ")
            
            # åŸºäºç‰©å“çš„ååŒè¿‡æ»¤
            item_cf = CollaborativeFiltering('item_based')
            item_cf.prepare_data(self.train_data)
            item_cf.calculate_similarity()
            self.algorithms['ItemCF'] = item_cf
            print("   âœ… åŸºäºç‰©å“çš„ååŒè¿‡æ»¤è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            print(f"   âš ï¸ ååŒè¿‡æ»¤è®­ç»ƒå¤±è´¥: {e}")
    
    def _train_transformer_recommender(self):
        """è®­ç»ƒTransformeræ¨èæ¨¡å‹"""
        try:
            transformer_rec = TransformerRecommender(
                hidden_size=64,
                num_layers=2,
                num_heads=4,
                max_seq_len=50
            )
            
            losses = transformer_rec.train(self.train_data, epochs=20, batch_size=64)
            self.algorithms['Transformer'] = transformer_rec
            print("   âœ… Transformeråºåˆ—æ¨èè®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            print(f"   âš ï¸ Transformerè®­ç»ƒå¤±è´¥: {e}")
    
    def _train_gnn_recommender(self):
        """è®­ç»ƒå›¾ç¥ç»ç½‘ç»œæ¨èæ¨¡å‹"""
        try:
            gnn_rec = GNNRecommenderSystem(
                embedding_dim=32,
                hidden_dim=16,
                num_layers=2,
                learning_rate=0.001
            )
            
            gnn_rec.setup(self.train_data)
            losses, val_losses = gnn_rec.train(
                self.train_data, 
                epochs=30, 
                batch_size=512
            )
            self.algorithms['GNN'] = gnn_rec
            print("   âœ… å›¾ç¥ç»ç½‘ç»œæ¨èè®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            print(f"   âš ï¸ GNNè®­ç»ƒå¤±è´¥: {e}")
    
    def _train_rl_recommender(self):
        """è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¨èæ¨¡å‹"""
        try:
            rl_rec = RLRecommenderSystem(
                state_dim=50,
                hidden_dim=128,
                learning_rate=0.001
            )
            
            rl_rec.setup(self.train_data)
            episode_rewards, episode_losses = rl_rec.train(
                episodes=200,
                max_steps_per_episode=10
            )
            self.algorithms['RL'] = rl_rec
            print("   âœ… å¼ºåŒ–å­¦ä¹ æ¨èè®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            print(f"   âš ï¸ å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤±è´¥: {e}")
    
    def evaluate_all_algorithms(self, top_k=10):
        """è¯„ä¼°æ‰€æœ‰ç®—æ³•æ€§èƒ½"""
        if not self.algorithms:
            print("âŒ è¯·å…ˆè®­ç»ƒç®—æ³•")
            return
            
        print("\n" + "="*60)
        print("ğŸ“Š å…¨é¢ç®—æ³•æ€§èƒ½è¯„ä¼°")
        print("="*60)
        
        # ä½¿ç”¨é«˜çº§è¯„ä¼°å™¨è¿›è¡Œè¯„ä¼°
        self.evaluation_results = self.evaluator.compare_algorithms(
            self.algorithms, 
            self.test_data, 
            k=top_k
        )
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self.evaluator.generate_evaluation_report(
            self.evaluation_results, 
            'enhanced_evaluation_report.md'
        )
        
        # å¯è§†åŒ–è¯„ä¼°ç»“æœ
        self._visualize_evaluation_results()
        
        print("âœ… ç®—æ³•è¯„ä¼°å®Œæˆ!")
        return self.evaluation_results
    
    def _visualize_evaluation_results(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        if self.evaluation_results is None or self.evaluation_results.empty:
            return
        
        print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°ç»“æœå¯è§†åŒ–...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¨èç®—æ³•æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # 1. å‡†ç¡®æ€§æŒ‡æ ‡å¯¹æ¯”
        metrics = ['precision_mean', 'recall_mean', 'ndcg_mean']
        ax1 = axes[0, 0]
        self.evaluation_results[metrics].plot(kind='bar', ax=ax1)
        ax1.set_title('å‡†ç¡®æ€§æŒ‡æ ‡å¯¹æ¯”')
        ax1.set_ylabel('æŒ‡æ ‡å€¼')
        ax1.legend(['Precision@10', 'Recall@10', 'NDCG@10'])
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. å¤šæ ·æ€§å’Œè¦†ç›–ç‡
        ax2 = axes[0, 1]
        diversity_coverage = self.evaluation_results[['diversity_mean', 'coverage']]
        diversity_coverage.plot(kind='bar', ax=ax2)
        ax2.set_title('å¤šæ ·æ€§å’Œè¦†ç›–ç‡')
        ax2.set_ylabel('æŒ‡æ ‡å€¼')
        ax2.legend(['å¤šæ ·æ€§', 'è¦†ç›–ç‡'])
        ax2.tick_params(axis='x', rotation=45)
        
        # 3. F1åˆ†æ•°é›·è¾¾å›¾
        ax3 = axes[1, 0]
        f1_scores = self.evaluation_results['f1_score'].sort_values(ascending=True)
        colors = plt.cm.Set3(np.linspace(0, 1, len(f1_scores)))
        bars = ax3.barh(range(len(f1_scores)), f1_scores.values, color=colors)
        ax3.set_yticks(range(len(f1_scores)))
        ax3.set_yticklabels(f1_scores.index)
        ax3.set_xlabel('F1åˆ†æ•°')
        ax3.set_title('F1åˆ†æ•°æ’å')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, f1_scores.values)):
            ax3.text(value + 0.001, i, f'{value:.3f}', 
                    va='center', ha='left', fontsize=10)
        
        # 4. ç»¼åˆæ€§èƒ½çƒ­åŠ›å›¾
        ax4 = axes[1, 1]
        heatmap_data = self.evaluation_results[
            ['precision_mean', 'recall_mean', 'ndcg_mean', 'diversity_mean', 'coverage']
        ].T
        
        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4)
        ax4.set_title('ç»¼åˆæ€§èƒ½çƒ­åŠ›å›¾')
        ax4.set_xlabel('ç®—æ³•')
        ax4.set_ylabel('è¯„ä¼°æŒ‡æ ‡')
        
        plt.tight_layout()
        plt.savefig('enhanced_algorithm_evaluation.png', dpi=300, bbox_inches='tight')
        print("   ğŸ’¾ è¯„ä¼°å›¾è¡¨å·²ä¿å­˜: enhanced_algorithm_evaluation.png")
        
        # æ˜¾ç¤ºå›¾è¡¨
        plt.show()
    
    def generate_recommendations(self, user_id, algorithm='best', top_k=10):
        """ä¸ºç”¨æˆ·ç”Ÿæˆæ¨è"""
        if not self.algorithms:
            print("âŒ è¯·å…ˆè®­ç»ƒç®—æ³•")
            return []
        
        # é€‰æ‹©ç®—æ³•
        if algorithm == 'best' and hasattr(self, 'evaluation_results') and not self.evaluation_results.empty:
            # ä½¿ç”¨F1åˆ†æ•°æœ€é«˜çš„ç®—æ³•
            best_algorithm_name = self.evaluation_results['f1_score'].idxmax()
            selected_algorithm = self.algorithms[best_algorithm_name]
            print(f"ğŸ¯ ä½¿ç”¨æœ€ä½³ç®—æ³•: {best_algorithm_name}")
        elif algorithm in self.algorithms:
            selected_algorithm = self.algorithms[algorithm]
            print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šç®—æ³•: {algorithm}")
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç®—æ³•
            algorithm_name = list(self.algorithms.keys())[0]
            selected_algorithm = self.algorithms[algorithm_name]
            print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤ç®—æ³•: {algorithm_name}")
        
        # ç”Ÿæˆæ¨è
        try:
            if hasattr(selected_algorithm, 'recommend'):
                recommendations = selected_algorithm.recommend(user_id, top_k=top_k)
            elif hasattr(selected_algorithm, 'recommend_for_user'):
                recommendations = selected_algorithm.recommend_for_user(user_id, top_k=top_k)
            else:
                print("âŒ ç®—æ³•ä¸æ”¯æŒæ¨èåŠŸèƒ½")
                return []
            
            print(f"âœ… ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆ {len(recommendations)} ä¸ªæ¨è")
            return recommendations
            
        except Exception as e:
            print(f"âŒ æ¨èç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def run_complete_analysis(self, sample_size=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("\nğŸš€ å¼€å§‹å®Œæ•´çš„æ¨èç³»ç»Ÿåˆ†ææµç¨‹")
        print("=" * 70)
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data(sample_size=sample_size):
            return False
        
        # 2. ç”¨æˆ·åˆ†æ
        if not self.analyze_users():
            return False
        
        # 3. è®­ç»ƒæ‰€æœ‰ç®—æ³•
        if not self.train_all_algorithms():
            return False
        
        # 4. è¯„ä¼°ç®—æ³•æ€§èƒ½
        if not self.evaluate_all_algorithms():
            return False
        
        # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_final_report()
        
        print("\nğŸ‰ å®Œæ•´åˆ†ææµç¨‹æ‰§è¡Œå®Œæˆ!")
        print("ğŸ“„ æŸ¥çœ‹ä»¥ä¸‹è¾“å‡ºæ–‡ä»¶:")
        print("   ğŸ“Š enhanced_analysis_results/ - ç”¨æˆ·åˆ†æç»“æœ")
        print("   ğŸ“‹ enhanced_evaluation_report.md - ç®—æ³•è¯„ä¼°æŠ¥å‘Š")
        print("   ğŸ“ˆ enhanced_algorithm_evaluation.png - æ€§èƒ½å¯¹æ¯”å›¾")
        print("   ğŸ“ enhanced_final_report.md - å®Œæ•´åˆ†ææŠ¥å‘Š")
        
        return True
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        with open('enhanced_final_report.md', 'w', encoding='utf-8') as f:
            f.write("# å¢å¼ºç‰ˆç”µå•†æ¨èç³»ç»Ÿåˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ•°æ®æ¦‚å†µ
            f.write("## 1. æ•°æ®æ¦‚å†µ\n\n")
            f.write(f"- **æ€»è®°å½•æ•°**: {len(self.data):,}\n")
            f.write(f"- **ç”¨æˆ·æ•°é‡**: {self.data['user_id'].nunique():,}\n")
            f.write(f"- **å•†å“æ•°é‡**: {self.data['item_id'].nunique():,}\n")
            
            if 'category_id' in self.data.columns:
                f.write(f"- **å“ç±»æ•°é‡**: {self.data['category_id'].nunique():,}\n")
            
            f.write("\n### è¡Œä¸ºåˆ†å¸ƒ\n\n")
            behavior_counts = self.data['behavior_type'].value_counts()
            for behavior, count in behavior_counts.items():
                percentage = count / len(self.data) * 100
                f.write(f"- **{behavior}**: {count:,} ({percentage:.1f}%)\n")
            
            # ç®—æ³•æ€§èƒ½
            f.write("\n## 2. ç®—æ³•æ€§èƒ½å¯¹æ¯”\n\n")
            if hasattr(self, 'evaluation_results') and not self.evaluation_results.empty:
                f.write("### ä¸»è¦æ€§èƒ½æŒ‡æ ‡\n\n")
                f.write(self.evaluation_results[
                    ['precision_mean', 'recall_mean', 'ndcg_mean', 'diversity_mean', 'coverage', 'f1_score']
                ].round(4).to_markdown())
                
                best_algorithm = self.evaluation_results['f1_score'].idxmax()
                f.write(f"\n**æœ€ä½³ç®—æ³•**: {best_algorithm}\n")
                f.write(f"**F1åˆ†æ•°**: {self.evaluation_results.loc[best_algorithm, 'f1_score']:.4f}\n")
            
            # æŠ€æœ¯ç‰¹ç‚¹
            f.write("\n## 3. æŠ€æœ¯ç‰¹ç‚¹\n\n")
            f.write("æœ¬æ¨èç³»ç»Ÿé›†æˆäº†ä»¥ä¸‹å‰æ²¿æŠ€æœ¯:\n\n")
            f.write("### ğŸ¤– æ·±åº¦å­¦ä¹ æŠ€æœ¯\n")
            f.write("- **Transformeråºåˆ—æ¨è**: ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•è·ç”¨æˆ·è¡Œä¸ºåºåˆ—æ¨¡å¼\n")
            f.write("- **å›¾ç¥ç»ç½‘ç»œ**: åŸºäºç”¨æˆ·-å•†å“äºŒéƒ¨å›¾è¿›è¡ŒååŒè¿‡æ»¤\n")
            f.write("- **å¼ºåŒ–å­¦ä¹ **: ä½¿ç”¨DQNå­¦ä¹ æ¨èç­–ç•¥\n\n")
            
            f.write("### ğŸ“Š è¯„ä¼°ä½“ç³»\n")
            f.write("- **å¤šç»´åº¦è¯„ä¼°**: Precision@K, Recall@K, NDCG@K\n")
            f.write("- **å¤šæ ·æ€§è¯„ä¼°**: æ¨èåˆ—è¡¨å¤šæ ·æ€§å’Œæ–°é¢–æ€§\n")
            f.write("- **è¦†ç›–ç‡è¯„ä¼°**: å•†å“è¦†ç›–å’Œé•¿å°¾æ¨èèƒ½åŠ›\n\n")
            
            f.write("### ğŸ‘¥ ç”¨æˆ·åˆ†æ\n")
            f.write("- **RFMåˆ†æ**: ç”¨æˆ·ä»·å€¼åˆ†ç¾¤\n")
            f.write("- **è¡Œä¸ºæ¨¡å¼æŒ–æ˜**: è½¬åŒ–æ¼æ–—å’Œåå¥½åˆ†æ\n")
            f.write("- **ç”¨æˆ·ç”»åƒ**: å¤šç»´åº¦æ ‡ç­¾ä½“ç³»\n\n")
            
            # åº”ç”¨å»ºè®®
            f.write("## 4. åº”ç”¨å»ºè®®\n\n")
            f.write("### ç®—æ³•é€‰æ‹©å»ºè®®\n")
            if hasattr(self, 'evaluation_results') and not self.evaluation_results.empty:
                for algorithm in self.evaluation_results.index:
                    precision = self.evaluation_results.loc[algorithm, 'precision_mean']
                    diversity = self.evaluation_results.loc[algorithm, 'diversity_mean']
                    
                    if precision > 0.1:
                        if diversity > 0.7:
                            scenario = "é€‚åˆé¦–é¡µæ¨èå’Œä¸ªæ€§åŒ–å‘ç°"
                        else:
                            scenario = "é€‚åˆè´­ä¹°è½¬åŒ–å’Œç²¾å‡†è¥é”€"
                    else:
                        scenario = "é€‚åˆå†·å¯åŠ¨å’Œæ¢ç´¢æ€§æ¨è"
                    
                    f.write(f"- **{algorithm}**: {scenario}\n")
            
            f.write("\n### ä¸šåŠ¡ä»·å€¼\n")
            f.write("- **ä¸ªæ€§åŒ–æ¨è**: æå‡ç”¨æˆ·ä½“éªŒå’Œè½¬åŒ–ç‡\n")
            f.write("- **ç”¨æˆ·è¿è¥**: ç²¾å‡†ç”¨æˆ·åˆ†ç¾¤å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†\n")
            f.write("- **å•†å“è¿è¥**: æ–°å“æ¨å¹¿å’Œåº“å­˜ä¼˜åŒ–\n")
            f.write("- **æ•°æ®é©±åŠ¨**: åŸºäºæ•°æ®çš„å†³ç­–æ”¯æŒ\n")
        
        print("ğŸ“ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: enhanced_final_report.md")

def demo_enhanced_system():
    """æ¼”ç¤ºå¢å¼ºç‰ˆæ¨èç³»ç»Ÿ"""
    print("ğŸ¬ å¢å¼ºç‰ˆæ¨èç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = EnhancedECommerceRecommenderSystem()
    
    # é€‰æ‹©æ•°æ®æ–‡ä»¶
    data_file = input("è¯·è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (å›è½¦ä½¿ç”¨é»˜è®¤ UserBehavior.csv): ").strip()
    if not data_file:
        data_file = "UserBehavior.csv"
    
    # é€‰æ‹©é‡‡æ ·å¤§å°
    sample_input = input("è¯·è¾“å…¥é‡‡æ ·å¤§å° (å›è½¦ä½¿ç”¨å…¨é‡æ•°æ®ï¼Œå»ºè®®æµ‹è¯•æ—¶ä½¿ç”¨ 100000): ").strip()
    sample_size = None
    if sample_input.isdigit():
        sample_size = int(sample_input)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    success = system.run_complete_analysis(sample_size=sample_size)
    
    if success:
        print("\nğŸ¯ æ¼”ç¤ºæ¨èç”Ÿæˆ:")
        
        # è·å–ä¸€äº›ç”¨æˆ·IDè¿›è¡Œæ¨èæ¼”ç¤º
        sample_users = system.data['user_id'].unique()[:5]
        
        for user_id in sample_users:
            print(f"\nğŸ‘¤ ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆæ¨è:")
            recommendations = system.generate_recommendations(user_id, top_k=5)
            
            if recommendations:
                for i, item_id in enumerate(recommendations, 1):
                    print(f"   {i}. å•†å“ {item_id}")
            else:
                print("   æš‚æ— æ¨è")
    
    return system

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸ›’ å¢å¼ºç‰ˆç”µå•†ç”¨æˆ·è¡Œä¸ºæ¨èç³»ç»Ÿ")
    print("=" * 50)
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿæ¼”ç¤º (é‡‡æ ·æ•°æ®)")
    print("2. å®Œæ•´åˆ†æ (å…¨é‡æ•°æ®)")
    print("3. è‡ªå®šä¹‰åˆ†æ")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
    
    if choice == "1":
        system = EnhancedECommerceRecommenderSystem()
        system.run_complete_analysis(sample_size=50000)  # 5ä¸‡æ¡è®°å½•å¿«é€Ÿæµ‹è¯•
        
    elif choice == "2":
        system = EnhancedECommerceRecommenderSystem()
        system.run_complete_analysis()  # å…¨é‡æ•°æ®åˆ†æ
        
    elif choice == "3":
        demo_enhanced_system()
        
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main() 