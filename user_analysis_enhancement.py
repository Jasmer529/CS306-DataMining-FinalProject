"""
ç”¨æˆ·è¡Œä¸ºåˆ†æå¢å¼ºæ¨¡å—
åŒ…å«ç”¨æˆ·åˆ†ç¾¤ã€RFMåˆ†æã€ç‰¹å¾å·¥ç¨‹ç­‰åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class UserAnalysisEnhancer:
    """ç”¨æˆ·è¡Œä¸ºæ·±åº¦åˆ†æç±»"""
    
    def __init__(self, data):
        """
        åˆå§‹åŒ–
        :param data: ç”¨æˆ·è¡Œä¸ºæ•°æ®DataFrame
        """
        self.data = data.copy()
        self.user_features = None
        self.rfm_data = None
        self.user_clusters = None
        
    def calculate_rfm_features(self):
        """è®¡ç®—RFMç‰¹å¾"""
        print("ğŸ” è®¡ç®—RFMç‰¹å¾...")
        
        # è·å–åˆ†ææ—¶é—´ç‚¹ï¼ˆæ•°æ®æœ€åä¸€å¤©çš„ä¸‹ä¸€å¤©ï¼‰
        max_date = self.data['datetime'].max()
        analysis_date = max_date + timedelta(days=1)
        
        # è®¡ç®—RFMæŒ‡æ ‡
        rfm_data = self.data.groupby('user_id').agg({
            'datetime': lambda x: (analysis_date - x.max()).days,  # Recency
            'behavior_type': 'count',  # Frequency
            'item_id': 'nunique'  # æµè§ˆå•†å“æ•°é‡
        }).rename(columns={
            'datetime': 'recency',
            'behavior_type': 'frequency', 
            'item_id': 'item_variety'
        })
        
        # è®¡ç®—è´­ä¹°é‡‘é¢ï¼ˆè´§å¸ä»·å€¼ï¼‰- è¿™é‡Œç”¨è´­ä¹°æ¬¡æ•°ä»£æ›¿
        purchase_data = self.data[self.data['behavior_type'] == 'buy'].groupby('user_id').agg({
            'behavior_type': 'count'
        }).rename(columns={'behavior_type': 'monetary'})
        
        # åˆå¹¶RFMæ•°æ®
        rfm_data = rfm_data.join(purchase_data, how='left')
        rfm_data['monetary'] = rfm_data['monetary'].fillna(0)
        
        self.rfm_data = rfm_data
        print(f"âœ… RFMç‰¹å¾è®¡ç®—å®Œæˆï¼Œç”¨æˆ·æ•°é‡: {len(rfm_data)}")
        return rfm_data
    
    def calculate_advanced_features(self):
        """è®¡ç®—é«˜çº§ç”¨æˆ·ç‰¹å¾"""
        print("ğŸ” è®¡ç®—é«˜çº§ç”¨æˆ·ç‰¹å¾...")
        
        features_list = []
        
        for user_id in self.data['user_id'].unique():
            user_data = self.data[self.data['user_id'] == user_id]
            
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            total_actions = len(user_data)
            unique_items = user_data['item_id'].nunique()
            unique_categories = user_data['category_id'].nunique()
            
            # è¡Œä¸ºç±»å‹åˆ†å¸ƒ
            behavior_counts = user_data['behavior_type'].value_counts()
            pv_count = behavior_counts.get('pv', 0)
            cart_count = behavior_counts.get('cart', 0)
            fav_count = behavior_counts.get('fav', 0)
            buy_count = behavior_counts.get('buy', 0)
            
            # è½¬åŒ–ç‡è®¡ç®—
            pv_to_cart_rate = cart_count / pv_count if pv_count > 0 else 0
            pv_to_buy_rate = buy_count / pv_count if pv_count > 0 else 0
            cart_to_buy_rate = buy_count / cart_count if cart_count > 0 else 0
            
            # æ—¶é—´ç‰¹å¾
            time_span = (user_data['datetime'].max() - user_data['datetime'].min()).days
            avg_daily_actions = total_actions / max(time_span, 1)
            
            # æ´»è·ƒæ—¶æ®µåˆ†æ
            hour_counts = user_data['hour'].value_counts()
            most_active_hour = hour_counts.index[0] if not hour_counts.empty else 0
            
            # å“ç±»é›†ä¸­åº¦ (åŸºå°¼ç³»æ•°)
            category_dist = user_data['category_id'].value_counts(normalize=True)
            category_concentration = 1 - (category_dist ** 2).sum()
            
            # ä¼šè¯åˆ†æï¼ˆç›¸é‚»æ“ä½œæ—¶é—´é—´éš”è¶…è¿‡30åˆ†é’Ÿè®¤ä¸ºæ˜¯æ–°ä¼šè¯ï¼‰
            user_data_sorted = user_data.sort_values('datetime')
            time_diffs = user_data_sorted['datetime'].diff()
            session_breaks = (time_diffs > timedelta(minutes=30)).sum()
            avg_session_length = total_actions / max(session_breaks + 1, 1)
            
            features = {
                'user_id': user_id,
                'total_actions': total_actions,
                'unique_items': unique_items,
                'unique_categories': unique_categories,
                'pv_count': pv_count,
                'cart_count': cart_count,
                'fav_count': fav_count,
                'buy_count': buy_count,
                'pv_to_cart_rate': pv_to_cart_rate,
                'pv_to_buy_rate': pv_to_buy_rate,
                'cart_to_buy_rate': cart_to_buy_rate,
                'time_span_days': time_span,
                'avg_daily_actions': avg_daily_actions,
                'most_active_hour': most_active_hour,
                'category_concentration': category_concentration,
                'avg_session_length': avg_session_length,
                'session_count': session_breaks + 1
            }
            
            features_list.append(features)
        
        self.user_features = pd.DataFrame(features_list)
        print(f"âœ… é«˜çº§ç‰¹å¾è®¡ç®—å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(self.user_features.columns)-1}")
        return self.user_features
    
    def rfm_segmentation(self):
        """åŸºäºRFMè¿›è¡Œç”¨æˆ·åˆ†ç¾¤"""
        if self.rfm_data is None:
            self.calculate_rfm_features()
            
        print("ğŸ¯ è¿›è¡ŒRFMç”¨æˆ·åˆ†ç¾¤...")
        
        # RFMæ‰“åˆ† (1-5åˆ†)
        rfm_scores = self.rfm_data.copy()
        
        # Recency: è¶Šå°è¶Šå¥½ (æœ€è¿‘è´­ä¹°)
        rfm_scores['R_score'] = pd.qcut(rfm_scores['recency'].rank(method='first'), 
                                       5, labels=[5,4,3,2,1])
        
        # Frequency: è¶Šå¤§è¶Šå¥½
        rfm_scores['F_score'] = pd.qcut(rfm_scores['frequency'].rank(method='first'), 
                                       5, labels=[1,2,3,4,5])
        
        # Monetary: è¶Šå¤§è¶Šå¥½
        rfm_scores['M_score'] = pd.qcut(rfm_scores['monetary'].rank(method='first'), 
                                       5, labels=[1,2,3,4,5])
        
        # åˆæˆRFMå¾—åˆ†
        rfm_scores['RFM_score'] = (rfm_scores['R_score'].astype(str) + 
                                  rfm_scores['F_score'].astype(str) + 
                                  rfm_scores['M_score'].astype(str))
        
        # ç”¨æˆ·åˆ†ç¾¤è§„åˆ™
        def segment_users(row):
            score = row['RFM_score']
            r, f, m = int(score[0]), int(score[1]), int(score[2])
            
            if r >= 4 and f >= 4 and m >= 4:
                return "å† å†›ç”¨æˆ·"
            elif r >= 3 and f >= 3 and m >= 3:
                return "å¿ è¯šç”¨æˆ·"
            elif r >= 4 and f <= 2:
                return "æ–°ç”¨æˆ·"
            elif r <= 2 and f >= 3 and m >= 3:
                return "æµå¤±é£é™©ç”¨æˆ·"
            elif r <= 2 and f <= 2:
                return "å·²æµå¤±ç”¨æˆ·"
            elif f >= 4 and m <= 2:
                return "æ½œåŠ›ç”¨æˆ·" 
            else:
                return "ä¸€èˆ¬ç”¨æˆ·"
        
        rfm_scores['segment'] = rfm_scores.apply(segment_users, axis=1)
        
        # ç»Ÿè®¡å„åˆ†ç¾¤
        segment_stats = rfm_scores['segment'].value_counts()
        print("\nğŸ“Š ç”¨æˆ·åˆ†ç¾¤ç»“æœ:")
        for segment, count in segment_stats.items():
            percentage = count / len(rfm_scores) * 100
            print(f"  {segment}: {count:,} ç”¨æˆ· ({percentage:.1f}%)")
        
        self.rfm_scores = rfm_scores
        return rfm_scores
    
    def kmeans_clustering(self, n_clusters=None):
        """åŸºäºç”¨æˆ·ç‰¹å¾è¿›è¡ŒK-meansèšç±»"""
        if self.user_features is None:
            self.calculate_advanced_features()
            
        print("ğŸ¯ è¿›è¡ŒK-meansç”¨æˆ·èšç±»...")
        
        # é€‰æ‹©èšç±»ç‰¹å¾
        cluster_features = [
            'total_actions', 'unique_items', 'unique_categories',
            'pv_to_cart_rate', 'pv_to_buy_rate', 'avg_daily_actions',
            'category_concentration', 'avg_session_length'
        ]
        
        X = self.user_features[cluster_features].fillna(0)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ç¡®å®šæœ€ä¼˜èšç±»æ•°
        if n_clusters is None:
            silhouette_scores = []
            K_range = range(2, 11)
            
            for k in K_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(X_scaled)
                silhouette_avg = silhouette_score(X_scaled, cluster_labels)
                silhouette_scores.append(silhouette_avg)
            
            # é€‰æ‹©æœ€ä¼˜Kå€¼
            optimal_k = K_range[np.argmax(silhouette_scores)]
            print(f"ğŸ“ˆ æœ€ä¼˜èšç±»æ•°: {optimal_k} (è½®å»“ç³»æ•°: {max(silhouette_scores):.3f})")
        else:
            optimal_k = n_clusters
        
        # æ‰§è¡Œèšç±»
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        # æ·»åŠ èšç±»ç»“æœ
        self.user_features['cluster'] = cluster_labels
        
        # åˆ†æå„èšç±»ç‰¹å¾
        print("\nğŸ“Š èšç±»ç»“æœåˆ†æ:")
        for cluster_id in range(optimal_k):
            cluster_data = self.user_features[self.user_features['cluster'] == cluster_id]
            size = len(cluster_data)
            avg_actions = cluster_data['total_actions'].mean()
            avg_buy_rate = cluster_data['pv_to_buy_rate'].mean()
            
            print(f"  èšç±» {cluster_id}: {size:,} ç”¨æˆ· | "
                  f"å¹³å‡è¡Œä¸ºæ•°: {avg_actions:.1f} | "
                  f"å¹³å‡è´­ä¹°è½¬åŒ–ç‡: {avg_buy_rate:.3f}")
        
        self.cluster_model = kmeans
        self.scaler = scaler
        return self.user_features
    
    def visualize_user_segments(self):
        """å¯è§†åŒ–ç”¨æˆ·åˆ†ç¾¤ç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ç”¨æˆ·åˆ†ç¾¤åˆ†æå¯è§†åŒ–', fontsize=16, fontweight='bold')
        
        # 1. RFMåˆ†ç¾¤åˆ†å¸ƒ
        if hasattr(self, 'rfm_scores'):
            ax1 = axes[0, 0]
            segment_counts = self.rfm_scores['segment'].value_counts()
            ax1.pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%')
            ax1.set_title('RFMç”¨æˆ·åˆ†ç¾¤åˆ†å¸ƒ')
        
        # 2. èšç±»ç‰¹å¾åˆ†å¸ƒ
        if 'cluster' in self.user_features.columns:
            ax2 = axes[0, 1]
            self.user_features['cluster'].value_counts().plot(kind='bar', ax=ax2)
            ax2.set_title('K-meansèšç±»åˆ†å¸ƒ')
            ax2.set_xlabel('èšç±»ID')
            ax2.set_ylabel('ç”¨æˆ·æ•°é‡')
        
        # 3. ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ
        ax3 = axes[1, 0]
        self.user_features['total_actions'].hist(bins=50, ax=ax3, alpha=0.7)
        ax3.set_title('ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ')
        ax3.set_xlabel('æ€»è¡Œä¸ºæ•°é‡')
        ax3.set_ylabel('ç”¨æˆ·æ•°é‡')
        
        # 4. è´­ä¹°è½¬åŒ–ç‡åˆ†å¸ƒ
        ax4 = axes[1, 1]
        self.user_features['pv_to_buy_rate'].hist(bins=50, ax=ax4, alpha=0.7)
        ax4.set_title('è´­ä¹°è½¬åŒ–ç‡åˆ†å¸ƒ')
        ax4.set_xlabel('è½¬åŒ–ç‡')
        ax4.set_ylabel('ç”¨æˆ·æ•°é‡')
        
        plt.tight_layout()
        plt.show()
    
    def generate_user_profiles(self):
        """ç”Ÿæˆç”¨æˆ·ç”»åƒæŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç”¨æˆ·ç”»åƒæŠ¥å‘Š...")
        
        profiles = {}
        
        # RFMåˆ†ç¾¤ç”»åƒ
        if hasattr(self, 'rfm_scores'):
            for segment in self.rfm_scores['segment'].unique():
                segment_data = self.rfm_scores[self.rfm_scores['segment'] == segment]
                
                profile = {
                    'ç”¨æˆ·æ•°é‡': len(segment_data),
                    'å¹³å‡æœ€è¿‘æ€§': segment_data['recency'].mean(),
                    'å¹³å‡é¢‘ç‡': segment_data['frequency'].mean(),
                    'å¹³å‡è´­ä¹°æ¬¡æ•°': segment_data['monetary'].mean(),
                    'å æ¯”': len(segment_data) / len(self.rfm_scores) * 100
                }
                
                profiles[segment] = profile
        
        return profiles
    
    def save_analysis_results(self, output_dir='analysis_results'):
        """ä¿å­˜åˆ†æç»“æœ"""
        import os
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ä¿å­˜ç”¨æˆ·ç‰¹å¾
        if self.user_features is not None:
            self.user_features.to_csv(f'{output_dir}/user_features.csv', index=False)
            print(f"âœ… ç”¨æˆ·ç‰¹å¾å·²ä¿å­˜åˆ° {output_dir}/user_features.csv")
        
        # ä¿å­˜RFMåˆ†æç»“æœ
        if hasattr(self, 'rfm_scores'):
            self.rfm_scores.to_csv(f'{output_dir}/rfm_analysis.csv', index=False)
            print(f"âœ… RFMåˆ†æç»“æœå·²ä¿å­˜åˆ° {output_dir}/rfm_analysis.csv")
        
        # ä¿å­˜ç”¨æˆ·ç”»åƒ
        profiles = self.generate_user_profiles()
        if profiles:
            profile_df = pd.DataFrame(profiles).T
            profile_df.to_csv(f'{output_dir}/user_profiles.csv')
            print(f"âœ… ç”¨æˆ·ç”»åƒå·²ä¿å­˜åˆ° {output_dir}/user_profiles.csv")

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸš€ ç”¨æˆ·è¡Œä¸ºåˆ†æå¢å¼ºæ¨¡å—")
    print("=" * 50)
    
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # 1. åŠ è½½æ•°æ®
    data = pd.read_csv('UserBehavior.csv')
    
    # 2. åˆå§‹åŒ–åˆ†æå™¨
    analyzer = UserAnalysisEnhancer(data)
    
    # 3. è®¡ç®—RFMç‰¹å¾
    rfm_data = analyzer.calculate_rfm_features()
    
    # 4. è®¡ç®—é«˜çº§ç‰¹å¾
    user_features = analyzer.calculate_advanced_features()
    
    # 5. RFMåˆ†ç¾¤
    rfm_segments = analyzer.rfm_segmentation()
    
    # 6. K-meansèšç±»
    clusters = analyzer.kmeans_clustering()
    
    # 7. å¯è§†åŒ–ç»“æœ
    analyzer.visualize_user_segments()
    
    # 8. ä¿å­˜åˆ†æç»“æœ
    analyzer.save_analysis_results()
    """)

if __name__ == "__main__":
    main() 