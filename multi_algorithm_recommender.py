"""
å¤šç®—æ³•æ¨èç³»ç»Ÿæ¨¡å—
åŒ…å«ååŒè¿‡æ»¤ã€çŸ©é˜µåˆ†è§£ã€æ·±åº¦å­¦ä¹ ç­‰å¤šç§æ¨èç®—æ³•çš„å®ç°å’Œæ¯”è¾ƒ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import NMF
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class CollaborativeFiltering:
    """ååŒè¿‡æ»¤æ¨èç®—æ³•"""
    
    def __init__(self, method='user_based'):
        """
        åˆå§‹åŒ–ååŒè¿‡æ»¤
        :param method: 'user_based' æˆ– 'item_based'
        """
        self.method = method
        self.user_item_matrix = None
        self.similarity_matrix = None
        
    def prepare_data(self, data):
        """å‡†å¤‡ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ"""
        print(f"ğŸ” å‡†å¤‡{self.method}ååŒè¿‡æ»¤æ•°æ®...")
        
        # åˆ›å»ºç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µ (ç”¨è¡Œä¸ºæ¬¡æ•°ä½œä¸ºè¯„åˆ†)
        interaction_counts = data.groupby(['user_id', 'item_id']).size().reset_index(name='rating')
        
        # åˆ›å»ºè¯„åˆ†çŸ©é˜µ
        self.user_item_matrix = interaction_counts.pivot(
            index='user_id', 
            columns='item_id', 
            values='rating'
        ).fillna(0)
        
        print(f"âœ… è¯„åˆ†çŸ©é˜µæ„å»ºå®Œæˆ: {self.user_item_matrix.shape}")
        return self.user_item_matrix
    
    def calculate_similarity(self):
        """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
        print(f"ğŸ” è®¡ç®—{self.method}ç›¸ä¼¼åº¦çŸ©é˜µ...")
        
        if self.method == 'user_based':
            # ç”¨æˆ·ç›¸ä¼¼åº¦
            self.similarity_matrix = cosine_similarity(self.user_item_matrix)
            self.similarity_df = pd.DataFrame(
                self.similarity_matrix,
                index=self.user_item_matrix.index,
                columns=self.user_item_matrix.index
            )
        else:
            # ç‰©å“ç›¸ä¼¼åº¦
            self.similarity_matrix = cosine_similarity(self.user_item_matrix.T)
            self.similarity_df = pd.DataFrame(
                self.similarity_matrix,
                index=self.user_item_matrix.columns,
                columns=self.user_item_matrix.columns
            )
        
        print(f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ: {self.similarity_matrix.shape}")
        return self.similarity_matrix
    
    def recommend(self, user_id, top_k=10):
        """ä¸ºæŒ‡å®šç”¨æˆ·æ¨èç‰©å“"""
        if self.similarity_matrix is None:
            self.calculate_similarity()
            
        if user_id not in self.user_item_matrix.index:
            return []
        
        if self.method == 'user_based':
            return self._user_based_recommend(user_id, top_k)
        else:
            return self._item_based_recommend(user_id, top_k)
    
    def _user_based_recommend(self, user_id, top_k):
        """åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤æ¨è"""
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        user_similarities = self.similarity_df.iloc[user_idx].drop(user_id)
        
        # è·å–ç›¸ä¼¼ç”¨æˆ·
        similar_users = user_similarities.nlargest(50).index
        
        # è®¡ç®—æ¨èåˆ†æ•°
        recommendations = {}
        user_items = set(self.user_item_matrix.loc[user_id][self.user_item_matrix.loc[user_id] > 0].index)
        
        for item in self.user_item_matrix.columns:
            if item not in user_items:  # åªæ¨èç”¨æˆ·æœªäº¤äº’è¿‡çš„ç‰©å“
                score = 0
                sim_sum = 0
                
                for similar_user in similar_users:
                    if self.user_item_matrix.loc[similar_user, item] > 0:
                        similarity = user_similarities[similar_user]
                        rating = self.user_item_matrix.loc[similar_user, item]
                        score += similarity * rating
                        sim_sum += abs(similarity)
                
                if sim_sum > 0:
                    recommendations[item] = score / sim_sum
        
        # è¿”å›top-kæ¨è
        sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
        return [item for item, score in sorted_recs[:top_k]]
    
    def _item_based_recommend(self, user_id, top_k):
        """åŸºäºç‰©å“çš„ååŒè¿‡æ»¤æ¨è"""
        user_items = self.user_item_matrix.loc[user_id]
        user_interacted_items = user_items[user_items > 0].index
        
        recommendations = {}
        
        for item in self.user_item_matrix.columns:
            if item not in user_interacted_items:
                score = 0
                sim_sum = 0
                
                for interacted_item in user_interacted_items:
                    if item in self.similarity_df.index and interacted_item in self.similarity_df.columns:
                        similarity = self.similarity_df.loc[item, interacted_item]
                        rating = user_items[interacted_item]
                        score += similarity * rating
                        sim_sum += abs(similarity)
                
                if sim_sum > 0:
                    recommendations[item] = score / sim_sum
        
        sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
        return [item for item, score in sorted_recs[:top_k]]

class MatrixFactorization:
    """çŸ©é˜µåˆ†è§£æ¨èç®—æ³•"""
    
    def __init__(self, n_factors=50, learning_rate=0.01, regularization=0.01, n_epochs=100):
        self.n_factors = n_factors
        self.learning_rate = learning_rate
        self.regularization = regularization
        self.n_epochs = n_epochs
        self.user_factors = None
        self.item_factors = None
        
    def fit(self, user_item_matrix):
        """è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹"""
        print("ğŸ” è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹...")
        
        n_users, n_items = user_item_matrix.shape
        
        # åˆå§‹åŒ–ç”¨æˆ·å’Œç‰©å“å› å­çŸ©é˜µ
        self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))
        self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))
        
        # è·å–éé›¶è¯„åˆ†çš„ä½ç½®
        user_indices, item_indices = np.nonzero(user_item_matrix.values)
        ratings = user_item_matrix.values[user_indices, item_indices]
        
        # è®­ç»ƒè¿‡ç¨‹
        losses = []
        for epoch in range(self.n_epochs):
            epoch_loss = 0
            
            for i, (user_idx, item_idx, rating) in enumerate(zip(user_indices, item_indices, ratings)):
                # é¢„æµ‹è¯„åˆ†
                prediction = np.dot(self.user_factors[user_idx], self.item_factors[item_idx])
                error = rating - prediction
                
                # æ›´æ–°å› å­
                user_factor = self.user_factors[user_idx].copy()
                item_factor = self.item_factors[item_idx].copy()
                
                self.user_factors[user_idx] += self.learning_rate * (
                    error * item_factor - self.regularization * user_factor
                )
                self.item_factors[item_idx] += self.learning_rate * (
                    error * user_factor - self.regularization * item_factor
                )
                
                epoch_loss += error ** 2
            
            losses.append(epoch_loss / len(ratings))
            
            if epoch % 20 == 0:
                print(f"  Epoch {epoch}: Loss = {losses[-1]:.4f}")
        
        print(f"âœ… çŸ©é˜µåˆ†è§£è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        return losses
    
    def predict(self, user_idx, item_idx):
        """é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ†"""
        return np.dot(self.user_factors[user_idx], self.item_factors[item_idx])
    
    def recommend(self, user_idx, user_item_matrix, top_k=10):
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        user_items = user_item_matrix.iloc[user_idx]
        interacted_items = set(user_items[user_items > 0].index)
        
        recommendations = {}
        for item_idx, item_id in enumerate(user_item_matrix.columns):
            if item_id not in interacted_items:
                score = self.predict(user_idx, item_idx)
                recommendations[item_id] = score
        
        sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
        return [item for item, score in sorted_recs[:top_k]]

class NeuralCollaborativeFiltering(nn.Module):
    """ç¥ç»ååŒè¿‡æ»¤æ¨¡å‹"""
    
    def __init__(self, n_users, n_items, embedding_dim=50, hidden_dims=[128, 64]):
        super().__init__()
        self.n_users = n_users
        self.n_items = n_items
        self.embedding_dim = embedding_dim
        
        # åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)
        
        # MLPå±‚
        mlp_input_dim = embedding_dim * 2
        self.mlp_layers = nn.ModuleList()
        
        for i, hidden_dim in enumerate(hidden_dims):
            if i == 0:
                self.mlp_layers.append(nn.Linear(mlp_input_dim, hidden_dim))
            else:
                self.mlp_layers.append(nn.Linear(hidden_dims[i-1], hidden_dim))
            self.mlp_layers.append(nn.ReLU())
            self.mlp_layers.append(nn.Dropout(0.2))
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_dims[-1], 1)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.item_embedding.weight, std=0.01)
        
        for layer in self.mlp_layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, user_ids, item_ids):
        """å‰å‘ä¼ æ’­"""
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # æ‹¼æ¥åµŒå…¥å‘é‡
        mlp_input = torch.cat([user_emb, item_emb], dim=1)
        
        # MLPå‰å‘ä¼ æ’­
        x = mlp_input
        for layer in self.mlp_layers:
            x = layer(x)
        
        output = self.output_layer(x)
        return output.squeeze()

class DeepRecommenderSystem:
    """æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿ"""
    
    def __init__(self, embedding_dim=50, hidden_dims=[128, 64], learning_rate=0.001):
        self.embedding_dim = embedding_dim
        self.hidden_dims = hidden_dims
        self.learning_rate = learning_rate
        self.model = None
        self.user_to_idx = None
        self.item_to_idx = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def prepare_data(self, data):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ” å‡†å¤‡æ·±åº¦å­¦ä¹ è®­ç»ƒæ•°æ®...")
        
        # åˆ›å»ºç”¨æˆ·å’Œç‰©å“çš„ç´¢å¼•æ˜ å°„
        unique_users = data['user_id'].unique()
        unique_items = data['item_id'].unique()
        
        self.user_to_idx = {user: idx for idx, user in enumerate(unique_users)}
        self.item_to_idx = {item: idx for idx, item in enumerate(unique_items)}
        self.idx_to_user = {idx: user for user, idx in self.user_to_idx.items()}
        self.idx_to_item = {idx: item for item, idx in self.item_to_idx.items()}
        
        # åˆ›å»ºäº¤äº’æ•°æ®
        interaction_data = data.groupby(['user_id', 'item_id']).size().reset_index(name='rating')
        
        # è½¬æ¢ä¸ºç´¢å¼•
        interaction_data['user_idx'] = interaction_data['user_id'].map(self.user_to_idx)
        interaction_data['item_idx'] = interaction_data['item_id'].map(self.item_to_idx)
        
        # æ ‡å‡†åŒ–è¯„åˆ†
        max_rating = interaction_data['rating'].max()
        interaction_data['rating'] = interaction_data['rating'] / max_rating
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(interaction_data)} æ¡äº¤äº’è®°å½•")
        return interaction_data
    
    def train(self, interaction_data, epochs=50, batch_size=1024):
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("ğŸ” è®­ç»ƒç¥ç»ååŒè¿‡æ»¤æ¨¡å‹...")
        
        n_users = len(self.user_to_idx)
        n_items = len(self.item_to_idx)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = NeuralCollaborativeFiltering(
            n_users, n_items, self.embedding_dim, self.hidden_dims
        ).to(self.device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        dataset = RecommenderDataset(interaction_data)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # è®­ç»ƒå¾ªç¯
        losses = []
        self.model.train()
        
        for epoch in range(epochs):
            epoch_loss = 0
            batch_count = 0
            
            for batch_users, batch_items, batch_ratings in dataloader:
                batch_users = batch_users.to(self.device)
                batch_items = batch_items.to(self.device)
                batch_ratings = batch_ratings.to(self.device)
                
                optimizer.zero_grad()
                
                predictions = self.model(batch_users, batch_items)
                loss = criterion(predictions, batch_ratings)
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_loss = epoch_loss / batch_count
            losses.append(avg_loss)
            
            if epoch % 10 == 0:
                print(f"  Epoch {epoch}: Loss = {avg_loss:.4f}")
        
        print(f"âœ… ç¥ç»ç½‘ç»œè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        return losses
    
    def recommend(self, user_id, interaction_data, top_k=10):
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        if user_id not in self.user_to_idx:
            return []
        
        user_idx = self.user_to_idx[user_id]
        
        # è·å–ç”¨æˆ·å·²äº¤äº’çš„ç‰©å“
        user_interactions = interaction_data[interaction_data['user_id'] == user_id]
        interacted_items = set(user_interactions['item_id'])
        
        # é¢„æµ‹æ‰€æœ‰æœªäº¤äº’ç‰©å“çš„è¯„åˆ†
        recommendations = {}
        self.model.eval()
        
        with torch.no_grad():
            for item_id, item_idx in self.item_to_idx.items():
                if item_id not in interacted_items:
                    user_tensor = torch.tensor([user_idx]).to(self.device)
                    item_tensor = torch.tensor([item_idx]).to(self.device)
                    
                    score = self.model(user_tensor, item_tensor).item()
                    recommendations[item_id] = score
        
        # è¿”å›top-kæ¨è
        sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
        return [item for item, score in sorted_recs[:top_k]]

class RecommenderDataset(Dataset):
    """æ¨èç³»ç»Ÿæ•°æ®é›†"""
    
    def __init__(self, interaction_data):
        self.user_indices = torch.tensor(interaction_data['user_idx'].values, dtype=torch.long)
        self.item_indices = torch.tensor(interaction_data['item_idx'].values, dtype=torch.long)
        self.ratings = torch.tensor(interaction_data['rating'].values, dtype=torch.float32)
    
    def __len__(self):
        return len(self.user_indices)
    
    def __getitem__(self, idx):
        return self.user_indices[idx], self.item_indices[idx], self.ratings[idx]

class MultiAlgorithmRecommender:
    """å¤šç®—æ³•æ¨èç³»ç»Ÿé›†æˆ"""
    
    def __init__(self):
        self.algorithms = {}
        self.evaluation_results = {}
        
    def add_algorithm(self, name, algorithm):
        """æ·»åŠ æ¨èç®—æ³•"""
        self.algorithms[name] = algorithm
        print(f"âœ… å·²æ·»åŠ ç®—æ³•: {name}")
    
    def train_all(self, data):
        """è®­ç»ƒæ‰€æœ‰ç®—æ³•"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨èç®—æ³•...")
        
        for name, algorithm in self.algorithms.items():
            print(f"\nğŸ“ˆ è®­ç»ƒç®—æ³•: {name}")
            
            if isinstance(algorithm, CollaborativeFiltering):
                algorithm.prepare_data(data)
                algorithm.calculate_similarity()
                
            elif isinstance(algorithm, MatrixFactorization):
                # å‡†å¤‡ç”¨æˆ·-ç‰©å“çŸ©é˜µ
                interaction_counts = data.groupby(['user_id', 'item_id']).size().reset_index(name='rating')
                user_item_matrix = interaction_counts.pivot(
                    index='user_id', columns='item_id', values='rating'
                ).fillna(0)
                algorithm.fit(user_item_matrix)
                
            elif isinstance(algorithm, DeepRecommenderSystem):
                interaction_data = algorithm.prepare_data(data)
                algorithm.train(interaction_data)
        
        print("\nğŸ‰ æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆ!")
    
    def evaluate_algorithms(self, data, test_users=None, top_k=10):
        """è¯„ä¼°æ‰€æœ‰ç®—æ³•"""
        print("ğŸ“Š å¼€å§‹è¯„ä¼°æ¨èç®—æ³•...")
        
        if test_users is None:
            # éšæœºé€‰æ‹©æµ‹è¯•ç”¨æˆ·
            all_users = data['user_id'].unique()
            test_users = np.random.choice(all_users, size=min(100, len(all_users)), replace=False)
        
        for name, algorithm in self.algorithms.items():
            print(f"\nğŸ“Š è¯„ä¼°ç®—æ³•: {name}")
            
            recommendations_count = 0
            total_users = len(test_users)
            
            for user_id in test_users:
                try:
                    if isinstance(algorithm, CollaborativeFiltering):
                        recs = algorithm.recommend(user_id, top_k)
                    elif isinstance(algorithm, MatrixFactorization):
                        # éœ€è¦å‡†å¤‡çŸ©é˜µå’Œç”¨æˆ·ç´¢å¼•
                        interaction_counts = data.groupby(['user_id', 'item_id']).size().reset_index(name='rating')
                        user_item_matrix = interaction_counts.pivot(
                            index='user_id', columns='item_id', values='rating'
                        ).fillna(0)
                        if user_id in user_item_matrix.index:
                            user_idx = user_item_matrix.index.get_loc(user_id)
                            recs = algorithm.recommend(user_idx, user_item_matrix, top_k)
                        else:
                            recs = []
                    elif isinstance(algorithm, DeepRecommenderSystem):
                        interaction_data = algorithm.prepare_data(data)
                        recs = algorithm.recommend(user_id, interaction_data, top_k)
                    
                    if len(recs) > 0:
                        recommendations_count += 1
                        
                except Exception as e:
                    continue
            
            coverage = recommendations_count / total_users
            self.evaluation_results[name] = {
                'coverage': coverage,
                'avg_recommendations': recommendations_count
            }
            
            print(f"  æ¨èè¦†ç›–ç‡: {coverage:.2%}")
            print(f"  æˆåŠŸæ¨èç”¨æˆ·æ•°: {recommendations_count}/{total_users}")
    
    def generate_recommendations(self, user_id, top_k=10):
        """ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆå¤šç®—æ³•æ¨èç»“æœ"""
        results = {}
        
        for name, algorithm in self.algorithms.items():
            try:
                if isinstance(algorithm, CollaborativeFiltering):
                    recs = algorithm.recommend(user_id, top_k)
                elif isinstance(algorithm, DeepRecommenderSystem):
                    # éœ€è¦é‡æ–°å‡†å¤‡æ•°æ®ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                    recs = []
                else:
                    recs = []
                
                results[name] = recs
            except Exception as e:
                results[name] = []
        
        return results
    
    def visualize_performance(self):
        """å¯è§†åŒ–ç®—æ³•æ€§èƒ½æ¯”è¾ƒ"""
        if not self.evaluation_results:
            print("âŒ è¯·å…ˆè¿è¡Œç®—æ³•è¯„ä¼°")
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('æ¨èç®—æ³•æ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
        
        algorithms = list(self.evaluation_results.keys())
        coverage_scores = [self.evaluation_results[alg]['coverage'] for alg in algorithms]
        recommendation_counts = [self.evaluation_results[alg]['avg_recommendations'] for alg in algorithms]
        
        # æ¨èè¦†ç›–ç‡æ¯”è¾ƒ
        axes[0].bar(algorithms, coverage_scores, color='skyblue')
        axes[0].set_title('æ¨èè¦†ç›–ç‡æ¯”è¾ƒ')
        axes[0].set_ylabel('è¦†ç›–ç‡')
        axes[0].set_ylim(0, 1)
        
        # æˆåŠŸæ¨èæ•°æ¯”è¾ƒ
        axes[1].bar(algorithms, recommendation_counts, color='lightgreen')
        axes[1].set_title('æˆåŠŸæ¨èç”¨æˆ·æ•°')
        axes[1].set_ylabel('ç”¨æˆ·æ•°')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸš€ å¤šç®—æ³•æ¨èç³»ç»Ÿ")
    print("=" * 50)
    
    print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # 1. åˆå§‹åŒ–å¤šç®—æ³•æ¨èå™¨
    multi_recommender = MultiAlgorithmRecommender()
    
    # 2. æ·»åŠ ä¸åŒç®—æ³•
    multi_recommender.add_algorithm('ç”¨æˆ·ååŒè¿‡æ»¤', CollaborativeFiltering('user_based'))
    multi_recommender.add_algorithm('ç‰©å“ååŒè¿‡æ»¤', CollaborativeFiltering('item_based'))
    multi_recommender.add_algorithm('çŸ©é˜µåˆ†è§£', MatrixFactorization())
    multi_recommender.add_algorithm('ç¥ç»ååŒè¿‡æ»¤', DeepRecommenderSystem())
    
    # 3. è®­ç»ƒæ‰€æœ‰ç®—æ³•
    multi_recommender.train_all(data)
    
    # 4. è¯„ä¼°ç®—æ³•æ€§èƒ½
    multi_recommender.evaluate_algorithms(data)
    
    # 5. å¯è§†åŒ–æ€§èƒ½æ¯”è¾ƒ
    multi_recommender.visualize_performance()
    
    # 6. ä¸ºç‰¹å®šç”¨æˆ·ç”Ÿæˆæ¨è
    recommendations = multi_recommender.generate_recommendations(user_id='12345')
    """)

if __name__ == "__main__":
    main() 