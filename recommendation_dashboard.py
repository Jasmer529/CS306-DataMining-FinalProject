import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
import os
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç›¸å…³å¯¼å…¥
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import random
import pickle
import time

warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç”µå•†ç”¨æˆ·è¡Œä¸ºæ¨èç³»ç»Ÿ",
    page_icon="ğŸ›’",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ===================== æœºå™¨å­¦ä¹ æ¨¡å‹å®šä¹‰ =====================

class NCFModel(nn.Module):
    """ç¥ç»ååŒè¿‡æ»¤æ¨¡å‹ï¼ˆNCFï¼‰"""
    def __init__(self, num_users, num_items, embedding_dim=32):
        super(NCFModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)

        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, user, item):
        user_vec = self.user_embedding(user)
        item_vec = self.item_embedding(item)
        x = torch.cat([user_vec, item_vec], dim=-1)
        return self.mlp(x).squeeze()

class MultiFeatureLSTM(nn.Module):
    """å¤šç‰¹å¾LSTMæ¨¡å‹"""
    def __init__(self, item_vocab_size, behavior_dim, category_vocab_size):
        super().__init__()
        self.item_embed = nn.Embedding(item_vocab_size, 50, padding_idx=0)
        self.behavior_embed = nn.Embedding(behavior_dim, 10)
        self.category_embed = nn.Embedding(category_vocab_size, 20, padding_idx=0)
        self.time_fc = nn.Linear(1, 10)

        self.lstm = nn.LSTM(50 + 10 + 20 + 10, 64, batch_first=True)
        self.fc = nn.Linear(64, category_vocab_size)

    def forward(self, items, behaviors, categories, time_diffs):
        items_emb = self.item_embed(items)
        behaviors_emb = self.behavior_embed(behaviors)
        categories_emb = self.category_embed(categories)
        time_emb = self.time_fc(time_diffs.unsqueeze(-1))

        combined = torch.cat([items_emb, behaviors_emb, categories_emb, time_emb], dim=-1)
        lstm_out, _ = self.lstm(combined)
        last_out = lstm_out[:, -1, :]
        return self.fc(last_out)

class InteractionDataset(Dataset):
    """ç”¨æˆ·ç‰©å“äº¤äº’æ•°æ®é›†"""
    def __init__(self, df):
        self.users = torch.tensor(df['user'].values, dtype=torch.long)
        self.items = torch.tensor(df['item'].values, dtype=torch.long)
        self.labels = torch.tensor(df['label'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        return self.users[idx], self.items[idx], self.labels[idx]

class CollaborativeFilteringRecommender:
    """ååŒè¿‡æ»¤æ¨èå™¨ - é’ˆå¯¹ç¨€ç–æ•°æ®ä¼˜åŒ–"""
    def __init__(self):
        self.user_item_matrix = None
        self.item_user_matrix = None  # å•†å“-ç”¨æˆ·çŸ©é˜µ
        self.user_sim_matrix = None
        self.item_sim_matrix = None   # å•†å“ç›¸ä¼¼åº¦çŸ©é˜µ
        self.trained = False
        self.is_sparse = False        # æ ‡è®°æ•°æ®æ˜¯å¦ç¨€ç–
        
    def fit(self, df):
        """è®­ç»ƒååŒè¿‡æ»¤æ¨¡å‹ - è‡ªé€‚åº”ç¨€ç–æ•°æ®"""
        # æ„å»ºç”¨æˆ·-å•†å“çŸ©é˜µ
        df_buy = df[df['behavior_type'] == 'buy'] if 'behavior_type' in df.columns else df
        print(f"Debug: CFè®­ç»ƒ - åŸå§‹è´­ä¹°è®°å½•æ•°: {len(df_buy)}")
        
        user_item_counts = df_buy.groupby(['user_id', 'item_id']).size().unstack(fill_value=0)
        self.user_item_matrix = user_item_counts
        
        # åŒæ—¶æ„å»ºå•†å“-ç”¨æˆ·çŸ©é˜µï¼ˆè½¬ç½®ï¼‰
        self.item_user_matrix = self.user_item_matrix.T
        
        print(f"Debug: CFè®­ç»ƒ - ç”¨æˆ·-å•†å“çŸ©é˜µå½¢çŠ¶: {self.user_item_matrix.shape}")
        sparsity = (self.user_item_matrix > 0).sum().sum() / (self.user_item_matrix.shape[0] * self.user_item_matrix.shape[1])
        print(f"Debug: CFè®­ç»ƒ - éé›¶å…ƒç´ æ¯”ä¾‹: {sparsity:.6f}")
        
        # åˆ¤æ–­æ•°æ®æ˜¯å¦ç¨€ç–
        self.is_sparse = sparsity < 0.01  # å¦‚æœéé›¶å…ƒç´ å°‘äº1%ï¼Œè®¤ä¸ºæ˜¯ç¨€ç–æ•°æ®
        print(f"Debug: CFè®­ç»ƒ - æ•°æ®ç¨€ç–çŠ¶æ€: {'ç¨€ç–' if self.is_sparse else 'ç¨ å¯†'}")
        
        if self.is_sparse:
            print(f"Debug: CFè®­ç»ƒ - æ£€æµ‹åˆ°ç¨€ç–æ•°æ®ï¼Œé‡‡ç”¨å•†å“-å•†å“ååŒè¿‡æ»¤ç­–ç•¥")
            # å¯¹äºç¨€ç–æ•°æ®ï¼Œä½¿ç”¨å•†å“-å•†å“ååŒè¿‡æ»¤
            self.item_sim_matrix = cosine_similarity(self.item_user_matrix)
            self.item_sim_df = pd.DataFrame(
                self.item_sim_matrix,
                index=self.item_user_matrix.index,
                columns=self.item_user_matrix.index
            )
            
            # å•†å“ç›¸ä¼¼åº¦ç»Ÿè®¡
            item_sim_values = self.item_sim_matrix[self.item_sim_matrix != 1.0]
            print(f"Debug: CFè®­ç»ƒ - å•†å“ç›¸ä¼¼åº¦ç»Ÿè®¡:")
            print(f"  - ç›¸ä¼¼åº¦èŒƒå›´: {item_sim_values.min():.6f} - {item_sim_values.max():.6f}")
            print(f"  - å¹³å‡ç›¸ä¼¼åº¦: {item_sim_values.mean():.6f}")
            print(f"  - ç›¸ä¼¼åº¦>0çš„æ¯”ä¾‹: {(item_sim_values > 0).sum() / len(item_sim_values):.6f}")
            print(f"  - ç›¸ä¼¼åº¦>0.1çš„æ¯”ä¾‹: {(item_sim_values > 0.1).sum() / len(item_sim_values):.6f}")
        else:
            print(f"Debug: CFè®­ç»ƒ - æ•°æ®è¾ƒç¨ å¯†ï¼Œé‡‡ç”¨ç”¨æˆ·-ç”¨æˆ·ååŒè¿‡æ»¤ç­–ç•¥")
            # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
            self.user_sim_matrix = cosine_similarity(self.user_item_matrix)
            self.user_sim_df = pd.DataFrame(
                self.user_sim_matrix, 
                index=self.user_item_matrix.index, 
                columns=self.user_item_matrix.index
            )
            
            # ç›¸ä¼¼åº¦çŸ©é˜µè¯Šæ–­
            sim_values = self.user_sim_matrix[self.user_sim_matrix != 1.0]
            print(f"Debug: CFè®­ç»ƒ - ç”¨æˆ·ç›¸ä¼¼åº¦ç»Ÿè®¡:")
            print(f"  - ç›¸ä¼¼åº¦èŒƒå›´: {sim_values.min():.6f} - {sim_values.max():.6f}")
            print(f"  - å¹³å‡ç›¸ä¼¼åº¦: {sim_values.mean():.6f}")
            print(f"  - ç›¸ä¼¼åº¦>0çš„æ¯”ä¾‹: {(sim_values > 0).sum() / len(sim_values):.6f}")
            print(f"  - ç›¸ä¼¼åº¦>0.01çš„æ¯”ä¾‹: {(sim_values > 0.01).sum() / len(sim_values):.6f}")
        
        self.trained = True
        print(f"Debug: CFè®­ç»ƒå®Œæˆ")
        
    def recommend(self, user_id, top_n=10):
        """ä¸ºç”¨æˆ·æ¨èå•†å“ - è‡ªé€‚åº”ç¨€ç–/ç¨ å¯†æ•°æ®"""
        if not self.trained:
            print(f"Debug: CFæ¨¡å‹æœªè®­ç»ƒ")
            return pd.Series(dtype=float)
            
        if user_id not in self.user_item_matrix.index:
            print(f"Debug: ç”¨æˆ· {user_id} ä¸åœ¨CFè®­ç»ƒæ•°æ®ä¸­")
            print(f"Debug: CFè®­ç»ƒæ•°æ®åŒ…å«ç”¨æˆ·æ•°: {len(self.user_item_matrix.index)}")
            print(f"Debug: CFè®­ç»ƒæ•°æ®ç”¨æˆ·IDèŒƒå›´: {self.user_item_matrix.index.min()} - {self.user_item_matrix.index.max()}")
            return pd.Series(dtype=float)
        
        if self.is_sparse:
            # å¯¹äºç¨€ç–æ•°æ®ï¼Œä½¿ç”¨åŸºäºå•†å“çš„ååŒè¿‡æ»¤
            return self._recommend_item_based(user_id, top_n)
        else:
            # å¯¹äºç¨ å¯†æ•°æ®ï¼Œä½¿ç”¨åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤
            return self._recommend_user_based(user_id, top_n)
    
    def _recommend_item_based(self, user_id, top_n=10):
        """åŸºäºå•†å“çš„ååŒè¿‡æ»¤æ¨è"""
        print(f"Debug: CFä½¿ç”¨åŸºäºå•†å“çš„ååŒè¿‡æ»¤ä¸ºç”¨æˆ· {user_id} æ¨è")
        
        user_vector = self.user_item_matrix.loc[user_id]
        user_items = user_vector[user_vector > 0].index  # ç”¨æˆ·è´­ä¹°è¿‡çš„å•†å“
        
        print(f"Debug: ç”¨æˆ· {user_id} è´­ä¹°è¿‡çš„å•†å“æ•°: {len(user_items)}")
        
        if len(user_items) == 0:
            print("Debug: ç”¨æˆ·æ²¡æœ‰è´­ä¹°è®°å½•ï¼Œæ— æ³•æ¨è")
            return pd.Series(dtype=float)
        
        scores = pd.Series(0.0, index=self.item_user_matrix.index)
        
        # åŸºäºç”¨æˆ·è´­ä¹°è¿‡çš„å•†å“ï¼Œæ‰¾ç›¸ä¼¼å•†å“
        for item in user_items:
            similar_items = self.item_sim_df[item].drop(item).sort_values(ascending=False)
            
            # ä½¿ç”¨è¾ƒä½çš„é˜ˆå€¼ï¼Œå› ä¸ºç¨€ç–æ•°æ®ç›¸ä¼¼åº¦æ™®éè¾ƒä½
            threshold = 0.05
            similar_items_filtered = similar_items[similar_items > threshold]
            
            print(f"Debug: å•†å“ {item} æ‰¾åˆ° {len(similar_items_filtered)} ä¸ªç›¸ä¼¼å•†å“ (é˜ˆå€¼: {threshold})")
            
            if len(similar_items_filtered) == 0:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼å•†å“ï¼Œé™ä½é˜ˆå€¼
                threshold = 0.01
                similar_items_filtered = similar_items[similar_items > threshold].head(10)
                print(f"Debug: é™ä½é˜ˆå€¼åˆ° {threshold}ï¼Œæ‰¾åˆ° {len(similar_items_filtered)} ä¸ªç›¸ä¼¼å•†å“")
            
            # ç´¯ç§¯åˆ†æ•°
            for similar_item, similarity in similar_items_filtered.head(20).items():
                scores[similar_item] += similarity * user_vector[item]
        
        # ç§»é™¤ç”¨æˆ·å·²è´­ä¹°çš„å•†å“
        candidate_scores = scores.drop(labels=user_items, errors='ignore')
        
        # è·å–æ­£åˆ†æ•°çš„æ¨è
        positive_scores = candidate_scores[candidate_scores > 0]
        print(f"Debug: CF(å•†å“)æ­£åˆ†æ•°å•†å“æ•°: {len(positive_scores)}")
        
        if len(positive_scores) == 0:
            print("Debug: CF(å•†å“)æ²¡æœ‰æ­£åˆ†æ•°çš„å•†å“")
            return pd.Series(dtype=float)
        
        result = positive_scores.sort_values(ascending=False).head(top_n)
        print(f"Debug: CF(å•†å“)æœ€ç»ˆæ¨èæ•°é‡: {len(result)}")
        if len(result) > 0:
            print(f"Debug: CF(å•†å“)æ¨èåˆ†æ•°èŒƒå›´: {result.max():.6f} - {result.min():.6f}")
        
        return result
    
    def _recommend_user_based(self, user_id, top_n=10):
        """åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤æ¨èï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        print(f"Debug: CFä½¿ç”¨åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤ä¸ºç”¨æˆ· {user_id} æ¨è")
        
        user_vector = self.user_item_matrix.loc[user_id]
        similar_users = self.user_sim_df[user_id].drop(user_id).sort_values(ascending=False)

        print(f"Debug: CFç”¨æˆ· {user_id} çš„ç›¸ä¼¼åº¦ç»Ÿè®¡:")
        print(f"  - æœ€é«˜ç›¸ä¼¼åº¦: {similar_users.max():.4f}")
        print(f"  - å¹³å‡ç›¸ä¼¼åº¦: {similar_users.mean():.4f}")
        print(f"  - ç›¸ä¼¼åº¦>0.01çš„ç”¨æˆ·æ•°: {(similar_users > 0.01).sum()}")
        print(f"  - ç›¸ä¼¼åº¦>0.05çš„ç”¨æˆ·æ•°: {(similar_users > 0.05).sum()}")

        scores = pd.Series(0.0, index=self.user_item_matrix.columns)
        
        # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½¿ç”¨æ›´å¤šç›¸ä¼¼ç”¨æˆ·
        used_users = 0
        similarity_threshold = 0.01  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
        for sim_user_id, similarity in similar_users.head(100).items():  # æ‰©å±•åˆ°top100
            if similarity > similarity_threshold:
                scores += similarity * self.user_item_matrix.loc[sim_user_id]
                used_users += 1
        
        print(f"Debug: CFä½¿ç”¨äº† {used_users} ä¸ªç›¸ä¼¼ç”¨æˆ· (é˜ˆå€¼: {similarity_threshold})")
        
        if used_users == 0:
            print(f"Debug: CFæ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·ï¼Œå°è¯•ä½¿ç”¨æ›´ä½é˜ˆå€¼...")
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç›¸ä¼¼ç”¨æˆ·ï¼Œå°è¯•æ›´ä½é˜ˆå€¼
            for sim_user_id, similarity in similar_users.head(50).items():
                if similarity > 0.001:  # éå¸¸ä½çš„é˜ˆå€¼
                    scores += similarity * self.user_item_matrix.loc[sim_user_id]
                    used_users += 1
            print(f"Debug: CFä½¿ç”¨æ›´ä½é˜ˆå€¼(0.001)åï¼Œä½¿ç”¨äº† {used_users} ä¸ªç›¸ä¼¼ç”¨æˆ·")
        
        # ç§»é™¤ç”¨æˆ·å·²ç»äº¤äº’è¿‡çš„å•†å“
        already_bought = user_vector[user_vector > 0].index
        print(f"Debug: ç”¨æˆ· {user_id} å·²äº¤äº’å•†å“æ•°: {len(already_bought)}")
        
        candidate_scores = scores.drop(labels=already_bought, errors='ignore')
        
        # å¦‚æœå€™é€‰å•†å“ä¸ºç©º
        if len(candidate_scores) == 0:
            print("Debug: CFå€™é€‰å•†å“ä¸ºç©º - æ‰€æœ‰å•†å“éƒ½å·²è¢«ç”¨æˆ·äº¤äº’è¿‡")
            return pd.Series(dtype=float)
        
        # é™ä½åˆ†æ•°é˜ˆå€¼ï¼Œå…è®¸æ›´å¤šå•†å“
        positive_scores = candidate_scores[candidate_scores > 0]
        print(f"Debug: CFæ­£åˆ†æ•°å•†å“æ•°: {len(positive_scores)}")
        
        if len(positive_scores) == 0:
            print("Debug: CFæ²¡æœ‰æ­£åˆ†æ•°çš„å•†å“")
            return pd.Series(dtype=float)
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è¶³å¤Ÿçš„æ¨è
        if len(positive_scores) < top_n:
            print(f"Debug: CFæ¨èä¸è¶³({len(positive_scores)})ï¼Œæ— å…œåº•ç­–ç•¥")
        
        result = positive_scores.sort_values(ascending=False).head(top_n)
        print(f"Debug: CFæœ€ç»ˆæ¨èæ•°é‡: {len(result)}")
        if len(result) > 0:
            print(f"Debug: CFæ¨èåˆ†æ•°èŒƒå›´: {result.max():.6f} - {result.min():.6f}")
        return result

class NCFRecommender:
    """NCFæ·±åº¦å­¦ä¹ æ¨èå™¨"""
    def __init__(self):
        self.model = None
        self.user2idx = {}
        self.item2idx = {}
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.trained = False
        
    def fit(self, df, epochs=5):
        """è®­ç»ƒNCFæ¨¡å‹"""
        print(f"Debug: NCFè®­ç»ƒå¼€å§‹ - ç›®æ ‡epochs: {epochs}")
        
        # åˆå§‹åŒ–è®­ç»ƒè®°å½•
        self.training_history = {
            'epochs': [],
            'losses': [],
            'accuracies': []
        }
        
        # æ•°æ®é¢„å¤„ç†
        df_buy = df[df['behavior_type'] == 'buy'] if 'behavior_type' in df.columns else df
        print(f"Debug: NCFè®­ç»ƒ - åŸå§‹è´­ä¹°è®°å½•æ•°: {len(df_buy)}")
        
        df_buy = df_buy.drop_duplicates(['user_id', 'item_id'])
        print(f"Debug: NCFè®­ç»ƒ - å»é‡åè´­ä¹°è®°å½•æ•°: {len(df_buy)}")
        
        # åˆ›å»ºç”¨æˆ·å’Œç‰©å“æ˜ å°„
        unique_users = df_buy['user_id'].unique()
        unique_items = df_buy['item_id'].unique()
        
        self.user2idx = {uid: idx for idx, uid in enumerate(unique_users)}
        self.item2idx = {iid: idx for idx, iid in enumerate(unique_items)}
        
        print(f"Debug: NCFè®­ç»ƒ - ç”¨æˆ·æ•°: {len(self.user2idx)}")
        print(f"Debug: NCFè®­ç»ƒ - å•†å“æ•°: {len(self.item2idx)}")
        print(f"Debug: NCFè®­ç»ƒ - ç”¨æˆ·IDèŒƒå›´: {min(unique_users)} - {max(unique_users)}")
        print(f"Debug: NCFè®­ç»ƒ - å•†å“IDèŒƒå›´: {min(unique_items)} - {max(unique_items)}")
        
        df_buy['user'] = df_buy['user_id'].map(self.user2idx)
        df_buy['item'] = df_buy['item_id'].map(self.item2idx)
        
        # æ„é€ æ­£è´Ÿæ ·æœ¬
        print(f"Debug: NCFè®­ç»ƒ - å¼€å§‹æ„é€ æ­£è´Ÿæ ·æœ¬...")
        interactions = set(zip(df_buy['user'], df_buy['item']))
        all_items = list(self.item2idx.values())
        
        print(f"Debug: NCFè®­ç»ƒ - æ­£æ ·æœ¬æ•°: {len(interactions)}")
        
        # é™åˆ¶è´Ÿæ ·æœ¬æ•°é‡ä»¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶ç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®
        max_samples = min(5000, len(interactions))  # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
        sampled_interactions = list(interactions)[:max_samples]
        
        neg_samples = []
        for u, i in sampled_interactions:
            j = random.choice(all_items)
            while (u, j) in interactions:
                j = random.choice(all_items)
            neg_samples.append([u, j, 0])
        
        print(f"Debug: NCFè®­ç»ƒ - è´Ÿæ ·æœ¬æ•°: {len(neg_samples)}")
        
        df_pos = df_buy[['user', 'item']].head(max_samples).copy()
        df_pos['label'] = 1
        df_neg = pd.DataFrame(neg_samples, columns=['user', 'item', 'label'])
        df_all = pd.concat([df_pos, df_neg], ignore_index=True)
        
        print(f"Debug: NCFè®­ç»ƒ - æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(df_all)}")
        print(f"Debug: NCFè®­ç»ƒ - æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {len(df_pos)}:{len(df_neg)}")
        
        # æ•°æ®ç¨€ç–åº¦åˆ†æ
        total_possible = len(self.user2idx) * len(self.item2idx)
        sparsity = len(interactions) / total_possible
        print(f"Debug: NCFè®­ç»ƒ - æ•°æ®ç¨€ç–åº¦: {sparsity:.6f}")
        
        # åˆ›å»ºæ¨¡å‹
        print(f"Debug: NCFè®­ç»ƒ - åˆ›å»ºæ¨¡å‹...")
        self.model = NCFModel(len(self.user2idx), len(self.item2idx)).to(self.device)
        
        # æ¨¡å‹å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Debug: NCFè®­ç»ƒ - æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"Debug: NCFè®­ç»ƒ - å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
        
        # ä¿å­˜æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
        self.model_stats = {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'num_users': len(self.user2idx),
            'num_items': len(self.item2idx),
            'sparsity': sparsity,
            'training_samples': len(df_all)
        }
        
        criterion = nn.BCELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        
        # è®­ç»ƒæ•°æ®
        train_dataset = InteractionDataset(df_all)
        batch_size = min(512, len(df_all) // 4)  # åŠ¨æ€è°ƒæ•´batch size
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        print(f"Debug: NCFè®­ç»ƒ - æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"Debug: NCFè®­ç»ƒ - æ€»æ‰¹æ¬¡æ•°: {len(train_loader)}")
        
        # è®­ç»ƒæ¨¡å‹
        print(f"Debug: NCFè®­ç»ƒ - å¼€å§‹è®­ç»ƒ...")
        
        for epoch in range(epochs):
            self.model.train()
            epoch_loss = 0.0
            batch_count = 0
            
            for users, items, labels in train_loader:
                users, items, labels = users.to(self.device), items.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(users, items)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_loss = epoch_loss / batch_count
            
            # è®¡ç®—å½“å‰epochçš„å‡†ç¡®ç‡
            self.model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for users, items, labels in train_loader:
                    users, items, labels = users.to(self.device), items.to(self.device), labels.to(self.device)
                    outputs = self.model(users, items)
                    predicted = (outputs > 0.5).float()
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            
            accuracy = 100 * correct / total
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['epochs'].append(epoch + 1)
            self.training_history['losses'].append(avg_loss)
            self.training_history['accuracies'].append(accuracy)
            
            print(f"Debug: NCFè®­ç»ƒ - Epoch {epoch+1}/{epochs}, æŸå¤±: {avg_loss:.6f}, å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        # è®­ç»ƒåè¯„ä¼°
        print(f"Debug: NCFè®­ç»ƒ - è®­ç»ƒå®Œæˆï¼Œè¿›è¡Œæ¨¡å‹è¯„ä¼°...")
        
        # æŸå¤±è¶‹åŠ¿åˆ†æ
        if len(self.training_history['losses']) > 1:
            loss_trend = self.training_history['losses'][-1] - self.training_history['losses'][0]
            print(f"Debug: NCFè®­ç»ƒ - æŸå¤±å˜åŒ–: {self.training_history['losses'][0]:.6f} -> {self.training_history['losses'][-1]:.6f} (å˜åŒ–: {loss_trend:.6f})")
            
            if loss_trend > -0.01:
                print(f"Debug: NCFè®­ç»ƒ - è­¦å‘Š: æŸå¤±ä¸‹é™ä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦æ›´å¤šepochsæˆ–è°ƒæ•´å­¦ä¹ ç‡")
        
        # é¢„æµ‹åˆ†å¸ƒåˆ†æ
        print(f"Debug: NCFè®­ç»ƒ - åˆ†æé¢„æµ‹åˆ†å¸ƒ...")
        sample_users = torch.tensor(list(range(min(100, len(self.user2idx)))), dtype=torch.long).to(self.device)
        sample_items = torch.tensor(list(range(min(100, len(self.item2idx)))), dtype=torch.long).to(self.device)
        
        if len(sample_users) > 0 and len(sample_items) > 0:
            # åˆ›å»ºç”¨æˆ·-å•†å“å¯¹çš„ç½‘æ ¼
            user_grid, item_grid = torch.meshgrid(sample_users, sample_items, indexing='ij')
            flat_users = user_grid.flatten()
            flat_items = item_grid.flatten()
            
            with torch.no_grad():
                sample_outputs = self.model(flat_users, flat_items)
                
            print(f"Debug: NCFè®­ç»ƒ - é¢„æµ‹åˆ†æ•°ç»Ÿè®¡:")
            print(f"  - åˆ†æ•°èŒƒå›´: {sample_outputs.min().item():.6f} - {sample_outputs.max().item():.6f}")
            print(f"  - å¹³å‡åˆ†æ•°: {sample_outputs.mean().item():.6f}")
            print(f"  - åˆ†æ•°æ ‡å‡†å·®: {sample_outputs.std().item():.6f}")
            print(f"  - é«˜åˆ†æ¯”ä¾‹(>0.7): {(sample_outputs > 0.7).float().mean().item():.4f}")
            print(f"  - ä½åˆ†æ¯”ä¾‹(<0.3): {(sample_outputs < 0.3).float().mean().item():.4f}")
        
        self.trained = True
        print(f"Debug: NCFè®­ç»ƒå®Œæˆï¼")
        print(f"Debug: NCFå¯æ¨èç”¨æˆ·æ•°: {len(self.user2idx)}")
        print(f"Debug: NCFå¯æ¨èå•†å“æ•°: {len(self.item2idx)}")
    
    def recommend(self, user_id_raw, k=10):
        """ä¸ºç”¨æˆ·æ¨èå•†å“"""
        if not self.trained:
            print(f"Debug: NCFæ¨¡å‹æœªè®­ç»ƒ")
            return []
            
        user_id = self.user2idx.get(user_id_raw)
        if user_id is None:
            print(f"Debug: ç”¨æˆ· {user_id_raw} ä¸åœ¨NCFè®­ç»ƒæ•°æ®ä¸­")
            print(f"Debug: NCFè®­ç»ƒæ•°æ®åŒ…å«ç”¨æˆ·æ•°: {len(self.user2idx)}")
            if self.user2idx:
                print(f"Debug: NCFè®­ç»ƒæ•°æ®ç”¨æˆ·IDèŒƒå›´: {min(self.user2idx.keys())} - {max(self.user2idx.keys())}")
            return []

        user_tensor = torch.tensor([user_id] * len(self.item2idx), dtype=torch.long).to(self.device)
        item_tensor = torch.tensor(list(self.item2idx.values()), dtype=torch.long).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            scores = self.model(user_tensor, item_tensor).cpu().numpy()
        
        # æ£€æŸ¥åˆ†æ•°åˆ†å¸ƒ
        print(f"Debug: NCFåˆ†æ•°èŒƒå›´: {scores.min():.4f} - {scores.max():.4f}")
        print(f"Debug: NCFå¹³å‡åˆ†æ•°: {scores.mean():.4f}")
        
        # è·å–æ¨èå•†å“
        top_items_idx = scores.argsort()[-k:][::-1]
        top_item_ids = [list(self.item2idx.keys())[i] for i in top_items_idx]
        top_scores = [float(scores[i]) for i in top_items_idx]  # ç¡®ä¿åˆ†æ•°æ˜¯floatç±»å‹
        
        # å¦‚æœåˆ†æ•°éƒ½å¾ˆä½ï¼Œç»™å‡ºè­¦å‘Šè€Œä¸æ˜¯å½’ä¸€åŒ–
        if max(top_scores) < 0.1:
            print(f"Debug: NCFåˆ†æ•°åä½(æœ€é«˜:{max(top_scores):.4f})ï¼Œå¯èƒ½æ¨¡å‹è®­ç»ƒä¸å……åˆ†")
        
        result = list(zip(top_item_ids, top_scores))
        print(f"Debug: NCFæ¨èç»“æœæ•°é‡: {len(result)}")
        print(f"Debug: NCFæ¨èåˆ†æ•°ç¤ºä¾‹: {[f'{score:.4f}' for _, score in result[:3]]}")
        return result

class LSTMRecommender:
    """LSTMåºåˆ—æ¨èå™¨"""
    def __init__(self):
        self.model = None
        self.item_to_idx = {}
        self.cat_to_idx = {}
        self.behavior_to_idx = {'pv': 1, 'cart': 2, 'fav': 3, 'buy': 4}
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.trained = False
        self.max_seq_len = 10
        
    def fit(self, df, epochs=3):
        """è®­ç»ƒLSTMåºåˆ—é¢„æµ‹æ¨¡å‹"""
        print(f"Debug: LSTMè®­ç»ƒå¼€å§‹ - ç›®æ ‡epochs: {epochs}")
        
        # åˆå§‹åŒ–è®­ç»ƒè®°å½•
        self.training_history = {
            'epochs': [],
            'losses': [],
            'accuracies': []
        }
        
        try:
            # æ•°æ®é¢„å¤„ç†
            df = df.copy()
            print(f"Debug: LSTMè®­ç»ƒ - åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
            
            df = df.sort_values(['user_id', 'timestamp'] if 'timestamp' in df.columns else ['user_id'])
            
            # æ„å»ºè¯æ±‡è¡¨
            print(f"Debug: LSTMè®­ç»ƒ - æ„å»ºè¯æ±‡è¡¨...")
            unique_items = df['item_id'].unique()
            unique_categories = df['category_id'].unique()
            
            self.item_to_idx = {item: idx+1 for idx, item in enumerate(unique_items)}
            self.cat_to_idx = {cat: idx+1 for idx, cat in enumerate(unique_categories)}
            
            print(f"Debug: LSTMè®­ç»ƒ - å•†å“è¯æ±‡è¡¨å¤§å°: {len(self.item_to_idx)}")
            print(f"Debug: LSTMè®­ç»ƒ - ç±»åˆ«è¯æ±‡è¡¨å¤§å°: {len(self.cat_to_idx)}")
            print(f"Debug: LSTMè®­ç»ƒ - è¡Œä¸ºç±»å‹è¯æ±‡è¡¨: {self.behavior_to_idx}")
            
            # ä¸ºæ¯ä¸ªç”¨æˆ·æ„å»ºåºåˆ—
            print(f"Debug: LSTMè®­ç»ƒ - æ„å»ºç”¨æˆ·è¡Œä¸ºåºåˆ—...")
            sequences = []
            all_users = df['user_id'].unique()
            target_users = all_users[:500]  # é™åˆ¶ç”¨æˆ·æ•°é‡ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
            
            print(f"Debug: LSTMè®­ç»ƒ - æ€»ç”¨æˆ·æ•°: {len(all_users)}, è®­ç»ƒç”¨æˆ·æ•°: {len(target_users)}")
            
            valid_sequences = 0
            for user_id in target_users:
                user_data = df[df['user_id'] == user_id].copy()
                if len(user_data) < 3:  # éœ€è¦è‡³å°‘3æ¡è®°å½•
                    continue
                    
                # æ˜ å°„åˆ°ç´¢å¼•
                user_data['item_idx'] = user_data['item_id'].map(self.item_to_idx)
                user_data['cat_idx'] = user_data['category_id'].map(self.cat_to_idx)
                user_data['behavior_idx'] = user_data['behavior_type'].map(self.behavior_to_idx)
                
                # æ£€æŸ¥æ˜ å°„æˆåŠŸç‡
                valid_items = user_data['item_idx'].notna().sum()
                valid_cats = user_data['cat_idx'].notna().sum()
                valid_behaviors = user_data['behavior_idx'].notna().sum()
                
                if valid_items < len(user_data) * 0.8 or valid_cats < len(user_data) * 0.8:
                    continue  # è·³è¿‡æ˜ å°„æˆåŠŸç‡ä½çš„ç”¨æˆ·
                
                # ç”Ÿæˆåºåˆ—
                for i in range(2, len(user_data)):
                    seq_items = user_data['item_idx'].iloc[:i].fillna(0).astype(int).tolist()
                    seq_behaviors = user_data['behavior_idx'].iloc[:i].fillna(0).astype(int).tolist()
                    seq_cats = user_data['cat_idx'].iloc[:i].fillna(0).astype(int).tolist()
                    target_cat = user_data['cat_idx'].iloc[i]
                    
                    if pd.isna(target_cat):
                        continue
                    
                    # æ—¶é—´å·®ç‰¹å¾ï¼ˆç®€åŒ–ä¸ºä½ç½®ç¼–ç ï¼‰
                    seq_times = list(range(len(seq_items)))
                    
                    sequences.append({
                        'items': seq_items[-self.max_seq_len:],
                        'behaviors': seq_behaviors[-self.max_seq_len:],
                        'categories': seq_cats[-self.max_seq_len:],
                        'times': seq_times[-self.max_seq_len:],
                        'target': int(target_cat)
                    })
                
                valid_sequences += 1
            
            print(f"Debug: LSTMè®­ç»ƒ - æœ‰æ•ˆç”¨æˆ·æ•°: {valid_sequences}")
            print(f"Debug: LSTMè®­ç»ƒ - ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")
            
            if len(sequences) < 10:
                print("Debug: LSTMè®­ç»ƒæ•°æ®ä¸è¶³")
                return False
            
            # åºåˆ—é•¿åº¦ç»Ÿè®¡
            seq_lengths = [len(seq['items']) for seq in sequences]
            print(f"Debug: LSTMè®­ç»ƒ - åºåˆ—é•¿åº¦ç»Ÿè®¡:")
            print(f"  - å¹³å‡é•¿åº¦: {np.mean(seq_lengths):.2f}")
            print(f"  - æœ€å¤§é•¿åº¦: {max(seq_lengths)}")
            print(f"  - æœ€å°é•¿åº¦: {min(seq_lengths)}")
            
            # ç›®æ ‡ç±»åˆ«åˆ†å¸ƒ
            target_cats = [seq['target'] for seq in sequences]
            target_distribution = pd.Series(target_cats).value_counts()
            print(f"Debug: LSTMè®­ç»ƒ - ç›®æ ‡ç±»åˆ«åˆ†å¸ƒ:")
            print(f"  - ç±»åˆ«æ•°: {len(target_distribution)}")
            print(f"  - æœ€é¢‘ç¹ç±»åˆ«: {target_distribution.index[0]} (å‡ºç°{target_distribution.iloc[0]}æ¬¡)")
            print(f"  - ç±»åˆ«åˆ†å¸ƒå‡åŒ€åº¦: {target_distribution.std()/target_distribution.mean():.3f}")
            
            # åˆ›å»ºæ¨¡å‹
            print(f"Debug: LSTMè®­ç»ƒ - åˆ›å»ºæ¨¡å‹...")
            vocab_size_item = len(self.item_to_idx) + 1
            vocab_size_cat = len(self.cat_to_idx) + 1
            behavior_dim = len(self.behavior_to_idx) + 1
            
            print(f"Debug: LSTMè®­ç»ƒ - æ¨¡å‹å‚æ•°:")
            print(f"  - å•†å“è¯æ±‡é‡: {vocab_size_item}")
            print(f"  - ç±»åˆ«è¯æ±‡é‡: {vocab_size_cat}")
            print(f"  - è¡Œä¸ºç»´åº¦: {behavior_dim}")
            print(f"  - åºåˆ—é•¿åº¦: {self.max_seq_len}")
            
            self.model = MultiFeatureLSTM(
                item_vocab_size=vocab_size_item,
                behavior_dim=behavior_dim,
                category_vocab_size=vocab_size_cat
            ).to(self.device)
            
            # æ¨¡å‹å‚æ•°ç»Ÿè®¡
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            print(f"Debug: LSTMè®­ç»ƒ - æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
            print(f"Debug: LSTMè®­ç»ƒ - å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
            
            # ä¿å­˜æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
            self.model_stats = {
                'total_params': total_params,
                'trainable_params': trainable_params,
                'vocab_size_item': vocab_size_item,
                'vocab_size_cat': vocab_size_cat,
                'num_sequences': len(sequences),
                'valid_users': valid_sequences
            }
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            print(f"Debug: LSTMè®­ç»ƒ - å‡†å¤‡è®­ç»ƒæ•°æ®...")
            train_data = self._prepare_sequences(sequences)
            print(f"Debug: LSTMè®­ç»ƒ - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_data)}")
            
            # è®­ç»ƒæ¨¡å‹
            criterion = nn.CrossEntropyLoss(ignore_index=0)
            optimizer = optim.Adam(self.model.parameters(), lr=0.001)
            
            print(f"Debug: LSTMè®­ç»ƒ - å¼€å§‹è®­ç»ƒ...")
            
            self.model.train()
            for epoch in range(epochs):
                epoch_loss = 0.0
                batch_count = 0
                
                for batch in train_data:
                    items, behaviors, categories, times, targets = batch
                    
                    items = items.to(self.device)
                    behaviors = behaviors.to(self.device)
                    categories = categories.to(self.device)
                    times = times.to(self.device)
                    targets = targets.to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = self.model(items, behaviors, categories, times)
                    loss = criterion(outputs, targets)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    batch_count += 1
                
                avg_loss = epoch_loss / batch_count if batch_count > 0 else 0
                
                # è®¡ç®—å½“å‰epochçš„å‡†ç¡®ç‡
                self.model.eval()
                correct = 0
                total = 0
                
                with torch.no_grad():
                    for batch in train_data:
                        items, behaviors, categories, times, targets = batch
                        
                        items = items.to(self.device)
                        behaviors = behaviors.to(self.device)
                        categories = categories.to(self.device)
                        times = times.to(self.device)
                        targets = targets.to(self.device)
                        
                        outputs = self.model(items, behaviors, categories, times)
                        _, predicted = torch.max(outputs.data, 1)
                        total += targets.size(0)
                        correct += (predicted == targets).sum().item()
                
                accuracy = 100 * correct / total if total > 0 else 0
                
                # è®°å½•è®­ç»ƒå†å²
                self.training_history['epochs'].append(epoch + 1)
                self.training_history['losses'].append(avg_loss)
                self.training_history['accuracies'].append(accuracy)
                
                print(f"Debug: LSTMè®­ç»ƒ - Epoch {epoch+1}/{epochs}, æŸå¤±: {avg_loss:.6f}, å‡†ç¡®ç‡: {accuracy:.2f}%")
                
                # å›åˆ°è®­ç»ƒæ¨¡å¼
                self.model.train()
            
            # è®­ç»ƒåè¯„ä¼°
            print(f"Debug: LSTMè®­ç»ƒ - è®­ç»ƒå®Œæˆï¼Œè¿›è¡Œæ¨¡å‹è¯„ä¼°...")
            
            # æŸå¤±è¶‹åŠ¿åˆ†æ
            if len(self.training_history['losses']) > 1:
                loss_trend = self.training_history['losses'][-1] - self.training_history['losses'][0]
                print(f"Debug: LSTMè®­ç»ƒ - æŸå¤±å˜åŒ–: {self.training_history['losses'][0]:.6f} -> {self.training_history['losses'][-1]:.6f} (å˜åŒ–: {loss_trend:.6f})")
                
                if loss_trend > -0.1:
                    print(f"Debug: LSTMè®­ç»ƒ - è­¦å‘Š: æŸå¤±ä¸‹é™ä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦æ›´å¤šepochsæˆ–è°ƒæ•´å­¦ä¹ ç‡")
            
            # é¢„æµ‹åˆ†å¸ƒåˆ†æ
            print(f"Debug: LSTMè®­ç»ƒ - åˆ†æç±»åˆ«é¢„æµ‹åˆ†å¸ƒ...")
            sample_predictions = []
            
            self.model.eval()
            with torch.no_grad():
                for batch in train_data[:5]:  # å–å‰5ä¸ªæ‰¹æ¬¡è¿›è¡Œåˆ†æ
                    items, behaviors, categories, times, targets = batch
                    
                    items = items.to(self.device)
                    behaviors = behaviors.to(self.device)
                    categories = categories.to(self.device)
                    times = times.to(self.device)
                    
                    outputs = self.model(items, behaviors, categories, times)
                    probs = torch.softmax(outputs, dim=1)
                    max_probs, predicted_cats = torch.max(probs, 1)
                    
                    sample_predictions.extend(max_probs.cpu().numpy())
            
            if sample_predictions:
                print(f"Debug: LSTMè®­ç»ƒ - é¢„æµ‹ç½®ä¿¡åº¦ç»Ÿè®¡:")
                print(f"  - ç½®ä¿¡åº¦èŒƒå›´: {min(sample_predictions):.4f} - {max(sample_predictions):.4f}")
                print(f"  - å¹³å‡ç½®ä¿¡åº¦: {np.mean(sample_predictions):.4f}")
                print(f"  - é«˜ç½®ä¿¡åº¦æ¯”ä¾‹(>0.8): {np.mean(np.array(sample_predictions) > 0.8):.4f}")
                print(f"  - ä½ç½®ä¿¡åº¦æ¯”ä¾‹(<0.3): {np.mean(np.array(sample_predictions) < 0.3):.4f}")
            
            self.trained = True
            print(f"Debug: LSTMè®­ç»ƒå®Œæˆï¼")
            print(f"Debug: LSTMå¯é¢„æµ‹ç”¨æˆ·éœ€åœ¨åŸå§‹æ•°æ®ä¸­æœ‰è¶³å¤Ÿåºåˆ—")
            return True
            
        except Exception as e:
            print(f"Debug: LSTMè®­ç»ƒå¤±è´¥: {str(e)}")
            return False
    
    def _prepare_sequences(self, sequences):
        """å‡†å¤‡è®­ç»ƒåºåˆ—"""
        batch_size = 32
        batches = []
        
        for i in range(0, len(sequences), batch_size):
            batch_seqs = sequences[i:i+batch_size]
            
            # Padding
            items_batch = []
            behaviors_batch = []
            categories_batch = []
            times_batch = []
            targets_batch = []
            
            for seq in batch_seqs:
                # Pad sequences to max_seq_len
                items = seq['items'] + [0] * (self.max_seq_len - len(seq['items']))
                behaviors = seq['behaviors'] + [0] * (self.max_seq_len - len(seq['behaviors']))
                categories = seq['categories'] + [0] * (self.max_seq_len - len(seq['categories']))
                times = seq['times'] + [0] * (self.max_seq_len - len(seq['times']))
                
                items_batch.append(items[-self.max_seq_len:])
                behaviors_batch.append(behaviors[-self.max_seq_len:])
                categories_batch.append(categories[-self.max_seq_len:])
                times_batch.append(times[-self.max_seq_len:])
                targets_batch.append(seq['target'])
            
            batch_tensors = (
                torch.tensor(items_batch, dtype=torch.long),
                torch.tensor(behaviors_batch, dtype=torch.long),
                torch.tensor(categories_batch, dtype=torch.long),
                torch.tensor(times_batch, dtype=torch.float),
                torch.tensor(targets_batch, dtype=torch.long)
            )
            batches.append(batch_tensors)
        
        return batches
    
    def recommend_categories(self, user_id, df, k=5):
        """ä¸ºç”¨æˆ·æ¨èç±»åˆ«"""
        if not self.trained:
            print("Debug: LSTMæ¨¡å‹æœªè®­ç»ƒ")
            return []
        
        try:
            # è·å–ç”¨æˆ·å†å²åºåˆ—
            user_data = df[df['user_id'] == user_id].copy()
            if len(user_data) == 0:
                print(f"Debug: ç”¨æˆ· {user_id} åœ¨åŸå§‹æ•°æ®ä¸­æ²¡æœ‰å†å²æ•°æ®")
                return []
            
            # æ£€æŸ¥ç”¨æˆ·æ•°æ®æ˜¯å¦è¶³å¤Ÿ
            if len(user_data) < 3:
                print(f"Debug: ç”¨æˆ· {user_id} å†å²æ•°æ®ä¸è¶³({len(user_data)}æ¡)ï¼Œéœ€è¦è‡³å°‘3æ¡")
                return []
            
            user_data = user_data.sort_values('timestamp' if 'timestamp' in user_data.columns else user_data.columns[0])
            
            # æ„å»ºè¾“å…¥åºåˆ—
            seq_items = [self.item_to_idx.get(item, 0) for item in user_data['item_id'].tail(self.max_seq_len)]
            seq_behaviors = [self.behavior_to_idx.get(behavior, 0) for behavior in user_data['behavior_type'].tail(self.max_seq_len)]
            seq_cats = [self.cat_to_idx.get(cat, 0) for cat in user_data['category_id'].tail(self.max_seq_len)]
            seq_times = list(range(len(seq_items)))
            
            # æ£€æŸ¥æ˜ å°„æˆåŠŸç‡
            valid_items = sum(1 for x in seq_items if x > 0)
            valid_behaviors = sum(1 for x in seq_behaviors if x > 0) 
            valid_cats = sum(1 for x in seq_cats if x > 0)
            
            print(f"Debug: LSTMç”¨æˆ· {user_id} æ˜ å°„ç»Ÿè®¡ - å•†å“:{valid_items}/{len(seq_items)}, è¡Œä¸º:{valid_behaviors}/{len(seq_behaviors)}, ç±»åˆ«:{valid_cats}/{len(seq_cats)}")
            
            if valid_items == 0 or valid_cats == 0:
                print(f"Debug: ç”¨æˆ· {user_id} çš„å•†å“æˆ–ç±»åˆ«æ— æ³•æ˜ å°„åˆ°è®­ç»ƒè¯æ±‡è¡¨")
                return []
            
            # Padding
            if len(seq_items) < self.max_seq_len:
                pad_len = self.max_seq_len - len(seq_items)
                seq_items = [0] * pad_len + seq_items
                seq_behaviors = [0] * pad_len + seq_behaviors
                seq_cats = [0] * pad_len + seq_cats
                seq_times = [0] * pad_len + seq_times
            
            # é¢„æµ‹
            items_tensor = torch.tensor([seq_items], dtype=torch.long).to(self.device)
            behaviors_tensor = torch.tensor([seq_behaviors], dtype=torch.long).to(self.device)
            cats_tensor = torch.tensor([seq_cats], dtype=torch.long).to(self.device)
            times_tensor = torch.tensor([seq_times], dtype=torch.float).to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(items_tensor, behaviors_tensor, cats_tensor, times_tensor)
                probs = torch.softmax(outputs, dim=1)
                topk_probs, topk_ids = torch.topk(probs, k)
            
            # è½¬æ¢å›ç±»åˆ«åç§°
            results = []
            idx_to_cat = {v: k for k, v in self.cat_to_idx.items()}
            
            for prob, cat_id in zip(topk_probs[0], topk_ids[0]):
                cat_id = cat_id.item()
                if cat_id in idx_to_cat:
                    category = idx_to_cat[cat_id]
                    results.append((category, float(prob.item())))
            
            print(f"Debug: LSTMæ¨èç±»åˆ«æ•°é‡: {len(results)}")
            print(f"Debug: LSTMæ¨èç±»åˆ«ç¤ºä¾‹: {[f'{cat}:{prob:.4f}' for cat, prob in results[:3]]}")
            return results
            
        except Exception as e:
            print(f"Debug: LSTMæ¨èå¤±è´¥: {str(e)}")
            return []
    
    def recommend(self, user_id, df, k=10):
        """ä¸ºç”¨æˆ·æ¨èå•†å“ï¼ˆåŸºäºç±»åˆ«é¢„æµ‹ï¼‰"""
        if not self.trained:
            print("Debug: LSTMæ¨¡å‹æœªè®­ç»ƒ")
            return []
        
        try:
            # é¦–å…ˆé¢„æµ‹ç”¨æˆ·æ„Ÿå…´è¶£çš„ç±»åˆ«
            category_recommendations = self.recommend_categories(user_id, df, k=min(5, k))
            
            if not category_recommendations:
                print(f"Debug: ç”¨æˆ· {user_id} æ— æ³•é¢„æµ‹ç±»åˆ«")
                return []
            
            # è·å–ç”¨æˆ·å†å²è¡Œä¸ºï¼Œç”¨äºä¸ªæ€§åŒ–
            user_data = df[df['user_id'] == user_id].copy()
            user_history_items = set(user_data['item_id'].tolist())
            user_preferred_categories = user_data['category_id'].value_counts().to_dict()
            user_behavior_weights = {
                'pv': 0.1, 'cart': 0.3, 'fav': 0.5, 'buy': 1.0
            }
            
            # è®¡ç®—ç”¨æˆ·å¯¹æ¯ä¸ªå•†å“çš„å†å²åå¥½åˆ†æ•°
            user_item_preference = {}
            for _, row in user_data.iterrows():
                item_id = row['item_id']
                behavior = row['behavior_type']
                weight = user_behavior_weights.get(behavior, 0.1)
                
                if item_id in user_item_preference:
                    user_item_preference[item_id] += weight
                else:
                    user_item_preference[item_id] = weight
            
            # åŸºäºé¢„æµ‹çš„ç±»åˆ«æ¨èå•†å“
            recommendations = []
            
            for category, category_score in category_recommendations:
                # è·å–è¯¥ç±»åˆ«ä¸‹çš„å•†å“ï¼Œæ’é™¤ç”¨æˆ·å·²äº¤äº’çš„
                category_items = df[df['category_id'] == category]['item_id'].value_counts()
                
                # ç»™ç”¨æˆ·åå¥½ç±»åˆ«æ›´é«˜æƒé‡
                category_preference_bonus = user_preferred_categories.get(category, 0) * 0.1
                
                for item_id, popularity in category_items.head(3).items():  # æ¯ä¸ªç±»åˆ«å–top3
                    if item_id in user_history_items:
                        continue  # è·³è¿‡ç”¨æˆ·å·²äº¤äº’çš„å•†å“
                    
                    # è®¡ç®—ç»¼åˆåˆ†æ•°
                    # 1. ç±»åˆ«é¢„æµ‹åˆ†æ•°
                    base_score = category_score
                    
                    # 2. å•†å“çƒ­åº¦åˆ†æ•° (å½’ä¸€åŒ–)
                    popularity_score = popularity / category_items.max() * 0.3
                    
                    # 3. ç±»åˆ«åå¥½å¥–åŠ±
                    preference_bonus = category_preference_bonus
                    
                    # 4. ç¡®å®šæ€§è°ƒæ•´å› å­ï¼ˆåŸºäºç”¨æˆ·IDï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼‰
                    user_hash = hash(str(user_id)) % 1000
                    consistency_factor = 0.9 + (user_hash / 10000)  # 0.9-0.999ä¹‹é—´çš„å›ºå®šå€¼
                    
                    # 5. ç”¨æˆ·å†å²è¡Œä¸ºæ¨¡å¼åŒ¹é…åº¦
                    behavior_match = 0.1
                    user_avg_interactions = len(user_data) / user_data['item_id'].nunique() if user_data['item_id'].nunique() > 0 else 1
                    if user_avg_interactions > 2:  # æ´»è·ƒç”¨æˆ·
                        behavior_match = 0.2
                    
                    final_score = (base_score + popularity_score + preference_bonus + behavior_match) * consistency_factor
                    
                    recommendations.append((item_id, float(final_score)))
            
            # å¦‚æœæ¨èæ•°é‡ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜åŸå› è€Œä¸æ˜¯è¡¥å……
            if len(recommendations) < k:
                print(f"Debug: LSTMæ¨èä¸è¶³({len(recommendations)})ï¼ŒåŸå› åˆ†æ:")
                if not category_recommendations:
                    print("  - ç±»åˆ«é¢„æµ‹å¤±è´¥")
                else:
                    print("  - é¢„æµ‹ç±»åˆ«ä¸­å¯æ¨èå•†å“ä¸è¶³")
                    print(f"  - é¢„æµ‹çš„ç±»åˆ«: {[cat for cat, _ in category_recommendations]}")
                    if user_preferred_categories:
                        print(f"  - ç”¨æˆ·å†å²ç±»åˆ«: {list(user_preferred_categories.keys())[:3]}")
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top k
            recommendations.sort(key=lambda x: x[1], reverse=True)
            result = recommendations[:k]
            
            print(f"Debug: LSTMæ¨èå•†å“æ•°é‡: {len(result)}")
            if user_preferred_categories:
                print(f"Debug: LSTMç”¨æˆ· {user_id} åå¥½ç±»åˆ«: {list(user_preferred_categories.keys())[:3]}")
            if result:
                print(f"Debug: LSTMæ¨èåˆ†æ•°èŒƒå›´: {result[0][1]:.4f} - {result[-1][1]:.4f}")
            else:
                print("Debug: LSTMæ— æ³•ä¸ºè¯¥ç”¨æˆ·ç”Ÿæˆæ¨è")
            return result
            
        except Exception as e:
            print(f"Debug: LSTMå•†å“æ¨èå¤±è´¥: {str(e)}")
            return []

# ===================== åŸæœ‰ä»£ç ç»§ç»­ =====================

# è®¾ç½®æ–‡ä»¶ä¸Šä¼ å¤§å°é™åˆ¶ä¸º5GB
@st.cache_resource
def configure_upload_size():
    """é…ç½®æ–‡ä»¶ä¸Šä¼ å¤§å°é™åˆ¶"""
    # Streamlit é»˜è®¤é™åˆ¶æ˜¯200MBï¼Œæˆ‘ä»¬é€šè¿‡é…ç½®å°†å…¶æå‡åˆ°5GB
    import streamlit.config as stconfig
    try:
        # è®¾ç½®æœ€å¤§ä¸Šä¼ æ–‡ä»¶å¤§å°ä¸º5120MB (5GB)
        os.environ['STREAMLIT_SERVER_MAX_UPLOAD_SIZE'] = '5120'
        return True
    except Exception:
        return False

# è°ƒç”¨é…ç½®å‡½æ•°
configure_upload_size()

class RecommendationDashboard:
    """æ¨èç³»ç»Ÿå¯è§†åŒ–ç•Œé¢ç±»"""
    
    def __init__(self):
        self.data = None
        self.user_features = None
        self.recommendations = None
        # åˆå§‹åŒ–æ¨èå™¨
        self.cf_recommender = CollaborativeFilteringRecommender()
        self.ncf_recommender = NCFRecommender()
        self.lstm_recommender = LSTMRecommender()
        
        # åˆå§‹åŒ–session state
        if 'models_trained' not in st.session_state:
            st.session_state.models_trained = False
        
        if 'trained_ncf_recommender' not in st.session_state:
            st.session_state.trained_ncf_recommender = None
        
        if 'trained_lstm_recommender' not in st.session_state:
            st.session_state.trained_lstm_recommender = None
    
    @st.cache_data
    def load_data(_self, file_path):
        """åŠ è½½æ•°æ®"""
        try:
            data = pd.read_csv(file_path)
            return data
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def render_sidebar(self):
        """æ¸²æŸ“ä¾§è¾¹æ """
        st.sidebar.title("ğŸ›’ ç”µå•†æ¨èç³»ç»Ÿ")
        st.sidebar.markdown("---")
        
        # æ•°æ®åŠ è½½
        st.sidebar.subheader("ğŸ“ æ•°æ®åŠ è½½")
        
        
        uploaded_file = st.sidebar.file_uploader(
            "é€‰æ‹©æ•°æ®æ–‡ä»¶", 
            type=['csv'],
            help="è¯·ä¸Šä¼ åŒ…å«user_id, item_id, behavior_type, datetimeåˆ—çš„CSVæ–‡ä»¶ (æœ€å¤§5GB)"
        )
        
        if uploaded_file:
            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            file_size_mb = uploaded_file.size / (1024 * 1024)
            st.sidebar.info(f"ğŸ“„ æ–‡ä»¶ä¿¡æ¯:\n"
                           f"- æ–‡ä»¶å: {uploaded_file.name}\n"
                           f"- æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
            
            # åŠ è½½æ•°æ®ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
            with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                try:
                    self.data = pd.read_csv(uploaded_file)
                    st.sidebar.success(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data):,} æ¡è®°å½•")
                    
                    # æ•°æ®é¢„å¤„ç† - æ—¶é—´æˆ³å¤„ç†
                    if 'timestamp' in self.data.columns and 'timestamp_dt' not in self.data.columns:
                        st.sidebar.info("ğŸ•’ æ£€æµ‹åˆ°åŸå§‹æ—¶é—´æˆ³ï¼Œæ­£åœ¨è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´...")
                        # è½¬æ¢Unixæ—¶é—´æˆ³ä¸ºdatetimeï¼Œå¹¶è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
                        self.data['timestamp_dt'] = pd.to_datetime(self.data['timestamp'], unit='s', errors='coerce')
                        self.data['timestamp_dt'] = self.data['timestamp_dt'] + pd.Timedelta(hours=8)
                        
                        # æ·»åŠ æ—¶é—´ç‰¹å¾
                        self.data['date'] = self.data['timestamp_dt'].dt.date
                        self.data['hour'] = self.data['timestamp_dt'].dt.hour
                        self.data['weekday'] = self.data['timestamp_dt'].dt.day_name()
                        self.data['day_of_week'] = self.data['timestamp_dt'].dt.dayofweek
                        self.data['day_of_month'] = self.data['timestamp_dt'].dt.day
                        self.data['is_weekend'] = self.data['timestamp_dt'].dt.weekday >= 5
                        
                        st.sidebar.success("âœ… æ—¶é—´æˆ³è½¬æ¢å®Œæˆï¼ˆå·²è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼‰")
                    
                    # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
                    st.sidebar.write("**æ•°æ®å­—æ®µ:**")
                    st.sidebar.write(f"- åˆ—æ•°: {len(self.data.columns)}")
                    st.sidebar.write(f"- å­—æ®µ: {', '.join(self.data.columns.tolist()[:5])}{'...' if len(self.data.columns) > 5 else ''}")
                    
                    # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶æä¾›å»ºè®®
                    if 'timestamp_dt' in self.data.columns:
                        if 'date' in self.data.columns:
                            st.sidebar.info("ğŸ“Š æ£€æµ‹åˆ°é¢„å¤„ç†æ•°æ®æ ¼å¼ï¼ˆåŒ…å«åŒ—äº¬æ—¶é—´ï¼‰")
                        else:
                            st.sidebar.info("ğŸ“Š æ£€æµ‹åˆ°æ—¶é—´æˆ³æ•°æ®ï¼Œå·²è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´")
                    elif 'timestamp' in self.data.columns:
                        st.sidebar.info("ğŸ“Š æ£€æµ‹åˆ°åŸå§‹æ•°æ®æ ¼å¼")
                    
                except Exception as e:
                    st.sidebar.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
                    st.sidebar.info("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆCSVæ ¼å¼ï¼ŒåŒ…å«å¿…è¦å­—æ®µï¼‰")
        
        # åˆ†æé€‰é¡¹
        st.sidebar.markdown("---")
        st.sidebar.subheader("ğŸ” åˆ†æé€‰é¡¹")
        
        analysis_type = st.sidebar.selectbox(
            "é€‰æ‹©åˆ†æç±»å‹",
            ["æ•°æ®æ¦‚è§ˆ", "ç”¨æˆ·è¡Œä¸ºåˆ†æ", "ç”¨æˆ·ç”»åƒåˆ†æ", "æ¨èç®—æ³•æ¯”è¾ƒ", "ä¸ªæ€§åŒ–æ¨è"]
        )
        
        return analysis_type
    
    def render_data_overview(self):
        st.header("ğŸ“Š æ•°æ®æ¦‚è§ˆä¸æ¢ç´¢æ€§åˆ†æ")
        
        if self.data is None or self.data.empty:
            st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return
        
        df = self.data
        
        # åˆ›å»ºé€‰é¡¹å¡
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“ˆ åŸºç¡€ç»Ÿè®¡", 
            "ğŸ¯ å•å˜é‡åˆ†æ", 
            "ğŸ” å¤šå˜é‡åˆ†æ", 
            "ğŸ‘¥ ç”¨æˆ·è¡Œä¸ºåˆ†æ",
            "ğŸ”„ åºåˆ—åˆ†æ"
        ])
        
        with tab1:
            st.subheader("åŸºç¡€ç»Ÿè®¡ä¿¡æ¯")
            
            # åŸºç¡€ä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»è®°å½•æ•°", f"{len(df):,}")
            with col2:
                st.metric("ç‹¬ç«‹ç”¨æˆ·æ•°", f"{df['user_id'].nunique():,}")
            with col3:
                st.metric("ç‹¬ç«‹å•†å“æ•°", f"{df['item_id'].nunique():,}")
            with col4:
                st.metric("ç‹¬ç«‹ç±»ç›®æ•°", f"{df['category_id'].nunique():,}")
            
            # æ•°æ®æ—¶é—´èŒƒå›´
            if 'timestamp_dt' in df.columns:
                st.write(f"**æ•°æ®æ—¶é—´èŒƒå›´**: {df['timestamp_dt'].min()} åˆ° {df['timestamp_dt'].max()}")
            elif 'date' in df.columns:
                st.write(f"**æ•°æ®æ—¶é—´èŒƒå›´**: {df['date'].min()} åˆ° {df['date'].max()}")
            
            # æ•°æ®é¢„è§ˆ
            st.subheader("æ•°æ®é¢„è§ˆ")
            # ä¿®å¤Arrowåºåˆ—åŒ–é—®é¢˜ - ç¡®ä¿æ•°æ®ç±»å‹å…¼å®¹
            display_df = df.head(10).copy()
            for col in display_df.columns:
                if display_df[col].dtype == 'object':
                    try:
                        display_df[col] = display_df[col].astype(str)
                    except:
                        pass
            st.dataframe(display_df)
            
            # æ•°æ®ç±»å‹
            st.subheader("æ•°æ®ç±»å‹")
            st.write(df.dtypes)
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡
            st.subheader("ç¼ºå¤±å€¼ç»Ÿè®¡")
            missing_data = df.isnull().sum()
            if missing_data.sum() > 0:
                st.write(missing_data[missing_data > 0])
            else:
                st.success("æ•°æ®ä¸­æ²¡æœ‰ç¼ºå¤±å€¼")
        
        with tab2:
            st.subheader("ğŸ¯ å•å˜é‡åˆ†æ")
            
            # è¡Œä¸ºç±»å‹åˆ†å¸ƒ
            st.subheader("è¡Œä¸ºç±»å‹åˆ†å¸ƒ")
            behavior_counts = df['behavior_type'].value_counts()
            
            col1, col2 = st.columns(2)
            with col1:
                st.write("**è¡Œä¸ºç±»å‹ç»Ÿè®¡:**")
                st.write(behavior_counts)
            
            with col2:
                fig, ax = plt.subplots(figsize=(8, 6))
                sns.barplot(x=behavior_counts.index, y=behavior_counts.values, palette='viridis', ax=ax)
                ax.set_title('Distribution of Behavior Types')
                ax.set_xlabel('Behavior Type')
                ax.set_ylabel('Count')
                st.pyplot(fig)
            
            # æ—¶é—´åºåˆ—åˆ†æ
            if 'date' in df.columns:
                st.subheader("æ—¶é—´åºåˆ—åˆ†æ")
                
                # æŒ‰å¤©ç»Ÿè®¡
                st.write("**æ¯æ—¥ç”¨æˆ·è¡Œä¸ºæ€»é‡**")
                daily_behavior_counts = df.groupby('date')['user_id'].count()
                
                fig, ax = plt.subplots(figsize=(12, 6))
                daily_behavior_counts.plot(kind='line', marker='o', ax=ax)
                ax.set_title('Total User Behaviors per Day')
                ax.set_xlabel('Date')
                ax.set_ylabel('Number of Behaviors')
                ax.tick_params(axis='x', rotation=45)
                ax.grid(True)
                plt.tight_layout()
                st.pyplot(fig)
                
                # æŒ‰å°æ—¶ç»Ÿè®¡
                if 'hour' in df.columns:
                    st.write("**æ¯å°æ—¶ç”¨æˆ·è¡Œä¸ºæ€»é‡**")
                    hourly_behavior_counts = df.groupby('hour')['user_id'].count()
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    hourly_behavior_counts.plot(kind='bar', color='skyblue', ax=ax)
                    ax.set_title('Total User Behaviors per Hour of Day')
                    ax.set_xlabel('Hour of Day')
                    ax.set_ylabel('Number of Behaviors')
                    ax.tick_params(axis='x', rotation=0)
                    ax.grid(axis='y')
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # æŒ‰æ˜ŸæœŸå‡ ç»Ÿè®¡
                if 'weekday' in df.columns:
                    st.write("**æ¯å‘¨å„å¤©ç”¨æˆ·è¡Œä¸ºæ€»é‡**")
                    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                    weekday_behavior_counts = df.groupby('weekday')['user_id'].count()
                    if all(day in weekday_behavior_counts.index for day in weekday_order):
                        weekday_behavior_counts = weekday_behavior_counts.reindex(weekday_order)
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    weekday_behavior_counts.plot(kind='bar', color='lightcoral', ax=ax)
                    ax.set_title('Total User Behaviors per Day of Week')
                    ax.set_xlabel('Day of Week')
                    ax.set_ylabel('Number of Behaviors')
                    ax.tick_params(axis='x', rotation=45)
                    ax.grid(axis='y')
                    plt.tight_layout()
                    st.pyplot(fig)
        
        with tab3:
            st.subheader("ğŸ” å¤šå˜é‡åˆ†æä¸çƒ­é—¨åˆ†æ")
            
            top_n = st.slider("æ˜¾ç¤ºTop Né¡¹ç›®", min_value=5, max_value=20, value=10)
            
            # Top N å•†å“ (åŸºäºPVè¡Œä¸º)
            st.subheader("çƒ­é—¨å•†å“åˆ†æ")
            pv_df = df[df['behavior_type'] == 'pv']
            
            if not pv_df.empty:
                top_items_pv = pv_df['item_id'].value_counts().head(top_n)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**Top {top_n} æœ€å—å…³æ³¨å•†å“ (PV):**")
                    st.write(top_items_pv)
                
                with col2:
                    fig, ax = plt.subplots(figsize=(12, 6))
                    sns.barplot(x=top_items_pv.index, y=top_items_pv.values, palette='coolwarm', ax=ax)
                    ax.set_title(f'Top {top_n} Most Viewed Items (PV)')
                    ax.set_xlabel('Item ID')
                    ax.set_ylabel('Number of Page Views (PV)')
                    ax.tick_params(axis='x', rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # Top N å•†å“ç±»ç›® (åŸºäºPVè¡Œä¸º)
                st.write("**çƒ­é—¨å•†å“ç±»ç›®**")
                top_categories_pv = pv_df['category_id'].value_counts().head(top_n)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**Top {top_n} æœ€å—å…³æ³¨ç±»ç›® (PV):**")
                    st.write(top_categories_pv)
                
                with col2:
                    fig, ax = plt.subplots(figsize=(12, 6))
                    sns.barplot(x=top_categories_pv.index, y=top_categories_pv.values, palette='autumn', ax=ax)
                    ax.set_title(f'Top {top_n} Most Viewed Categories (PV)')
                    ax.set_xlabel('Category ID')
                    ax.set_ylabel('Number of Page Views (PV)')
                    ax.tick_params(axis='x', rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
            
            # Top N è´­ä¹°çš„å•†å“
            buy_df = df[df['behavior_type'] == 'buy']
            if not buy_df.empty:
                st.subheader("è´­ä¹°è¡Œä¸ºåˆ†æ")
                top_items_buy = buy_df['item_id'].value_counts().head(top_n)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**Top {top_n} æœ€å¤šè´­ä¹°å•†å“:**")
                    st.write(top_items_buy)
                
                with col2:
                    fig, ax = plt.subplots(figsize=(12, 6))
                    sns.barplot(x=top_items_buy.index, y=top_items_buy.values, palette='winter', ax=ax)
                    ax.set_title(f'Top {top_n} Most Purchased Items')
                    ax.set_xlabel('Item ID')
                    ax.set_ylabel('Number of Purchases')
                    ax.tick_params(axis='x', rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
            else:
                st.info("æ•°æ®ä¸­æ²¡æœ‰è´­ä¹°è¡Œä¸ºï¼Œæ— æ³•æ˜¾ç¤ºè´­ä¹°å•†å“åˆ†æ")
            
            # ä¸åŒè¡Œä¸ºç±»å‹çš„æ—¶é—´åˆ†å¸ƒ
            if 'date' in df.columns:
                st.subheader("è¡Œä¸ºç±»å‹æ—¶é—´åˆ†å¸ƒ")
                
                # æŒ‰æ—¥æœŸå’Œè¡Œä¸ºç±»å‹åˆ†ç»„
                behaviors_by_date_type = df.groupby(['date', 'behavior_type'])['user_id'].count().unstack('behavior_type').fillna(0)
                
                fig, ax = plt.subplots(figsize=(14, 7))
                behaviors_by_date_type.plot(kind='line', marker='.', ax=ax)
                ax.set_title('User Behaviors per Day by Type')
                ax.set_xlabel('Date')
                ax.set_ylabel('Number of Behaviors')
                ax.tick_params(axis='x', rotation=45)
                ax.legend(title='Behavior Type')
                ax.grid(True)
                plt.tight_layout()
                st.pyplot(fig)
                
                if 'hour' in df.columns:
                    # æŒ‰å°æ—¶å’Œè¡Œä¸ºç±»å‹åˆ†ç»„
                    behaviors_by_hour_type = df.groupby(['hour', 'behavior_type'])['user_id'].count().unstack('behavior_type').fillna(0)
                    
                    fig, ax = plt.subplots(figsize=(14, 7))
                    behaviors_by_hour_type.plot(kind='line', marker='.', ax=ax)
                    ax.set_title('User Behaviors per Hour by Type')
                    ax.set_xlabel('Hour of Day')
                    ax.set_ylabel('Number of Behaviors')
                    ax.legend(title='Behavior Type')
                    ax.grid(True)
                    plt.tight_layout()
                    st.pyplot(fig)
        
        with tab4:
            st.subheader("ğŸ‘¥ ç”¨æˆ·è¡Œä¸ºåˆ†æ")
            
            # ç”¨æˆ·å¹³å‡è¡Œä¸ºæ¬¡æ•°
            user_behavior_counts = df.groupby('user_id')['behavior_type'].count()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç”¨æˆ·å¹³å‡è¡Œä¸ºæ¬¡æ•°", f"{user_behavior_counts.mean():.2f}")
            with col2:
                st.metric("ç”¨æˆ·è¡Œä¸ºæ¬¡æ•°ä¸­ä½æ•°", f"{user_behavior_counts.median():.2f}")
            with col3:
                st.metric("æœ€æ´»è·ƒç”¨æˆ·è¡Œä¸ºæ¬¡æ•°", f"{user_behavior_counts.max()}")
            
            # ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ
            st.subheader("ç”¨æˆ·è¡Œä¸ºæ¬¡æ•°åˆ†å¸ƒ")
            fig, ax = plt.subplots(figsize=(10, 6))
            # ä½¿ç”¨matplotlibçš„histè€Œä¸æ˜¯seabornçš„histplotæ¥é¿å…å…¼å®¹æ€§é—®é¢˜
            ax.hist(user_behavior_counts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            ax.set_title('Distribution of Number of Behaviors per User')
            ax.set_xlabel('Number of Behaviors')
            ax.set_ylabel('Number of Users')
            ax.set_yscale('log')
            ax.grid(True, alpha=0.3)
            st.pyplot(fig)
            
            # æ´»è·ƒç”¨æˆ·
            st.subheader("æœ€æ´»è·ƒç”¨æˆ·")
            top_n_users = st.slider("æ˜¾ç¤ºTop Nç”¨æˆ·", min_value=5, max_value=20, value=10, key="top_users")
            top_active_users = user_behavior_counts.sort_values(ascending=False).head(top_n_users)
            
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**Top {top_n_users} æœ€æ´»è·ƒç”¨æˆ·:**")
                st.write(top_active_users)
            
            with col2:
                fig, ax = plt.subplots(figsize=(12, 6))
                sns.barplot(x=top_active_users.index, y=top_active_users.values, palette="crest", ax=ax)
                ax.set_title(f"Top {top_n_users} Most Active Users")
                ax.set_xlabel("User ID")
                ax.set_ylabel("Total Behaviors")
                ax.tick_params(axis='x', rotation=45)
                plt.tight_layout()
                st.pyplot(fig)
            
            # è½¬åŒ–ç‡åˆ†æ
            st.subheader("è½¬åŒ–ç‡åˆ†æ")
            total_pv = behavior_counts.get('pv', 0)
            total_buy = behavior_counts.get('buy', 0)
            
            if total_pv > 0:
                pv_to_buy_ratio = (total_buy / total_pv) * 100
                st.metric("å…¨å±€ PV åˆ° Buy è½¬åŒ–ç‡", f"{pv_to_buy_ratio:.2f}%")
            else:
                st.info("æ•°æ®ä¸­æ²¡æœ‰PVè¡Œä¸ºï¼Œæ— æ³•è®¡ç®—è½¬åŒ–ç‡")
        
        with tab5:
            st.subheader("ğŸ”„ ç”¨æˆ·è¡Œä¸ºåºåˆ—åˆ†æ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ—¶é—´æˆ³åˆ—
            if 'timestamp_dt' not in df.columns and 'timestamp' not in df.columns:
                st.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œåºåˆ—åˆ†æ")
                return
            
            with st.spinner("æ„å»ºç”¨æˆ·è¡Œä¸ºåºåˆ—..."):
                try:
                    # ç¡®ä¿æ•°æ®æŒ‰ç…§ç”¨æˆ·IDå’Œæ—¶é—´æˆ³æ’åº
                    if 'timestamp_dt' in df.columns:
                        df_sorted = df.sort_values(by=['user_id', 'timestamp_dt'], ascending=True)
                    else:
                        df_sorted = df.sort_values(by=['user_id', 'timestamp'], ascending=True)
                    
                    # ä¸ºæ¯ä¸ªç”¨æˆ·æ„å»ºè¡Œä¸ºåºåˆ—
                    user_sequences = df_sorted.groupby('user_id').agg(
                        item_sequence=('item_id', list),
                        behavior_sequence=('behavior_type', list),
                        category_sequence=('category_id', list)
                    ).reset_index()
                    
                    # è®¡ç®—åºåˆ—é•¿åº¦
                    user_sequences['sequence_length'] = user_sequences['item_sequence'].apply(len)
                    
                    st.success(f"æˆåŠŸä¸º {len(user_sequences):,} ä¸ªç”¨æˆ·æ„å»ºäº†è¡Œä¸ºåºåˆ—")
                    
                    # åºåˆ—é•¿åº¦åˆ†æ
                    st.subheader("åºåˆ—é•¿åº¦åˆ†æ")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å¹³å‡åºåˆ—é•¿åº¦", f"{user_sequences['sequence_length'].mean():.2f}")
                    with col2:
                        st.metric("åºåˆ—é•¿åº¦ä¸­ä½æ•°", f"{user_sequences['sequence_length'].median():.2f}")
                    with col3:
                        st.metric("æœ€é•¿åºåˆ—", f"{user_sequences['sequence_length'].max()}")
                    
                    # åºåˆ—é•¿åº¦åˆ†å¸ƒ
                    fig, ax = plt.subplots(figsize=(12, 7))
                    ax.hist(user_sequences['sequence_length'], bins=100, alpha=0.7, color='lightblue', edgecolor='black')
                    ax.set_title('Distribution of User Sequence Lengths')
                    ax.set_xlabel('Sequence Length (Number of Actions per User)')
                    ax.set_ylabel('Number of Users')
                    ax.set_yscale('log')
                    ax.grid(True, alpha=0.3)
                    st.pyplot(fig)
                    
                    # è´­ä¹°è¡Œä¸ºåˆ†æ
                    st.subheader("è´­ä¹°è¡Œä¸ºåºåˆ—åˆ†æ")
                    
                    def has_purchase(behavior_list):
                        return 'buy' in behavior_list
                    
                    user_sequences['has_purchase'] = user_sequences['behavior_sequence'].apply(has_purchase)
                    purchase_user_count = user_sequences['has_purchase'].sum()
                    total_users_in_sequences = len(user_sequences)
                    purchase_percentage = (purchase_user_count / total_users_in_sequences) * 100 if total_users_in_sequences > 0 else 0
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("æœ‰è´­ä¹°è¡Œä¸ºçš„ç”¨æˆ·", f"{purchase_user_count:,}")
                    with col2:
                        st.metric("è´­ä¹°ç”¨æˆ·å æ¯”", f"{purchase_percentage:.2f}%")
                    
                    # è´­ä¹°ç”¨æˆ· vs æœªè´­ä¹°ç”¨æˆ·çš„åºåˆ—é•¿åº¦å¯¹æ¯”
                    if purchase_user_count > 0:
                        fig, ax = plt.subplots(figsize=(8, 6))
                        # ä½¿ç”¨matplotlibåˆ›å»ºç®±çº¿å›¾
                        purchase_lengths = user_sequences[user_sequences['has_purchase']]['sequence_length']
                        no_purchase_lengths = user_sequences[~user_sequences['has_purchase']]['sequence_length']
                        
                        ax.boxplot([no_purchase_lengths, purchase_lengths], labels=['No Purchase', 'Has Purchase'])
                        ax.set_title('Sequence Length by Purchase Behavior')
                        ax.set_ylabel('Sequence Length')
                        ax.set_yscale('log')
                        st.pyplot(fig)
                    
                    # è¡Œä¸ºç±»å‹ç»Ÿè®¡
                    st.subheader("ç”¨æˆ·è¡Œä¸ºç±»å‹ç»Ÿè®¡")
                    behavior_types = ['pv', 'cart', 'fav', 'buy']
                    
                    for b_type in behavior_types:
                        if b_type in df['behavior_type'].values:
                            user_sequences[f'{b_type}_count'] = user_sequences['behavior_sequence'].apply(lambda x: x.count(b_type))
                    
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    stats_cols = [col for col in user_sequences.columns if col.endswith('_count')]
                    if stats_cols:
                        st.write("**å„è¡Œä¸ºç±»å‹ç»Ÿè®¡æè¿°:**")
                        st.write(user_sequences[stats_cols].describe())
                    
                    # ç”¨æˆ·å…´è¶£å¤šæ ·æ€§
                    st.subheader("ç”¨æˆ·å…´è¶£å¤šæ ·æ€§")
                    user_sequences['unique_items_count'] = user_sequences['item_sequence'].apply(lambda x: len(set(x)))
                    user_sequences['unique_categories_count'] = user_sequences['category_sequence'].apply(lambda x: len(set(x)))
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("å¹³å‡å…³æ³¨å•†å“æ•°", f"{user_sequences['unique_items_count'].mean():.2f}")
                    with col2:
                        st.metric("å¹³å‡å…³æ³¨ç±»ç›®æ•°", f"{user_sequences['unique_categories_count'].mean():.2f}")
                    
                except Exception as e:
                    st.error(f"åºåˆ—åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                    st.info("è¿™å¯èƒ½æ˜¯ç”±äºæ•°æ®é‡è¿‡å¤§æˆ–æ ¼å¼é—®é¢˜å¯¼è‡´çš„ã€‚å»ºè®®å°è¯•ä½¿ç”¨è¾ƒå°çš„æ•°æ®æ ·æœ¬ã€‚")
    
    def render_user_behavior_analysis(self):
        """æ¸²æŸ“ç”¨æˆ·è¡Œä¸ºåˆ†æé¡µé¢"""
        st.title("ğŸ‘¥ ç”¨æˆ·è¡Œä¸ºåˆ†æ")
        
        if self.data is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return
        
        # ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ
        st.subheader("ğŸ“ˆ ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ")
        
        # åŠ¨æ€æ£€æŸ¥å¯ç”¨çš„æ—¶é—´åˆ—
        time_columns = ['timestamp_dt', 'date', 'datetime', 'timestamp']
        available_time_column = None
        for col in time_columns:
            if col in self.data.columns:
                available_time_column = col
                break
        
        # æ„å»ºèšåˆå­—å…¸
        agg_dict = {
            'behavior_type': 'count',
            'item_id': 'nunique'
        }
        
        # å¦‚æœæœ‰æ—¶é—´åˆ—ï¼Œæ·»åŠ æ—¶é—´ç›¸å…³çš„èšåˆ
        if available_time_column:
            agg_dict[available_time_column] = ['min', 'max']
            column_names = ['æ€»è¡Œä¸ºæ•°', 'æµè§ˆå•†å“æ•°', 'é¦–æ¬¡æ´»è·ƒ', 'æœ€åæ´»è·ƒ']
        else:
            column_names = ['æ€»è¡Œä¸ºæ•°', 'æµè§ˆå•†å“æ•°']
        
        user_activity = self.data.groupby('user_id').agg(agg_dict).round(2)
        user_activity.columns = column_names
        
        # æ´»è·ƒåº¦åˆ†å¸ƒ
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.histogram(
                user_activity['æ€»è¡Œä¸ºæ•°'],
                title="ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ",
                labels={'value': 'æ€»è¡Œä¸ºæ•°', 'count': 'ç”¨æˆ·æ•°é‡'},
                nbins=50
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.histogram(
                user_activity['æµè§ˆå•†å“æ•°'],
                title="ç”¨æˆ·æµè§ˆå•†å“æ•°åˆ†å¸ƒ",
                labels={'value': 'æµè§ˆå•†å“æ•°', 'count': 'ç”¨æˆ·æ•°é‡'},
                nbins=50
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ
        st.subheader("ğŸ¯ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼")
        
        # è®¡ç®—ç”¨æˆ·è½¬åŒ–ç‡
        user_behavior_analysis = []
        
        for user_id in self.data['user_id'].unique()[:1000]:  # é™åˆ¶åˆ†æç”¨æˆ·æ•°é‡
            user_data = self.data[self.data['user_id'] == user_id]
            behavior_counts = user_data['behavior_type'].value_counts()
            
            pv_count = behavior_counts.get('pv', 0)
            cart_count = behavior_counts.get('cart', 0)
            fav_count = behavior_counts.get('fav', 0)
            buy_count = behavior_counts.get('buy', 0)
            
            user_behavior_analysis.append({
                'user_id': user_id,
                'pv_count': pv_count,
                'cart_count': cart_count,
                'fav_count': fav_count,
                'buy_count': buy_count,
                'pv_to_cart_rate': cart_count / pv_count if pv_count > 0 else 0,
                'pv_to_buy_rate': buy_count / pv_count if pv_count > 0 else 0,
                'cart_to_buy_rate': buy_count / cart_count if cart_count > 0 else 0
            })
        
        behavior_df = pd.DataFrame(user_behavior_analysis)
        
        # è½¬åŒ–ç‡åˆ†å¸ƒ
        col1, col2, col3 = st.columns(3)
        
        with col1:
            fig = px.histogram(
                behavior_df['pv_to_cart_rate'],
                title="æµè§ˆåˆ°åŠ è´­è½¬åŒ–ç‡åˆ†å¸ƒ",
                labels={'value': 'è½¬åŒ–ç‡', 'count': 'ç”¨æˆ·æ•°é‡'},
                nbins=30
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.histogram(
                behavior_df['pv_to_buy_rate'],
                title="æµè§ˆåˆ°è´­ä¹°è½¬åŒ–ç‡åˆ†å¸ƒ",
                labels={'value': 'è½¬åŒ–ç‡', 'count': 'ç”¨æˆ·æ•°é‡'},
                nbins=30
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col3:
            fig = px.histogram(
                behavior_df['cart_to_buy_rate'],
                title="åŠ è´­åˆ°è´­ä¹°è½¬åŒ–ç‡åˆ†å¸ƒ",
                labels={'value': 'è½¬åŒ–ç‡', 'count': 'ç”¨æˆ·æ•°é‡'},
                nbins=30
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡è¡¨
        st.subheader("ğŸ“Š ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡")
        
        summary_stats = behavior_df[['pv_to_cart_rate', 'pv_to_buy_rate', 'cart_to_buy_rate']].describe()
        st.dataframe(summary_stats, use_container_width=True)
    
    def render_user_segmentation(self):
        """ç”¨æˆ·ç”»åƒåˆ†æé¡µé¢ - åŸºäºK-Meansèšç±»çš„ç”¨æˆ·åˆ†ç¾¤"""
        st.header("ğŸ‘¥ ç”¨æˆ·ç”»åƒåˆ†æ")
        
        if self.data is None or self.data.empty:
            st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return
        
        df = self.data
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ—¶é—´æˆ³åˆ—
        if 'timestamp_dt' not in df.columns and 'timestamp' not in df.columns:
            st.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œç”¨æˆ·ç”»åƒåˆ†æ")
            return
        
        st.markdown("é€šè¿‡ç”¨æˆ·è¡Œä¸ºåºåˆ—ç‰¹å¾è¿›è¡Œèšç±»åˆ†æï¼Œè¯†åˆ«ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ç±»å‹")
        
        with st.spinner("æ„å»ºç”¨æˆ·è¡Œä¸ºåºåˆ—ç‰¹å¾..."):
            try:
                # ç¡®ä¿æ•°æ®æŒ‰ç…§ç”¨æˆ·IDå’Œæ—¶é—´æˆ³æ’åº
                if 'timestamp_dt' in df.columns:
                    df_sorted = df.sort_values(by=['user_id', 'timestamp_dt'], ascending=True)
                else:
                    df_sorted = df.sort_values(by=['user_id', 'timestamp'], ascending=True)
                
                # ä¸ºæ¯ä¸ªç”¨æˆ·æ„å»ºè¡Œä¸ºåºåˆ—
                user_sequences = df_sorted.groupby('user_id').agg(
                    item_sequence=('item_id', list),
                    behavior_sequence=('behavior_type', list),
                    category_sequence=('category_id', list)
                ).reset_index()
                
                # è®¡ç®—èšç±»ç‰¹å¾
                user_sequences['sequence_length'] = user_sequences['item_sequence'].apply(len)
                
                # å„ç§è¡Œä¸ºç±»å‹çš„è®¡æ•°
                behavior_types = ['pv', 'cart', 'fav', 'buy']
                for b_type in behavior_types:
                    user_sequences[f'{b_type}_count'] = user_sequences['behavior_sequence'].apply(lambda x: x.count(b_type))
                
                # äº¤äº’çš„ç‹¬ç«‹å•†å“æ•°å’Œç±»ç›®æ•°
                user_sequences['unique_items_count'] = user_sequences['item_sequence'].apply(lambda x: len(set(x)))
                user_sequences['unique_categories_count'] = user_sequences['category_sequence'].apply(lambda x: len(set(x)))
                
                # è´­ä¹°è½¬åŒ–ç‡
                user_sequences['user_pv_to_buy_conversion_rate'] = user_sequences.apply(
                    lambda row: (row['buy_count'] / row['pv_count'] * 100) if row['pv_count'] > 0 else 0, axis=1
                )
                
                st.success(f"âœ… æˆåŠŸæ„å»º {len(user_sequences):,} ä¸ªç”¨æˆ·çš„è¡Œä¸ºç‰¹å¾")
                
                # åˆ›å»ºé€‰é¡¹å¡
                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "ğŸ”§ ç‰¹å¾å·¥ç¨‹", 
                    "ğŸ“Š èšç±»åˆ†æ", 
                    "ğŸ¯ ç”¨æˆ·ç”»åƒ",
                    "ğŸ“ˆ ç¾¤ä½“å¯¹æ¯”",
                    "ğŸ“Š RFMåˆ†æ"
                ])
                
                with tab1:
                    st.subheader("ğŸ”§ ç”¨æˆ·è¡Œä¸ºç‰¹å¾å·¥ç¨‹")
                    
                    # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
                    features_for_clustering = [
                        'sequence_length', 'pv_count', 'cart_count', 'fav_count', 
                        'buy_count', 'unique_items_count', 'unique_categories_count', 
                        'user_pv_to_buy_conversion_rate'
                    ]
                    
                    st.write("**é€‰æ‹©çš„èšç±»ç‰¹å¾:**")
                    for feature in features_for_clustering:
                        st.write(f"- {feature}")
                    
                    # ç‰¹å¾æè¿°æ€§ç»Ÿè®¡
                    st.subheader("ç‰¹å¾æè¿°æ€§ç»Ÿè®¡")
                    clustering_data = user_sequences[features_for_clustering].copy()
                    
                    # å¤„ç†å¼‚å¸¸å€¼
                    if clustering_data.isnull().sum().any():
                        st.warning("âš ï¸ æ£€æµ‹åˆ°ç©ºå€¼ï¼Œå°†ç”¨ä¸­ä½æ•°å¡«å……")
                        for col in clustering_data.columns[clustering_data.isnull().any()]:
                            clustering_data[col] = clustering_data[col].fillna(clustering_data[col].median())
                    
                    if np.isinf(clustering_data.values).any():
                        st.warning("âš ï¸ æ£€æµ‹åˆ°æ— ç©·å€¼ï¼Œå°†è¿›è¡Œå¤„ç†")
                        clustering_data.replace([np.inf, -np.inf], np.nan, inplace=True)
                        for col in clustering_data.columns[clustering_data.isnull().any()]:
                            clustering_data[col] = clustering_data[col].fillna(clustering_data[col].median())
                    
                    st.write(clustering_data.describe())
                    
                    # ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–
                    st.subheader("ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–")
                    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
                    axes = axes.ravel()
                    
                    for i, feature in enumerate(features_for_clustering):
                        axes[i].hist(clustering_data[feature], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
                        axes[i].set_title(f'{feature}')
                        axes[i].set_xlabel(feature)
                        axes[i].set_ylabel('é¢‘æ¬¡')
                        axes[i].set_yscale('log')
                        axes[i].grid(True, alpha=0.3)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                
                with tab2:
                    st.subheader("ğŸ“Š K-Means èšç±»åˆ†æ")
                    
                    # ç‰¹å¾æ ‡å‡†åŒ–
                    scaler = StandardScaler()
                    scaled_features = scaler.fit_transform(clustering_data)
                    st.success("âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
                    
                    # è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜Kå€¼
                    st.subheader("è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜èšç±»æ•°")
                    
                    with st.spinner("è®¡ç®—ä¸åŒKå€¼çš„æƒ¯æ€§..."):
                        possible_k_values = range(2, 11)
                        inertia_values = []
                        
                        for k in possible_k_values:
                            kmeans_temp = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
                            kmeans_temp.fit(scaled_features)
                            inertia_values.append(kmeans_temp.inertia_)
                    
                    # ç»˜åˆ¶è‚˜éƒ¨æ³•åˆ™å›¾
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.plot(possible_k_values, inertia_values, marker='o', linestyle='-', linewidth=2, markersize=8)
                    ax.set_title('è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜Kå€¼', fontsize=14)
                    ax.set_xlabel('èšç±»æ•°é‡ (K)')
                    ax.set_ylabel('æƒ¯æ€§å€¼ (Inertia)')
                    ax.grid(True, linestyle='--', alpha=0.7)
                    ax.set_xticks(possible_k_values)
                    
                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for i, (k, inertia) in enumerate(zip(possible_k_values, inertia_values)):
                        ax.annotate(f'{inertia:.0f}', (k, inertia), textcoords="offset points", 
                                   xytext=(0,10), ha='center', fontsize=9)
                    
                    st.pyplot(fig)
                    
                    # è®©ç”¨æˆ·é€‰æ‹©Kå€¼
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**å„Kå€¼å¯¹åº”çš„æƒ¯æ€§å€¼:**")
                        for k, inertia in zip(possible_k_values, inertia_values):
                            st.write(f"K={k}: {inertia:.2f}")
                    
                    with col2:
                        chosen_k = st.selectbox(
                            "æ ¹æ®è‚˜éƒ¨æ³•åˆ™å›¾é€‰æ‹©æœ€ä¼˜Kå€¼:",
                            options=list(possible_k_values),
                            index=3,  # é»˜è®¤é€‰æ‹©K=5
                            help="é€šå¸¸é€‰æ‹©æƒ¯æ€§å€¼ä¸‹é™è¶‹åŠ¿æ˜æ˜¾æ”¾ç¼“çš„æ‹ç‚¹"
                        )
                    
                    # æ‰§è¡Œèšç±»
                    if st.button("ğŸš€ æ‰§è¡ŒK-Meansèšç±»", type="primary"):
                        with st.spinner(f"æ‰§è¡ŒK={chosen_k}èšç±»åˆ†æ..."):
                            kmeans = KMeans(n_clusters=chosen_k, init='k-means++', n_init=10, random_state=42)
                            cluster_labels = kmeans.fit_predict(scaled_features)
                            
                            # å°†èšç±»ç»“æœä¿å­˜åˆ°session state
                            st.session_state.user_sequences = user_sequences.copy()
                            st.session_state.user_sequences['cluster'] = cluster_labels
                            st.session_state.chosen_k = chosen_k
                            st.session_state.clustering_data = clustering_data
                            st.session_state.features_for_clustering = features_for_clustering
                            
                            st.success(f"âœ… èšç±»å®Œæˆï¼æˆåŠŸå°†ç”¨æˆ·åˆ†ä¸º {chosen_k} ä¸ªç¾¤ä½“")
                
                with tab3:
                    st.subheader("ğŸ¯ ç”¨æˆ·ç¾¤ä½“ç”»åƒ")
                    
                    if 'user_sequences' not in st.session_state:
                        st.info("è¯·å…ˆåœ¨ 'èšç±»åˆ†æ' æ ‡ç­¾é¡µæ‰§è¡Œèšç±»åˆ†æ")
                        return
                    
                    user_sequences_with_clusters = st.session_state.user_sequences
                    chosen_k = st.session_state.chosen_k
                    features_for_clustering = st.session_state.features_for_clustering
                    
                    # å„ç¾¤ä½“ç”¨æˆ·æ•°é‡
                    cluster_counts = user_sequences_with_clusters['cluster'].value_counts().sort_index()
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**å„ç¾¤ä½“ç”¨æˆ·æ•°é‡:**")
                        for cluster_id, count in cluster_counts.items():
                            percentage = (count / len(user_sequences_with_clusters)) * 100
                            st.write(f"ç¾¤ä½“ {cluster_id}: {count:,} ç”¨æˆ· ({percentage:.1f}%)")
                    
                    with col2:
                        # ç¾¤ä½“åˆ†å¸ƒé¥¼å›¾
                        fig, ax = plt.subplots(figsize=(8, 8))
                        colors = plt.cm.Set3(np.linspace(0, 1, chosen_k))
                        wedges, texts, autotexts = ax.pie(cluster_counts.values, 
                                                         labels=[f'ç¾¤ä½“ {i}' for i in cluster_counts.index],
                                                         autopct='%1.1f%%',
                                                         colors=colors,
                                                         startangle=90)
                        ax.set_title(f'ç”¨æˆ·ç¾¤ä½“åˆ†å¸ƒ (K={chosen_k})', fontsize=14)
                        st.pyplot(fig)
                    
                    # ç¾¤ä½“ç‰¹å¾ç”»åƒ
                    st.subheader("ç¾¤ä½“ç‰¹å¾ç”»åƒå¯¹æ¯”")
                    cluster_profiles = user_sequences_with_clusters.groupby('cluster')[features_for_clustering].mean()
                    
                    # æ˜¾ç¤ºæ•°å€¼è¡¨æ ¼
                    st.write("**å„ç¾¤ä½“ç‰¹å¾å‡å€¼:**")
                    st.dataframe(cluster_profiles.round(2))
                    
                    # å¯è§†åŒ–ç¾¤ä½“ç”»åƒ
                    profile_plot_data = cluster_profiles.reset_index().melt(
                        id_vars='cluster', var_name='feature', value_name='mean_value'
                    )
                    
                    # åˆ›å»ºå­å›¾
                    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
                    axes = axes.ravel()
                    
                    for i, feature in enumerate(features_for_clustering):
                        feature_data = profile_plot_data[profile_plot_data['feature'] == feature]
                        
                        bars = axes[i].bar(feature_data['cluster'], feature_data['mean_value'], 
                                          color=plt.cm.viridis(np.linspace(0, 1, chosen_k)))
                        axes[i].set_title(f'{feature}', fontsize=12)
                        axes[i].set_xlabel('ç¾¤ä½“ ID')
                        axes[i].set_ylabel('å¹³å‡å€¼')
                        axes[i].grid(True, alpha=0.3)
                        
                        # æ·»åŠ æ•°å€¼æ ‡ç­¾
                        for bar, value in zip(bars, feature_data['mean_value']):
                            height = bar.get_height()
                            axes[i].text(bar.get_x() + bar.get_width()/2., height,
                                       f'{value:.1f}', ha='center', va='bottom', fontsize=9)
                    
                    plt.suptitle(f'å„ç¾¤ä½“ç‰¹å¾ç”»åƒå¯¹æ¯” (K={chosen_k})', fontsize=16, y=1.02)
                    plt.tight_layout()
                    st.pyplot(fig)
                
                with tab4:
                    st.subheader("ğŸ“ˆ ç”¨æˆ·ç¾¤ä½“æ·±åº¦å¯¹æ¯”")
                    
                    if 'user_sequences' not in st.session_state:
                        st.info("è¯·å…ˆåœ¨ 'èšç±»åˆ†æ' æ ‡ç­¾é¡µæ‰§è¡Œèšç±»åˆ†æ")
                        return
                    
                    user_sequences_with_clusters = st.session_state.user_sequences
                    
                    # é€‰æ‹©è¦å¯¹æ¯”çš„ç¾¤ä½“
                    cluster_ids = sorted(user_sequences_with_clusters['cluster'].unique())
                    selected_clusters = st.multiselect(
                        "é€‰æ‹©è¦å¯¹æ¯”çš„ç”¨æˆ·ç¾¤ä½“:",
                        options=cluster_ids,
                        default=cluster_ids[:3] if len(cluster_ids) >= 3 else cluster_ids,
                        help="å¯ä»¥é€‰æ‹©å¤šä¸ªç¾¤ä½“è¿›è¡Œå¯¹æ¯”åˆ†æ"
                    )
                    
                    if len(selected_clusters) < 2:
                        st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªç¾¤ä½“è¿›è¡Œå¯¹æ¯”")
                        return
                    
                    # è´­ä¹°è¡Œä¸ºå¯¹æ¯”
                    st.subheader("è´­ä¹°è¡Œä¸ºå¯¹æ¯”")
                    
                    def has_purchase(behavior_list):
                        return 'buy' in behavior_list
                    
                    purchase_stats = []
                    for cluster_id in selected_clusters:
                        cluster_data = user_sequences_with_clusters[user_sequences_with_clusters['cluster'] == cluster_id]
                        has_purchase_count = cluster_data['behavior_sequence'].apply(has_purchase).sum()
                        total_users = len(cluster_data)
                        purchase_rate = (has_purchase_count / total_users * 100) if total_users > 0 else 0
                        
                        purchase_stats.append({
                            'cluster': f'ç¾¤ä½“ {cluster_id}',
                            'total_users': total_users,
                            'buyers': has_purchase_count,
                            'purchase_rate': purchase_rate
                        })
                    
                    purchase_df = pd.DataFrame(purchase_stats)
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**è´­ä¹°è¡Œä¸ºç»Ÿè®¡:**")
                        st.dataframe(purchase_df)
                    
                    with col2:
                        fig, ax = plt.subplots(figsize=(8, 6))
                        bars = ax.bar(purchase_df['cluster'], purchase_df['purchase_rate'], 
                                     color=plt.cm.viridis(np.linspace(0, 1, len(selected_clusters))))
                        ax.set_title('å„ç¾¤ä½“è´­ä¹°è½¬åŒ–ç‡å¯¹æ¯”')
                        ax.set_ylabel('è´­ä¹°è½¬åŒ–ç‡ (%)')
                        ax.set_xlabel('ç”¨æˆ·ç¾¤ä½“')
                        
                        # æ·»åŠ æ•°å€¼æ ‡ç­¾
                        for bar, rate in zip(bars, purchase_df['purchase_rate']):
                            height = bar.get_height()
                            ax.text(bar.get_x() + bar.get_width()/2., height,
                                   f'{rate:.1f}%', ha='center', va='bottom')
                        
                        plt.xticks(rotation=45)
                        plt.tight_layout()
                        st.pyplot(fig)
                    
                    # è¡Œä¸ºåºåˆ—é•¿åº¦å¯¹æ¯”
                    st.subheader("è¡Œä¸ºæ´»è·ƒåº¦å¯¹æ¯”")
                    
                    sequence_length_data = []
                    for cluster_id in selected_clusters:
                        cluster_data = user_sequences_with_clusters[user_sequences_with_clusters['cluster'] == cluster_id]
                        sequence_length_data.extend([(f'ç¾¤ä½“ {cluster_id}', length) 
                                                   for length in cluster_data['sequence_length']])
                    
                    sequence_df = pd.DataFrame(sequence_length_data, columns=['cluster', 'sequence_length'])
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    
                    # åˆ›å»ºç®±çº¿å›¾
                    cluster_names = [f'ç¾¤ä½“ {cid}' for cid in selected_clusters]
                    sequence_data_by_cluster = [sequence_df[sequence_df['cluster'] == name]['sequence_length'].values 
                                              for name in cluster_names]
                    
                    box_plot = ax.boxplot(sequence_data_by_cluster, labels=cluster_names, patch_artist=True)
                    
                    # è®¾ç½®é¢œè‰²
                    colors = plt.cm.viridis(np.linspace(0, 1, len(selected_clusters)))
                    for patch, color in zip(box_plot['boxes'], colors):
                        patch.set_facecolor(color)
                        patch.set_alpha(0.7)
                    
                    ax.set_title('å„ç¾¤ä½“ç”¨æˆ·è¡Œä¸ºåºåˆ—é•¿åº¦åˆ†å¸ƒ')
                    ax.set_ylabel('åºåˆ—é•¿åº¦ï¼ˆè¡Œä¸ºæ¬¡æ•°ï¼‰')
                    ax.set_xlabel('ç”¨æˆ·ç¾¤ä½“')
                    ax.set_yscale('log')
                    ax.grid(True, alpha=0.3)
                    
                    plt.xticks(rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # ç”¨æˆ·ç”»åƒè§£è¯»
                st.markdown("---")
                st.subheader("ğŸ“‹ ç”¨æˆ·ç”»åƒè§£è¯»å»ºè®®")
                st.markdown("""
                **åŸºäºèšç±»ç»“æœï¼Œå¯ä»¥ä»ä»¥ä¸‹è§’åº¦è§£è¯»ç”¨æˆ·ç¾¤ä½“:**
                
                1. **é«˜ä»·å€¼ç”¨æˆ·ç¾¤** - è´­ä¹°é¢‘æ¬¡é«˜ã€è½¬åŒ–ç‡é«˜ã€æ´»è·ƒåº¦é«˜
                   - ç‰¹å¾ï¼šbuy_counté«˜ã€user_pv_to_buy_conversion_rateé«˜ã€sequence_lengthè¾ƒé«˜
                   - ç­–ç•¥ï¼šVIPæœåŠ¡ã€å¿ è¯šåº¦è®¡åˆ’ã€é«˜ç«¯å•†å“æ¨è
                
                2. **æ½œåŠ›ç”¨æˆ·ç¾¤** - æ´»è·ƒåº¦é«˜ä½†è´­ä¹°è¾ƒå°‘
                   - ç‰¹å¾ï¼šsequence_lengthé«˜ã€pv_counté«˜ï¼Œä½†buy_countä½
                   - ç­–ç•¥ï¼šç²¾å‡†æ¨èã€ä¼˜æƒ ä¿ƒé”€ã€è´­ä¹°å¼•å¯¼
                
                3. **æµè§ˆå‹ç”¨æˆ·** - æµè§ˆå¤šä½†å¾ˆå°‘è´­ä¹°
                   - ç‰¹å¾ï¼špv_counté«˜ã€cart_countæˆ–fav_countä¸€èˆ¬ï¼Œbuy_countå¾ˆä½
                   - ç­–ç•¥ï¼šå†…å®¹ä¼˜åŒ–ã€å…´è¶£å¼•å¯¼ã€ä¿¡ä»»å»ºè®¾
                
                4. **ä½é¢‘ç”¨æˆ·** - å„é¡¹æŒ‡æ ‡éƒ½è¾ƒä½
                   - ç‰¹å¾ï¼šæ‰€æœ‰è®¡æ•°æŒ‡æ ‡éƒ½åä½
                   - ç­–ç•¥ï¼šæ¿€æ´»è¥é”€ã€æ–°ç”¨æˆ·å¼•å¯¼ã€åŸºç¡€æ¨è
                
                5. **ç›®æ ‡æ˜ç¡®ç”¨æˆ·** - æµè§ˆå°‘ä½†è½¬åŒ–ç‡é«˜
                   - ç‰¹å¾ï¼špv_countç›¸å¯¹è¾ƒä½ä½†buy_countä¸é”™
                   - ç­–ç•¥ï¼šç²¾å‡†åŒ¹é…ã€å¿«é€Ÿå“åº”ã€ç®€åŒ–æµç¨‹
                """)
                
                with tab5:
                    st.subheader("ğŸ“Š RFMåˆ†æ")
                    st.markdown("åŸºäºæœ€è¿‘æ€§(Recency)ã€é¢‘ç‡(Frequency)ã€è´§å¸ä»·å€¼(Monetary)è¿›è¡Œç”¨æˆ·ä»·å€¼åˆ†æ")
                    
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ—¶é—´æˆ³
                    if 'timestamp_dt' not in df.columns and 'date' not in df.columns:
                        st.warning("éœ€è¦æ—¶é—´ä¿¡æ¯æ¥è®¡ç®—RFMæŒ‡æ ‡ï¼Œè¯·ç¡®ä¿æ•°æ®åŒ…å«æ—¶é—´æˆ³")
                        return
                    
                    with st.spinner("è®¡ç®—RFMæŒ‡æ ‡..."):
                        # è®¡ç®—RFMæŒ‡æ ‡
                        try:
                            # ç¡®å®šå½“å‰æ—¥æœŸ
                            if 'timestamp_dt' in df.columns:
                                current_date = pd.to_datetime(df['timestamp_dt']).max()
                                date_column = 'timestamp_dt'
                            elif 'date' in df.columns:
                                current_date = pd.to_datetime(df['date']).max()
                                date_column = 'date'
                            else:
                                st.error("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´åˆ—")
                                return
                        except Exception as e:
                            st.error(f"æ—¥æœŸè§£æé”™è¯¯: {str(e)}")
                            return
                        
                        rfm_data = []
                        unique_users = df['user_id'].unique()
                        
                        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å–å‰10000ä¸ªç”¨æˆ·ï¼ˆå¦‚æœç”¨æˆ·æ•°è¿‡å¤šï¼‰
                        if len(unique_users) > 10000:
                            st.info(f"ç”¨æˆ·æ•°é‡è¾ƒå¤š({len(unique_users):,})ï¼Œå°†åˆ†æå‰10,000ä¸ªç”¨æˆ·")
                            unique_users = unique_users[:10000]
                        
                        # æ·»åŠ è¿›åº¦æ¡
                        progress_bar = st.progress(0)
                        progress_text = st.empty()
                        
                        for i, user_id in enumerate(unique_users):
                            # æ›´æ–°è¿›åº¦
                            if i % 1000 == 0:
                                progress = (i + 1) / len(unique_users)
                                progress_bar.progress(progress)
                                progress_text.text(f"å¤„ç†è¿›åº¦: {i+1:,}/{len(unique_users):,} ç”¨æˆ·")
                            
                            try:
                                user_data = df[df['user_id'] == user_id]
                                
                                # R - Recency: æœ€è¿‘ä¸€æ¬¡äº¤äº’è·ä»Šå¤©æ•°
                                if date_column == 'timestamp_dt':
                                    last_interaction = pd.to_datetime(user_data['timestamp_dt']).max()
                                else:
                                    last_interaction = pd.to_datetime(user_data['date']).max()
                                
                                # è®¡ç®—å¤©æ•°å·®
                                recency = (current_date - last_interaction).days
                                
                                # ç¡®ä¿recencyæ˜¯æœ‰æ•ˆæ•°å€¼
                                if pd.isna(recency) or recency < 0:
                                    recency = 999  # ç»™ä¸€ä¸ªé»˜è®¤çš„å¤§å€¼
                                
                                # F - Frequency: äº¤äº’é¢‘ç‡ï¼ˆæ€»è¡Œä¸ºæ¬¡æ•°ï¼‰
                                frequency = len(user_data)
                                
                                # M - Monetary: è´§å¸ä»·å€¼ï¼ˆè¿™é‡Œç”¨è´­ä¹°æ¬¡æ•°ä»£æ›¿ï¼Œå› ä¸ºæ²¡æœ‰é‡‘é¢æ•°æ®ï¼‰
                                monetary = len(user_data[user_data['behavior_type'] == 'buy']) if 'buy' in user_data['behavior_type'].values else 0
                                
                                # è®¡ç®—é¢å¤–çš„è¡Œä¸ºæŒ‡æ ‡
                                pv_count = len(user_data[user_data['behavior_type'] == 'pv'])
                                cart_count = len(user_data[user_data['behavior_type'] == 'cart'])
                                fav_count = len(user_data[user_data['behavior_type'] == 'fav'])
                                
                                # RFMåˆ†ç¾¤è§„åˆ™ï¼ˆè°ƒæ•´é˜ˆå€¼ä½¿å…¶æ›´åˆç†ï¼‰
                                if recency <= 3 and frequency >= 10 and monetary >= 2:
                                    segment = "å† å†›ç”¨æˆ·"
                                elif recency <= 7 and frequency >= 5 and monetary >= 1:
                                    segment = "å¿ è¯šç”¨æˆ·"
                                elif recency <= 3 and frequency < 5:
                                    segment = "æ–°ç”¨æˆ·"
                                elif recency > 7 and frequency >= 5:
                                    segment = "æµå¤±é£é™©ç”¨æˆ·"
                                elif monetary == 0 and frequency >= 3:
                                    segment = "æ½œåœ¨ç”¨æˆ·"
                                else:
                                    segment = "ä¸€èˆ¬ç”¨æˆ·"
                                
                                rfm_data.append({
                                    'user_id': user_id,
                                    'recency': int(recency),  # ç¡®ä¿æ˜¯æ•´æ•°
                                    'frequency': int(frequency),
                                    'monetary': int(monetary),
                                    'pv_count': int(pv_count),
                                    'cart_count': int(cart_count),
                                    'fav_count': int(fav_count),
                                    'segment': segment
                                })
                                
                            except Exception as user_error:
                                # å¦‚æœæŸä¸ªç”¨æˆ·å¤„ç†å¤±è´¥ï¼Œè·³è¿‡è¯¥ç”¨æˆ·
                                st.warning(f"è·³è¿‡ç”¨æˆ· {user_id}: {str(user_error)}")
                                continue
                        
                        # æ¸…é™¤è¿›åº¦æ¡
                        progress_bar.empty()
                        progress_text.empty()
                        
                        if not rfm_data:
                            st.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•ç”¨æˆ·æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
                            return
                        
                        rfm_df = pd.DataFrame(rfm_data)
                        
                        # æ•°æ®éªŒè¯
                        if len(rfm_df) == 0:
                            st.error("RFMè®¡ç®—ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®")
                            return
                        
                        st.success(f"âœ… æˆåŠŸè®¡ç®— {len(rfm_df):,} ä¸ªç”¨æˆ·çš„RFMæŒ‡æ ‡")
                    
                    # RFMæ¦‚è§ˆ
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å¹³å‡æœ€è¿‘æ€§", f"{rfm_df['recency'].mean():.1f} å¤©")
                    with col2:
                        st.metric("å¹³å‡é¢‘ç‡", f"{rfm_df['frequency'].mean():.1f} æ¬¡")
                    with col3:
                        st.metric("å¹³å‡è´­ä¹°æ¬¡æ•°", f"{rfm_df['monetary'].mean():.1f} æ¬¡")
                    
                    # æ•°æ®ç±»å‹ç¡®ä¿
                    rfm_df['recency'] = pd.to_numeric(rfm_df['recency'], errors='coerce').fillna(999).astype(int)
                    rfm_df['frequency'] = pd.to_numeric(rfm_df['frequency'], errors='coerce').fillna(0).astype(int)
                    rfm_df['monetary'] = pd.to_numeric(rfm_df['monetary'], errors='coerce').fillna(0).astype(int)
                    
                    # RFMåˆ†ç¾¤åˆ†å¸ƒ
                    st.subheader("RFMç”¨æˆ·åˆ†ç¾¤åˆ†å¸ƒ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # åˆ†ç¾¤åˆ†å¸ƒé¥¼å›¾
                        segment_counts = rfm_df['segment'].value_counts()
                        fig = px.pie(
                            values=segment_counts.values,
                            names=segment_counts.index,
                            title="RFMç”¨æˆ·åˆ†ç¾¤åˆ†å¸ƒ",
                            color_discrete_sequence=px.colors.qualitative.Set3
                        )
                        fig.update_traces(textposition='inside', textinfo='percent+label')
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        # åˆ†ç¾¤æ•°é‡æŸ±çŠ¶å›¾
                        fig = px.bar(
                            x=segment_counts.index,
                            y=segment_counts.values,
                            title="å„åˆ†ç¾¤ç”¨æˆ·æ•°é‡",
                            labels={'x': 'ç”¨æˆ·åˆ†ç¾¤', 'y': 'ç”¨æˆ·æ•°é‡'},
                            color=segment_counts.values,
                            color_continuous_scale='viridis'
                        )
                        fig.update_layout(showlegend=False)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # RFM 3Dæ•£ç‚¹å›¾
                    st.subheader("RFM 3Dæ•£ç‚¹å›¾")
                    
                    try:
                        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                        plot_data = rfm_df.copy()
                        plot_data = plot_data.dropna(subset=['recency', 'frequency', 'monetary'])
                        
                        if len(plot_data) == 0:
                            st.error("æ²¡æœ‰æœ‰æ•ˆçš„RFMæ•°æ®ç”¨äºç»˜åˆ¶3Då›¾")
                            return
                        
                        # åˆ›å»º3Dæ•£ç‚¹å›¾
                        fig = px.scatter_3d(
                            plot_data,
                            x='recency',
                            y='frequency',
                            z='monetary',
                            color='segment',
                            title="RFMä¸‰ç»´åˆ†å¸ƒ",
                            labels={
                                'recency': 'æœ€è¿‘æ€§ (å¤©)',
                                'frequency': 'é¢‘ç‡ (æ¬¡)',
                                'monetary': 'è´­ä¹°æ¬¡æ•°'
                            },
                            hover_data=['user_id', 'pv_count', 'cart_count', 'fav_count'],
                            color_discrete_sequence=px.colors.qualitative.Set3
                        )
                        
                        # ä¼˜åŒ–3Då›¾çš„æ˜¾ç¤º
                        fig.update_traces(
                            marker=dict(size=5, opacity=0.7),
                            selector=dict(mode='markers')
                        )
                        
                        fig.update_layout(
                            scene=dict(
                                xaxis_title="æœ€è¿‘æ€§ (å¤©) - è¶Šå°è¶Šå¥½",
                                yaxis_title="é¢‘ç‡ (æ¬¡) - è¶Šå¤§è¶Šå¥½", 
                                zaxis_title="è´­ä¹°æ¬¡æ•° - è¶Šå¤§è¶Šå¥½",
                                camera=dict(
                                    eye=dict(x=1.5, y=1.5, z=1.5)
                                )
                            ),
                            width=800,
                            height=600
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                    except Exception as plot_error:
                        st.error(f"3Dæ•£ç‚¹å›¾ç»˜åˆ¶å¤±è´¥: {str(plot_error)}")
                        st.info("å°è¯•æ˜¾ç¤ºç®€åŒ–çš„2Då›¾è¡¨")
                        
                        # å¤‡ç”¨2Då›¾è¡¨
                        col1, col2 = st.columns(2)
                        with col1:
                            fig = px.scatter(rfm_df, x='recency', y='frequency', color='segment',
                                           title="æœ€è¿‘æ€§ vs é¢‘ç‡")
                            st.plotly_chart(fig, use_container_width=True)
                        with col2:
                            fig = px.scatter(rfm_df, x='frequency', y='monetary', color='segment',
                                           title="é¢‘ç‡ vs è´­ä¹°æ¬¡æ•°")
                            st.plotly_chart(fig, use_container_width=True)
                    
                    # RFMåˆ†ç¾¤ç‰¹å¾å¯¹æ¯”
                    st.subheader("RFMåˆ†ç¾¤ç‰¹å¾å¯¹æ¯”")
                    
                    try:
                        segment_summary = rfm_df.groupby('segment').agg({
                            'recency': 'mean',
                            'frequency': 'mean',
                            'monetary': 'mean',
                            'pv_count': 'mean',
                            'cart_count': 'mean',
                            'fav_count': 'mean'
                        }).round(2)
                        
                        segment_summary.columns = ['å¹³å‡æœ€è¿‘æ€§(å¤©)', 'å¹³å‡é¢‘ç‡', 'å¹³å‡è´­ä¹°æ¬¡æ•°', 'å¹³å‡æµè§ˆæ¬¡æ•°', 'å¹³å‡åŠ è´­æ¬¡æ•°', 'å¹³å‡æ”¶è—æ¬¡æ•°']
                        st.dataframe(segment_summary, use_container_width=True)
                        
                    except Exception as summary_error:
                        st.error(f"åˆ†ç¾¤ç‰¹å¾å¯¹æ¯”è®¡ç®—å¤±è´¥: {str(summary_error)}")
                        st.write("æ˜¾ç¤ºåŸå§‹æ•°æ®é¢„è§ˆ:")
                        # ä¿®å¤æ•°æ®æ˜¾ç¤ºé—®é¢˜
                        preview_df = rfm_df.head().copy()
                        for col in preview_df.columns:
                            if preview_df[col].dtype == 'object':
                                try:
                                    preview_df[col] = preview_df[col].astype(str)
                                except:
                                    pass
                        st.dataframe(preview_df, use_container_width=True)
                    
                    # åˆ†ç¾¤è¯¦æƒ…
                    st.subheader("åˆ†ç¾¤è¯¦æƒ…åˆ†æ")
                    
                    try:
                        selected_segment = st.selectbox(
                            "é€‰æ‹©è¦æŸ¥çœ‹çš„ç”¨æˆ·åˆ†ç¾¤",
                            options=rfm_df['segment'].unique(),
                            key="rfm_segment_select"
                        )
                        
                        segment_users = rfm_df[rfm_df['segment'] == selected_segment]
                        
                        if len(segment_users) == 0:
                            st.warning(f"åˆ†ç¾¤ '{selected_segment}' ä¸­æ²¡æœ‰ç”¨æˆ·")
                            return
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**{selected_segment}** åŒ…å« {len(segment_users):,} ä¸ªç”¨æˆ·")
                            st.write(f"å æ€»ç”¨æˆ·çš„ {(len(segment_users)/len(rfm_df)*100):.1f}%")
                            
                            # è¯¥åˆ†ç¾¤çš„ç»Ÿè®¡ä¿¡æ¯
                            st.write("**åˆ†ç¾¤ç‰¹å¾:**")
                            st.write(f"- å¹³å‡æœ€è¿‘æ€§: {segment_users['recency'].mean():.1f} å¤©")
                            st.write(f"- å¹³å‡é¢‘ç‡: {segment_users['frequency'].mean():.1f} æ¬¡")
                            st.write(f"- å¹³å‡è´­ä¹°: {segment_users['monetary'].mean():.1f} æ¬¡")
                        
                        with col2:
                            try:
                                # è¯¥åˆ†ç¾¤çš„RFMåˆ†å¸ƒ
                                fig, axes = plt.subplots(1, 3, figsize=(15, 4))
                                
                                # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
                                if len(segment_users) > 0:
                                    axes[0].hist(segment_users['recency'], bins=min(20, len(segment_users)), 
                                                alpha=0.7, color='skyblue', edgecolor='black')
                                    axes[0].set_title('æœ€è¿‘æ€§åˆ†å¸ƒ')
                                    axes[0].set_xlabel('å¤©æ•°')
                                    axes[0].set_ylabel('ç”¨æˆ·æ•°')
                                    
                                    axes[1].hist(segment_users['frequency'], bins=min(20, len(segment_users)), 
                                                alpha=0.7, color='lightgreen', edgecolor='black')
                                    axes[1].set_title('é¢‘ç‡åˆ†å¸ƒ')
                                    axes[1].set_xlabel('äº¤äº’æ¬¡æ•°')
                                    axes[1].set_ylabel('ç”¨æˆ·æ•°')
                                    
                                    axes[2].hist(segment_users['monetary'], bins=min(20, len(segment_users)), 
                                                alpha=0.7, color='lightcoral', edgecolor='black')
                                    axes[2].set_title('è´­ä¹°æ¬¡æ•°åˆ†å¸ƒ')
                                    axes[2].set_xlabel('è´­ä¹°æ¬¡æ•°')
                                    axes[2].set_ylabel('ç”¨æˆ·æ•°')
                                    
                                    plt.tight_layout()
                                    st.pyplot(fig)
                                else:
                                    st.info("è¯¥åˆ†ç¾¤æ²¡æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œåˆ†å¸ƒå›¾ç»˜åˆ¶")
                                    
                            except Exception as hist_error:
                                st.error(f"åˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(hist_error)}")
                        
                        # æ˜¾ç¤ºè¯¥åˆ†ç¾¤çš„ç”¨æˆ·æ ·æœ¬
                        st.write("**ç”¨æˆ·æ ·æœ¬æ•°æ®:**")
                        display_columns = ['user_id', 'recency', 'frequency', 'monetary', 'pv_count', 'cart_count', 'fav_count']
                        available_columns = [col for col in display_columns if col in segment_users.columns]
                        st.dataframe(segment_users[available_columns].head(20), use_container_width=True)
                        
                    except Exception as detail_error:
                        st.error(f"åˆ†ç¾¤è¯¦æƒ…åˆ†æå¤±è´¥: {str(detail_error)}")
                        st.write("æ˜¾ç¤ºåŸºæœ¬åˆ†ç¾¤ä¿¡æ¯:")
                        st.write(rfm_df['segment'].value_counts())
                    
                    # RFMè¥é”€å»ºè®®
                    st.subheader("ğŸ“ˆ RFMè¥é”€ç­–ç•¥å»ºè®®")
                    
                    strategy_recommendations = {
                        "å† å†›ç”¨æˆ·": {
                            "ç‰¹å¾": "æœ€è¿‘è´­ä¹°ã€è´­ä¹°é¢‘æ¬¡é«˜ã€æ¶ˆè´¹é‡‘é¢é«˜",
                            "ç­–ç•¥": "VIPä¸“å±æœåŠ¡ã€æ–°å“é¢„è§ˆã€å¿ è¯šåº¦å¥–åŠ±ã€ä¸ªæ€§åŒ–æ¨è",
                            "é‡ç‚¹": "ç»´æŠ¤å…³ç³»ï¼Œæå‡å®¢å•ä»·"
                        },
                        "å¿ è¯šç”¨æˆ·": {
                            "ç‰¹å¾": "è´­ä¹°é¢‘æ¬¡è¾ƒé«˜ï¼Œä½†æœ€è¿‘æ€§ä¸€èˆ¬",
                            "ç­–ç•¥": "ä¼šå‘˜æƒç›Šã€å®šæœŸä¼˜æƒ ã€ç”Ÿæ—¥ç‰¹æƒã€ç¤¾ç¾¤å»ºè®¾",
                            "é‡ç‚¹": "å¢åŠ äº’åŠ¨é¢‘æ¬¡ï¼Œé˜²æ­¢æµå¤±"
                        },
                        "æ–°ç”¨æˆ·": {
                            "ç‰¹å¾": "æœ€è¿‘æœ‰äº¤äº’ï¼Œä½†é¢‘æ¬¡å’Œæ¶ˆè´¹è¾ƒä½",
                            "ç­–ç•¥": "æ–°ç”¨æˆ·å¼•å¯¼ã€é¦–è´­ä¼˜æƒ ã€æ•™è‚²å†…å®¹ã€ç®€åŒ–æµç¨‹",
                            "é‡ç‚¹": "å¿«é€Ÿè½¬åŒ–ï¼Œå»ºç«‹ä¹ æƒ¯"
                        },
                        "æµå¤±é£é™©ç”¨æˆ·": {
                            "ç‰¹å¾": "æ›¾ç»æ´»è·ƒï¼Œä½†æœ€è¿‘äº¤äº’å‡å°‘",
                            "ç­–ç•¥": "å¬å›æ´»åŠ¨ã€é™æ—¶ä¼˜æƒ ã€é—®å·è°ƒç ”ã€é‡æ–°æ¿€æ´»",
                            "é‡ç‚¹": "åŠæ—¶æŒ½å›ï¼Œæ‰¾å‡ºæµå¤±åŸå› "
                        },
                        "æ½œåœ¨ç”¨æˆ·": {
                            "ç‰¹å¾": "æœ‰ä¸€å®šæ´»è·ƒåº¦ä½†ä»æœªè´­ä¹°",
                            "ç­–ç•¥": "è´­ä¹°å¼•å¯¼ã€è¯•ç”¨æ´»åŠ¨ã€ä¿¡ä»»å»ºè®¾ã€é™ä½é—¨æ§›",
                            "é‡ç‚¹": "è½¬åŒ–ä¸ºä»˜è´¹ç”¨æˆ·"
                        },
                        "ä¸€èˆ¬ç”¨æˆ·": {
                            "ç‰¹å¾": "å„é¡¹æŒ‡æ ‡éƒ½ä¸­ç­‰",
                            "ç­–ç•¥": "åˆ†å±‚è¥é”€ã€å…´è¶£æ¢ç´¢ã€ä¸ªæ€§åŒ–å†…å®¹ã€é€æ­¥åŸ¹å…»",
                            "é‡ç‚¹": "æå‡æ´»è·ƒåº¦å’Œä»·å€¼"
                        }
                    }
                    
                    for segment, info in strategy_recommendations.items():
                        if segment in rfm_df['segment'].unique():
                            with st.expander(f"ğŸ¯ {segment} è¥é”€ç­–ç•¥"):
                                st.write(f"**ç”¨æˆ·ç‰¹å¾:** {info['ç‰¹å¾']}")
                                st.write(f"**è¥é”€ç­–ç•¥:** {info['ç­–ç•¥']}")
                                st.write(f"**é‡ç‚¹å…³æ³¨:** {info['é‡ç‚¹']}")
                                
                                # æ˜¾ç¤ºè¯¥åˆ†ç¾¤çš„ç”¨æˆ·æ•°å’Œå æ¯”
                                segment_count = len(rfm_df[rfm_df['segment'] == segment])
                                segment_percent = (segment_count / len(rfm_df)) * 100
                                st.write(f"**åˆ†ç¾¤è§„æ¨¡:** {segment_count:,} ç”¨æˆ· ({segment_percent:.1f}%)")
            except Exception as e:
                st.error(f"ç”¨æˆ·ç”»åƒåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                st.info("è¿™å¯èƒ½æ˜¯ç”±äºæ•°æ®é‡è¿‡å¤§æˆ–æ ¼å¼é—®é¢˜å¯¼è‡´çš„ã€‚å»ºè®®å°è¯•ä½¿ç”¨è¾ƒå°çš„æ•°æ®æ ·æœ¬ã€‚")
    
    def render_algorithm_comparison(self):
        """æ¸²æŸ“æ¨èç®—æ³•æ¯”è¾ƒé¡µé¢"""
        st.title("ğŸ”¬ æ¨èç®—æ³•æ¯”è¾ƒ")
        
        if self.data is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return
        
        # æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        st.subheader("ğŸ¯ æ¨¡å‹è®­ç»ƒ")
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            if not st.session_state.models_trained:
                if st.button("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹", type="primary"):
                    self.train_models()
            else:
                st.success("âœ… æ¨¡å‹å·²è®­ç»ƒå®Œæˆ")
                if st.button("ğŸ”„ é‡æ–°è®­ç»ƒæ¨¡å‹", type="secondary"):
                    # é‡ç½®æ¨¡å‹çŠ¶æ€
                    st.session_state.models_trained = False
                    st.session_state.trained_ncf_recommender = None
                    st.session_state.trained_lstm_recommender = None
                    
                    st.warning("âš ï¸ æ¨¡å‹çŠ¶æ€å·²é‡ç½®ï¼Œè¯·é‡æ–°è®­ç»ƒ")
                    st.experimental_rerun()
        
        with col2:
            if st.session_state.models_trained:
                st.info("""
                **å·²è®­ç»ƒçš„æ¨¡å‹:**
                - âœ… NCFæ·±åº¦å­¦ä¹ æ¨èå™¨ (ç¥ç»ååŒè¿‡æ»¤)
                - âœ… LSTMåºåˆ—é¢„æµ‹å™¨ (åŸºäºç”¨æˆ·è¡Œä¸ºåºåˆ—)
                """)
            else:
                st.info("""
                **å¾…è®­ç»ƒçš„æ¨¡å‹:**
                - â³ NCFæ·±åº¦å­¦ä¹ æ¨èå™¨  
                - â³ LSTMåºåˆ—é¢„æµ‹å™¨
                
                ç‚¹å‡»"å¼€å§‹è®­ç»ƒæ¨¡å‹"æŒ‰é’®å¼€å§‹è®­ç»ƒ
                """)
        
        # å¦‚æœæ¨¡å‹å·²è®­ç»ƒï¼Œæ˜¾ç¤ºæ€§èƒ½æ¯”è¾ƒ
        if st.session_state.models_trained:
            self.render_model_performance_comparison()
        else:
            st.info("è¯·å…ˆè®­ç»ƒæ¨¡å‹ä»¥æŸ¥çœ‹æ€§èƒ½æ¯”è¾ƒç»“æœ")
    
    def render_model_performance_comparison(self):
        """æ¸²æŸ“æ¨¡å‹æ€§èƒ½æ¯”è¾ƒéƒ¨åˆ†"""
        st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
        
        # åˆ›å»ºé€‰é¡¹å¡
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹", "ğŸ¯ é›·è¾¾å›¾æ¯”è¾ƒ", "ğŸ“‹ è¯¦ç»†ç»Ÿè®¡"])
        
        with tab1:
            st.subheader("è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–")
            
            # è·å–è®­ç»ƒå†å²æ•°æ®
            ncf_history = None
            lstm_history = None
            
            if (st.session_state.trained_ncf_recommender and 
                hasattr(st.session_state.trained_ncf_recommender, 'training_history')):
                ncf_history = st.session_state.trained_ncf_recommender.training_history
            
            if (st.session_state.trained_lstm_recommender and 
                hasattr(st.session_state.trained_lstm_recommender, 'training_history')):
                lstm_history = st.session_state.trained_lstm_recommender.training_history
            
            if ncf_history or lstm_history:
                # æŸå¤±æ›²çº¿
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**ğŸ“‰ è®­ç»ƒæŸå¤±æ›²çº¿**")
                    fig_loss = go.Figure()
                    
                    if ncf_history and ncf_history['epochs']:
                        fig_loss.add_trace(go.Scatter(
                            x=ncf_history['epochs'],
                            y=ncf_history['losses'],
                            mode='lines+markers',
                            name='NCFæ·±åº¦å­¦ä¹ ',
                            line=dict(color='#FF6B6B', width=3),
                            marker=dict(size=8)
                        ))
                    
                    if lstm_history and lstm_history['epochs']:
                        fig_loss.add_trace(go.Scatter(
                            x=lstm_history['epochs'],
                            y=lstm_history['losses'],
                            mode='lines+markers',
                            name='LSTMåºåˆ—é¢„æµ‹',
                            line=dict(color='#4ECDC4', width=3),
                            marker=dict(size=8)
                        ))
                    
                    fig_loss.update_layout(
                        title="è®­ç»ƒæŸå¤±å˜åŒ–",
                        xaxis_title="Epoch",
                        yaxis_title="æŸå¤±å€¼",
                        hovermode='x unified',
                        template='plotly_white'
                    )
                    
                    st.plotly_chart(fig_loss, use_container_width=True)
                
                with col2:
                    st.write("**ğŸ“ˆ è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿**")
                    fig_acc = go.Figure()
                    
                    if ncf_history and ncf_history['epochs']:
                        fig_acc.add_trace(go.Scatter(
                            x=ncf_history['epochs'],
                            y=ncf_history['accuracies'],
                            mode='lines+markers',
                            name='NCFæ·±åº¦å­¦ä¹ ',
                            line=dict(color='#FF6B6B', width=3),
                            marker=dict(size=8)
                        ))
                    
                    if lstm_history and lstm_history['epochs']:
                        fig_acc.add_trace(go.Scatter(
                            x=lstm_history['epochs'],
                            y=lstm_history['accuracies'],
                            mode='lines+markers',
                            name='LSTMåºåˆ—é¢„æµ‹',
                            line=dict(color='#4ECDC4', width=3),
                            marker=dict(size=8)
                        ))
                    
                    fig_acc.update_layout(
                        title="è®­ç»ƒå‡†ç¡®ç‡å˜åŒ–",
                        xaxis_title="Epoch",
                        yaxis_title="å‡†ç¡®ç‡ (%)",
                        hovermode='x unified',
                        template='plotly_white'
                    )
                    
                    st.plotly_chart(fig_acc, use_container_width=True)
                
                # è®­ç»ƒè¿›å±•åˆ†æ
                st.subheader("ğŸ“Š è®­ç»ƒè¿›å±•åˆ†æ")
                col1, col2 = st.columns(2)
                
                with col1:
                    if ncf_history and len(ncf_history['losses']) > 1:
                        ncf_loss_improvement = ncf_history['losses'][0] - ncf_history['losses'][-1]
                        ncf_acc_improvement = ncf_history['accuracies'][-1] - ncf_history['accuracies'][0]
                        
                        st.metric(
                            "NCFæŸå¤±æ”¹å–„",
                            f"{ncf_loss_improvement:.4f}",
                            delta=f"{(ncf_loss_improvement/ncf_history['losses'][0]*100):.1f}%"
                        )
                        st.metric(
                            "NCFå‡†ç¡®ç‡æå‡",
                            f"{ncf_acc_improvement:.2f}%",
                            delta=f"æœ€ç»ˆ: {ncf_history['accuracies'][-1]:.2f}%"
                        )
                
                with col2:
                    if lstm_history and len(lstm_history['losses']) > 1:
                        lstm_loss_improvement = lstm_history['losses'][0] - lstm_history['losses'][-1]
                        lstm_acc_improvement = lstm_history['accuracies'][-1] - lstm_history['accuracies'][0]
                        
                        st.metric(
                            "LSTMæŸå¤±æ”¹å–„",
                            f"{lstm_loss_improvement:.4f}",
                            delta=f"{(lstm_loss_improvement/lstm_history['losses'][0]*100):.1f}%"
                        )
                        st.metric(
                            "LSTMå‡†ç¡®ç‡æå‡",
                            f"{lstm_acc_improvement:.2f}%",
                            delta=f"æœ€ç»ˆ: {lstm_history['accuracies'][-1]:.2f}%"
                        )
            else:
                st.warning("âš ï¸ è®­ç»ƒå†å²æ•°æ®ä¸å¯ç”¨")
        
        with tab2:
            st.subheader("æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾æ¯”è¾ƒ")
            
            # è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
            ncf_stats = None
            lstm_stats = None
            
            if (st.session_state.trained_ncf_recommender and 
                hasattr(st.session_state.trained_ncf_recommender, 'model_stats')):
                ncf_stats = st.session_state.trained_ncf_recommender.model_stats
            
            if (st.session_state.trained_lstm_recommender and 
                hasattr(st.session_state.trained_lstm_recommender, 'model_stats')):
                lstm_stats = st.session_state.trained_lstm_recommender.model_stats
            
            if ncf_stats and lstm_stats:
                # æ”¶é›†å®é™…æ•°å€¼ç”¨äºæ˜¾ç¤º
                raw_metrics = {}
                radar_values = {}
                
                # 1. æ¨¡å‹ç®€æ´æ€§ (å‚æ•°æ•°é‡çš„å€’æ•°ï¼Œè¶Šç®€æ´è¶Šå¥½)
                ncf_params = ncf_stats.get('total_params', 0)
                lstm_params = lstm_stats.get('total_params', 0)
                if ncf_params > 0 and lstm_params > 0:
                    # è®¡ç®—ç®€æ´æ€§åˆ†æ•° (è¶Šå°çš„å‚æ•°æ•°é‡å¾—åˆ†è¶Šé«˜)
                    max_params = max(ncf_params, lstm_params)
                    ncf_simplicity = (max_params - ncf_params) / max_params * 100 + 10  # æœ€å°‘ç»™10åˆ†
                    lstm_simplicity = (max_params - lstm_params) / max_params * 100 + 10
                    
                    raw_metrics['æ¨¡å‹ç®€æ´æ€§'] = {
                        'NCF': f"{ncf_params:,} å‚æ•°",
                        'LSTM': f"{lstm_params:,} å‚æ•°"
                    }
                    radar_values['æ¨¡å‹ç®€æ´æ€§'] = {
                        'NCF': ncf_simplicity,
                        'LSTM': lstm_simplicity
                    }
                
                # 2. è®­ç»ƒæ•°æ®è§„æ¨¡ (æ ·æœ¬æ•°é‡)
                ncf_samples = ncf_stats.get('training_samples', 0)
                lstm_samples = lstm_stats.get('num_sequences', 0)
                if ncf_samples > 0 and lstm_samples > 0:
                    # ä¸ç›´æ¥æ‹‰æ»¡ï¼Œç”¨å¯¹æ•°ç¼©æ”¾
                    import math
                    ncf_data_score = min(90, math.log10(ncf_samples) * 15)  # æœ€é«˜90åˆ†
                    lstm_data_score = min(90, math.log10(lstm_samples) * 15)
                    
                    raw_metrics['è®­ç»ƒæ•°æ®è§„æ¨¡'] = {
                        'NCF': f"{ncf_samples:,} æ ·æœ¬",
                        'LSTM': f"{lstm_samples:,} æ ·æœ¬"
                    }
                    radar_values['è®­ç»ƒæ•°æ®è§„æ¨¡'] = {
                        'NCF': ncf_data_score,
                        'LSTM': lstm_data_score
                    }
                
                # 3. æœ€ç»ˆå‡†ç¡®ç‡
                if ncf_history and lstm_history:
                    if ncf_history.get('accuracies') and lstm_history.get('accuracies'):
                        ncf_final_acc = ncf_history['accuracies'][-1] if ncf_history['accuracies'] else 0
                        lstm_final_acc = lstm_history['accuracies'][-1] if lstm_history['accuracies'] else 0
                        
                        raw_metrics['è®­ç»ƒå‡†ç¡®ç‡'] = {
                            'NCF': f"{ncf_final_acc:.1f}%",
                            'LSTM': f"{lstm_final_acc:.1f}%"
                        }
                        radar_values['è®­ç»ƒå‡†ç¡®ç‡'] = {
                            'NCF': ncf_final_acc,  # ç›´æ¥ä½¿ç”¨å‡†ç¡®ç‡ç™¾åˆ†æ¯”
                            'LSTM': lstm_final_acc
                        }
                
                # 4. æ”¶æ•›é€Ÿåº¦ (æŸå¤±ä¸‹é™ç¨‹åº¦ç™¾åˆ†æ¯”)
                if ncf_history and lstm_history:
                    if ncf_history.get('losses') and lstm_history.get('losses'):
                        ncf_convergence = 0
                        lstm_convergence = 0
                        
                        if len(ncf_history['losses']) > 1 and ncf_history['losses'][0] > 0:
                            ncf_convergence = (ncf_history['losses'][0] - ncf_history['losses'][-1]) / ncf_history['losses'][0] * 100
                        
                        if len(lstm_history['losses']) > 1 and lstm_history['losses'][0] > 0:
                            lstm_convergence = (lstm_history['losses'][0] - lstm_history['losses'][-1]) / lstm_history['losses'][0] * 100
                        
                        raw_metrics['æ”¶æ•›æ•ˆæœ'] = {
                            'NCF': f"{ncf_convergence:.1f}% æŸå¤±ä¸‹é™",
                            'LSTM': f"{lstm_convergence:.1f}% æŸå¤±ä¸‹é™"
                        }
                        radar_values['æ”¶æ•›æ•ˆæœ'] = {
                            'NCF': min(90, ncf_convergence),  # æœ€é«˜90åˆ†
                            'LSTM': min(90, lstm_convergence)
                        }
                
                # 5. ç”¨æˆ·è¦†ç›–ç‡
                ncf_users = ncf_stats.get('num_users', 0)
                lstm_users = lstm_stats.get('valid_users', 0)
                total_users = len(self.data['user_id'].unique()) if self.data is not None else max(ncf_users, lstm_users)
                
                if ncf_users > 0 and lstm_users > 0 and total_users > 0:
                    ncf_coverage = (ncf_users / total_users) * 100
                    lstm_coverage = (lstm_users / total_users) * 100
                    
                    raw_metrics['ç”¨æˆ·è¦†ç›–ç‡'] = {
                        'NCF': f"{ncf_coverage:.1f}% ({ncf_users:,}ç”¨æˆ·)",
                        'LSTM': f"{lstm_coverage:.1f}% ({lstm_users:,}ç”¨æˆ·)"
                    }
                    radar_values['ç”¨æˆ·è¦†ç›–ç‡'] = {
                        'NCF': ncf_coverage,
                        'LSTM': lstm_coverage
                    }
                
                # 6. æ•°æ®ç¨€ç–æ€§é€‚åº”åº¦ (ç¨€ç–åº¦è¶Šé«˜ï¼Œé€‚åº”æ€§è¦æ±‚è¶Šé«˜)
                ncf_sparsity = ncf_stats.get('sparsity', 0)
                if ncf_sparsity > 0:
                    # ç¨€ç–åº¦é€‚åº”æ€§ï¼šç¨€ç–æ•°æ®ä¸‹çš„è¡¨ç°èƒ½åŠ›
                    sparsity_challenge = ncf_sparsity * 1000000  # å°†ç¨€ç–åº¦æ”¾å¤§
                    ncf_sparsity_score = min(85, 20 + (1 - ncf_sparsity * 1000) * 65) if ncf_sparsity < 0.001 else 20
                    lstm_sparsity_score = min(75, 15 + (1 - ncf_sparsity * 1000) * 60) if ncf_sparsity < 0.001 else 15  # LSTMåœ¨æç¨€ç–æ•°æ®ä¸‹è¡¨ç°ç›¸å¯¹è¾ƒå·®
                    
                    raw_metrics['ç¨€ç–æ•°æ®é€‚åº”'] = {
                        'NCF': f"ç¨€ç–åº¦ {ncf_sparsity:.6f}",
                        'LSTM': f"ç¨€ç–åº¦ {ncf_sparsity:.6f}"
                    }
                    radar_values['ç¨€ç–æ•°æ®é€‚åº”'] = {
                        'NCF': ncf_sparsity_score,
                        'LSTM': lstm_sparsity_score
                    }
                
                # åˆ›å»ºé›·è¾¾å›¾
                if radar_values:
                    categories = list(radar_values.keys())
                    ncf_values = [radar_values[cat]['NCF'] for cat in categories]
                    lstm_values = [radar_values[cat]['LSTM'] for cat in categories]
                    
                    # æ˜¾ç¤ºå…·ä½“æ•°å€¼è¡¨æ ¼
                    st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½å…·ä½“æŒ‡æ ‡")
                    
                    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
                    comparison_data = []
                    for metric in categories:
                        if metric in raw_metrics:
                            comparison_data.append({
                                'æ€§èƒ½æŒ‡æ ‡': metric,
                                'NCFæ·±åº¦å­¦ä¹ ': raw_metrics[metric]['NCF'],
                                'LSTMåºåˆ—é¢„æµ‹': raw_metrics[metric]['LSTM'],
                                'NCFå¾—åˆ†': f"{radar_values[metric]['NCF']:.1f}",
                                'LSTMå¾—åˆ†': f"{radar_values[metric]['LSTM']:.1f}"
                            })
                    
                    if comparison_data:
                        comparison_df = pd.DataFrame(comparison_data)
                        st.dataframe(comparison_df, use_container_width=True)
                    
                    # ç»˜åˆ¶é›·è¾¾å›¾
                    fig_radar = go.Figure()
                    
                    fig_radar.add_trace(go.Scatterpolar(
                        r=ncf_values + [ncf_values[0]],  # é—­åˆå›¾å½¢
                        theta=categories + [categories[0]],
                        fill='toself',
                        name='NCFæ·±åº¦å­¦ä¹ ',
                        line_color='#FF6B6B',
                        fillcolor='rgba(255, 107, 107, 0.3)',
                        hovertemplate='<b>NCFæ·±åº¦å­¦ä¹ </b><br>' +
                                    '%{theta}: %{r:.1f}åˆ†<br>' +
                                    '<extra></extra>'
                    ))
                    
                    fig_radar.add_trace(go.Scatterpolar(
                        r=lstm_values + [lstm_values[0]],  # é—­åˆå›¾å½¢
                        theta=categories + [categories[0]],
                        fill='toself',
                        name='LSTMåºåˆ—é¢„æµ‹',
                        line_color='#4ECDC4',
                        fillcolor='rgba(78, 205, 196, 0.3)',
                        hovertemplate='<b>LSTMåºåˆ—é¢„æµ‹</b><br>' +
                                    '%{theta}: %{r:.1f}åˆ†<br>' +
                                    '<extra></extra>'
                    ))
                    
                    fig_radar.update_layout(
                        polar=dict(
                            radialaxis=dict(
                                visible=True,
                                range=[0, 100],
                                tickvals=[20, 40, 60, 80, 100],
                                ticktext=['20', '40', '60', '80', '100'],
                                gridcolor='lightgray'
                            ),
                            angularaxis=dict(
                                gridcolor='lightgray'
                            )
                        ),
                        showlegend=True,
                        title="æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾æ¯”è¾ƒ (è¯„åˆ†èŒƒå›´: 0-100)",
                        height=600,
                        font=dict(size=12)
                    )
                    
                    st.plotly_chart(fig_radar, use_container_width=True)
                    
                    # é›·è¾¾å›¾è¯´æ˜
                    st.info("""
                    **é›·è¾¾å›¾è¯„åˆ†è¯´æ˜:**
                    - **æ¨¡å‹ç®€æ´æ€§**: å‚æ•°è¶Šå°‘å¾—åˆ†è¶Šé«˜ (è½»é‡åŒ–ç¨‹åº¦)
                    - **è®­ç»ƒæ•°æ®è§„æ¨¡**: è®­ç»ƒæ ·æœ¬æ•°é‡ (å¯¹æ•°ç¼©æ”¾ï¼Œæœ€é«˜90åˆ†)
                    - **è®­ç»ƒå‡†ç¡®ç‡**: æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡ç™¾åˆ†æ¯” (ç›´æ¥ä½¿ç”¨%)
                    - **æ”¶æ•›æ•ˆæœ**: è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±ä¸‹é™ç™¾åˆ†æ¯” (æœ€é«˜90åˆ†)
                    - **ç”¨æˆ·è¦†ç›–ç‡**: å¯æ¨èç”¨æˆ·å æ€»ç”¨æˆ·çš„ç™¾åˆ†æ¯”
                    - **ç¨€ç–æ•°æ®é€‚åº”**: åœ¨ç¨€ç–æ•°æ®åœºæ™¯ä¸‹çš„è¡¨ç°èƒ½åŠ›
                    
                    **æ³¨**: å¾—åˆ†è¶Šé«˜è¡¨ç¤ºè¯¥æŒ‡æ ‡è¡¨ç°è¶Šå¥½ï¼Œæ»¡åˆ†100åˆ†
                    """)
                else:
                    st.warning("âš ï¸ æ— æ³•ç”Ÿæˆé›·è¾¾å›¾ï¼šç¼ºå°‘å¿…è¦çš„ç»Ÿè®¡æ•°æ®")
            else:
                st.warning("âš ï¸ é›·è¾¾å›¾ä¸å¯ç”¨ï¼šç¼ºå°‘æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯")
        
        with tab3:
            st.subheader("è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ğŸ§  NCFæ·±åº¦å­¦ä¹ æ¨¡å‹**")
                if st.session_state.trained_ncf_recommender:
                    ncf_model = st.session_state.trained_ncf_recommender
                    
                    # åŸºæœ¬ä¿¡æ¯
                    if hasattr(ncf_model, 'model_stats'):
                        stats = ncf_model.model_stats
                        st.info(f"""
                        **åŸºæœ¬ä¿¡æ¯:**
                        - ç”¨æˆ·æ•°: {stats.get('num_users', 'N/A'):,}
                        - å•†å“æ•°: {stats.get('num_items', 'N/A'):,}
                        - è®­ç»ƒæ ·æœ¬æ•°: {stats.get('training_samples', 'N/A'):,}
                        - æ•°æ®ç¨€ç–åº¦: {stats.get('sparsity', 'N/A'):.6f}
                        
                        **æ¨¡å‹å‚æ•°:**
                        - æ€»å‚æ•°æ•°: {stats.get('total_params', 'N/A'):,}
                        - å¯è®­ç»ƒå‚æ•°: {stats.get('trainable_params', 'N/A'):,}
                        """)
                    
                    # è®­ç»ƒç»“æœ
                    if hasattr(ncf_model, 'training_history') and ncf_model.training_history['epochs']:
                        history = ncf_model.training_history
                        st.success(f"""
                        **è®­ç»ƒç»“æœ:**
                        - è®­ç»ƒè½®æ•°: {len(history['epochs'])}
                        - åˆå§‹æŸå¤±: {history['losses'][0]:.6f}
                        - æœ€ç»ˆæŸå¤±: {history['losses'][-1]:.6f}
                        - æœ€ç»ˆå‡†ç¡®ç‡: {history['accuracies'][-1]:.2f}%
                        """)
                else:
                    st.warning("NCFæ¨¡å‹æœªè®­ç»ƒ")
            
            with col2:
                st.write("**ğŸ“ˆ LSTMåºåˆ—é¢„æµ‹æ¨¡å‹**")
                if st.session_state.trained_lstm_recommender:
                    lstm_model = st.session_state.trained_lstm_recommender
                    
                    # åŸºæœ¬ä¿¡æ¯
                    if hasattr(lstm_model, 'model_stats'):
                        stats = lstm_model.model_stats
                        st.info(f"""
                        **åŸºæœ¬ä¿¡æ¯:**
                        - æœ‰æ•ˆç”¨æˆ·æ•°: {stats.get('valid_users', 'N/A'):,}
                        - å•†å“è¯æ±‡é‡: {stats.get('vocab_size_item', 'N/A'):,}
                        - ç±»åˆ«è¯æ±‡é‡: {stats.get('vocab_size_cat', 'N/A'):,}
                        - è®­ç»ƒåºåˆ—æ•°: {stats.get('num_sequences', 'N/A'):,}
                        
                        **æ¨¡å‹å‚æ•°:**
                        - æ€»å‚æ•°æ•°: {stats.get('total_params', 'N/A'):,}
                        - å¯è®­ç»ƒå‚æ•°: {stats.get('trainable_params', 'N/A'):,}
                        """)
                    
                    # è®­ç»ƒç»“æœ
                    if hasattr(lstm_model, 'training_history') and lstm_model.training_history['epochs']:
                        history = lstm_model.training_history
                        st.success(f"""
                        **è®­ç»ƒç»“æœ:**
                        - è®­ç»ƒè½®æ•°: {len(history['epochs'])}
                        - åˆå§‹æŸå¤±: {history['losses'][0]:.6f}
                        - æœ€ç»ˆæŸå¤±: {history['losses'][-1]:.6f}
                        - æœ€ç»ˆå‡†ç¡®ç‡: {history['accuracies'][-1]:.2f}%
                        """)
                else:
                    st.warning("LSTMæ¨¡å‹æœªè®­ç»ƒ")
    
    def render_personalized_recommendation(self):
        """æ¸²æŸ“ä¸ªæ€§åŒ–æ¨èé¡µé¢"""
        st.title("ğŸ¯ ä¸ªæ€§åŒ–æ¨è")
        
        if self.data is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return
        
        # æ£€æŸ¥æ¨¡å‹è®­ç»ƒçŠ¶æ€
        if not st.session_state.models_trained:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ 'æ¨èç®—æ³•æ¯”è¾ƒ' é¡µé¢è®­ç»ƒæ¨¡å‹")
            
            col1, col2 = st.columns(2)
            with col1:
                st.info("""
                **è®­ç»ƒæ­¥éª¤:**
                1. ç‚¹å‡»ä¾§è¾¹æ é€‰æ‹© 'æ¨èç®—æ³•æ¯”è¾ƒ'
                2. ç‚¹å‡» 'å¼€å§‹è®­ç»ƒæ¨¡å‹' æŒ‰é’®
                3. ç­‰å¾…è®­ç»ƒå®Œæˆ
                4. è¿”å›æ­¤é¡µé¢è¿›è¡Œæ¨è
                """)
            
            with col2:
                st.image("data:image/svg+xml,%3csvg width='100' height='100' xmlns='http://www.w3.org/2000/svg'%3e%3ctext x='50' y='50' font-size='50' text-anchor='middle' dy='.3em'%3eğŸ¤–%3c/text%3e%3c/svg%3e", width=100)
                st.write("**æ¨¡å‹è®­ç»ƒä¸­...**")
            
            return
        
        # ç”¨æˆ·é€‰æ‹©
        st.subheader("ğŸ‘¤ é€‰æ‹©ç”¨æˆ·")
        

                
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            # æ™ºèƒ½ç”¨æˆ·é€‰æ‹©ï¼šåªæ˜¾ç¤ºå¯ä»¥æ¨èçš„ç”¨æˆ·
            if st.session_state.models_trained:
                # è·å–æ‰€æœ‰æ¨¡å‹ä¸­å¯æ¨èçš„ç”¨æˆ·
                ncf_users = set()
                
                if (st.session_state.trained_ncf_recommender and 
                    hasattr(st.session_state.trained_ncf_recommender, 'user2idx')):
                    ncf_users = set(st.session_state.trained_ncf_recommender.user2idx.keys())
                
                # ä½¿ç”¨NCFç”¨æˆ·ä½œä¸ºå¯æ¨èç”¨æˆ·
                available_users = ncf_users
                
                if available_users:
                    # é™åˆ¶ç”¨æˆ·åˆ—è¡¨é•¿åº¦ï¼Œå¹¶æ’åº
                    user_list = sorted(list(available_users))[:200]
                    st.success(f"âœ… æ‰¾åˆ° {len(available_users):,} ä¸ªå¯æ¨èç”¨æˆ· (æ˜¾ç¤ºå‰200ä¸ª)")
                    
                    # ç”¨æˆ·é€‰æ‹©æ–¹å¼
                    selection_method = st.radio(
                        "é€‰æ‹©ç”¨æˆ·æ–¹å¼",
                        ["ä»åˆ—è¡¨é€‰æ‹©", "è¾“å…¥ç”¨æˆ·ID"]
                    )
                    
                    if selection_method == "ä»åˆ—è¡¨é€‰æ‹©":
                        selected_user = st.selectbox("é€‰æ‹©ç”¨æˆ·ID", user_list)
                    else:
                        input_user = st.text_input("è¾“å…¥ç”¨æˆ·ID", placeholder="è¯·è¾“å…¥ä¸€ä¸ªç”¨æˆ·ID")
                        if input_user:
                            try:
                                # å°è¯•è½¬æ¢ä¸ºæ•°å­—ï¼ˆå¦‚æœæ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼‰
                                input_user_parsed = int(input_user) if input_user.isdigit() else input_user
                                if input_user_parsed in available_users:
                                    selected_user = input_user_parsed
                                    st.success(f"âœ… ç”¨æˆ· {input_user_parsed} å¯ä»¥æ¨è")
                                else:
                                    st.error(f"âŒ ç”¨æˆ· {input_user_parsed} ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­")
                                    st.info("å¯æ¨èçš„ç”¨æˆ·ç¤ºä¾‹: " + ", ".join(map(str, user_list[:5])))
                                    selected_user = user_list[0] if user_list else None
                            except ValueError:
                                st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„ç”¨æˆ·ID")
                                selected_user = user_list[0] if user_list else None
                        else:
                            selected_user = user_list[0] if user_list else None
                            
                    # æ˜¾ç¤ºç”¨æˆ·åœ¨å„æ¨¡å‹ä¸­çš„çŠ¶æ€
                    if selected_user:
                        status_info = f"**ç”¨æˆ· {selected_user} çŠ¶æ€:**\n"
                        status_info += f"- NCFæ¨¡å‹: {'âœ… å¯æ¨è' if selected_user in ncf_users else 'âŒ ä¸å¯ç”¨'}"
                        st.info(status_info)
                else:
                    st.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯æ¨èçš„ç”¨æˆ·")
                    st.error("**å¯èƒ½çš„åŸå› :**")
                    st.error("- æ•°æ®ä¸­æ²¡æœ‰è´­ä¹°è¡Œä¸ºè®°å½•")
                    st.error("- æ¨¡å‹è®­ç»ƒå¤±è´¥æˆ–æ•°æ®ä¸è¶³")
                    st.error("- è®­ç»ƒæ•°æ®è¿‡æ»¤å¤ªä¸¥æ ¼")
                    
                    # æä¾›è°ƒè¯•ä¿¡æ¯
                    st.info("**è°ƒè¯•ä¿¡æ¯:**")
                    st.info(f"- NCFæ¨¡å‹ç”¨æˆ·æ•°: {len(ncf_users)}")
                    
                    selected_user = None
            else:
                # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œæ˜¾ç¤ºæœ‰è´­ä¹°è¡Œä¸ºçš„ç”¨æˆ·ä½œä¸ºé¢„è§ˆ
                users_with_purchases = self.data[self.data['behavior_type'] == 'buy']['user_id'].unique()
                
                if len(users_with_purchases) == 0:
                    st.error("âŒ æ•°æ®ä¸­æ²¡æœ‰è´­ä¹°è¡Œä¸ºè®°å½•")
                    st.info("æ¨èç³»ç»Ÿéœ€è¦è´­ä¹°è¡Œä¸ºæ•°æ®è¿›è¡Œè®­ç»ƒ")
                    selected_user = None
                else:
                    user_list = sorted(users_with_purchases[:100])
                    selected_user = st.selectbox("é€‰æ‹©ç”¨æˆ·ID (æ¨¡å‹æœªè®­ç»ƒ)", user_list)
                    st.warning("âš ï¸ æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆåœ¨'æ¨èç®—æ³•æ¯”è¾ƒ'é¡µé¢è®­ç»ƒæ¨¡å‹")
            
            if selected_user is not None:
                # æ¨èç®—æ³•é€‰æ‹©
                algorithm = st.selectbox(
                    "é€‰æ‹©æ¨èç®—æ³•",
                    ["NCFæ·±åº¦å­¦ä¹ ", "LSTMåºåˆ—é¢„æµ‹"]
                )
                
                recommendation_count = st.slider("æ¨èæ•°é‡", 5, 20, 10)
                
        
        with col2:
            # ç”¨æˆ·å†å²è¡Œä¸ºåˆ†æ
            st.write("**ç”¨æˆ·å†å²è¡Œä¸ºåˆ†æ**")
            user_history = self.data[self.data['user_id'] == selected_user].tail(20)
            
            if len(user_history) > 0:
                # è¡Œä¸ºç»Ÿè®¡
                behavior_summary = user_history['behavior_type'].value_counts()
                
                # åˆ›å»ºè¡Œä¸ºåˆ†æçš„å­å›¾
                fig = make_subplots(
                    rows=1, cols=2,
                    subplot_titles=('è¡Œä¸ºç±»å‹åˆ†å¸ƒ', 'è´­ä¹°å“ç±»åˆ†å¸ƒ'),
                    specs=[[{"type": "pie"}, {"type": "pie"}]]
                )
                
                # è¡Œä¸ºç±»å‹é¥¼å›¾
                fig.add_trace(
                    go.Pie(
                        labels=behavior_summary.index,
                        values=behavior_summary.values,
                        name="è¡Œä¸ºç±»å‹"
                    ),
                    row=1, col=1
                )
                
                # è´­ä¹°å“ç±»åˆ†å¸ƒ
                if 'category_id' in user_history.columns:
                    category_summary = user_history[user_history['behavior_type'] == 'buy']['category_id'].value_counts().head(5)
                    if len(category_summary) > 0:
                        fig.add_trace(
                            go.Pie(
                                labels=category_summary.index,
                                values=category_summary.values,
                                name="è´­ä¹°å“ç±»"
                            ),
                            row=1, col=2
                        )
                
                fig.update_layout(height=300, showlegend=True)
                st.plotly_chart(fig, use_container_width=True)
                
                # ç”¨æˆ·è¡Œä¸ºæ—¶é—´åºåˆ—
                if 'timestamp_dt' in user_history.columns or 'date' in user_history.columns:
                    time_col = 'timestamp_dt' if 'timestamp_dt' in user_history.columns else 'date'
                    
                    try:
                        # æŒ‰æ—¥æœŸç»Ÿè®¡è¡Œä¸ºæ•°é‡
                        if time_col == 'timestamp_dt':
                            # ç¡®ä¿timestamp_dtæ˜¯datetimeç±»å‹
                            if not pd.api.types.is_datetime64_any_dtype(user_history[time_col]):
                                user_history[time_col] = pd.to_datetime(user_history[time_col], errors='coerce')
                            
                            # å¦‚æœè½¬æ¢æˆåŠŸï¼Œåˆ›å»ºdateåˆ—
                            if pd.api.types.is_datetime64_any_dtype(user_history[time_col]):
                                user_history['date'] = user_history[time_col].dt.date
                                time_series = user_history.groupby('date').size()
                            else:
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åˆ—
                                time_series = user_history.groupby(time_col).size()
                        else:
                            # å¯¹äºdateåˆ—ï¼Œå°è¯•ç¡®ä¿æ˜¯æ­£ç¡®çš„æ ¼å¼
                            if not pd.api.types.is_datetime64_any_dtype(user_history[time_col]):
                                try:
                                    user_history[time_col] = pd.to_datetime(user_history[time_col], errors='coerce')
                                except:
                                    pass  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹æ•°æ®
                            time_series = user_history.groupby(time_col).size()
                        
                        # åªæœ‰å½“time_seriesä¸ä¸ºç©ºæ—¶æ‰ç»˜åˆ¶å›¾è¡¨
                        if len(time_series) > 0:
                            fig = px.line(
                                x=time_series.index,
                                y=time_series.values,
                                title="ç”¨æˆ·è¡Œä¸ºæ—¶é—´åºåˆ—",
                                labels={'x': 'æ—¥æœŸ', 'y': 'è¡Œä¸ºæ¬¡æ•°'}
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        else:
                            st.info("è¯¥ç”¨æˆ·çš„æ—¶é—´åºåˆ—æ•°æ®ä¸è¶³ä»¥ç»˜åˆ¶å›¾è¡¨")
                            
                    except Exception as e:
                        st.warning(f"æ—¶é—´åºåˆ—å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")
                        st.info("è·³è¿‡æ—¶é—´åºåˆ—åˆ†æï¼Œç»§ç»­æ˜¾ç¤ºå…¶ä»–ä¿¡æ¯")
            
            # ç”¨æˆ·ç‰¹å¾æ‘˜è¦
            st.subheader("ğŸ“Š ç”¨æˆ·ç‰¹å¾æ‘˜è¦")
            col_a, col_b, col_c = st.columns(3)
            
            with col_a:
                total_actions = len(user_history)
                st.metric("æ€»è¡Œä¸ºæ•°", total_actions)
            
            with col_b:
                unique_items = user_history['item_id'].nunique()
                st.metric("æµè§ˆå•†å“æ•°", unique_items)
            
            with col_c:
                purchase_count = len(user_history[user_history['behavior_type'] == 'buy'])
                st.metric("è´­ä¹°æ¬¡æ•°", purchase_count)
        
        # ç”Ÿæˆæ¨èç»“æœ
        st.subheader("ğŸ“‹ æ¨èç»“æœ")
        
        if st.button("ğŸ¯ ç”Ÿæˆä¸ªæ€§åŒ–æ¨è", type="primary"):
            with st.spinner(f"æ­£åœ¨ä½¿ç”¨{algorithm}ç®—æ³•ç”Ÿæˆä¸ªæ€§åŒ–æ¨è..."):
                
                try:
                    recommendations = []
                    debug_info = []
                    
                    st.info(f"å¼€å§‹ä¸ºç”¨æˆ· {selected_user} ç”Ÿæˆæ¨è...")
                    
                    if algorithm == "NCFæ·±åº¦å­¦ä¹ ":
                        if st.session_state.trained_ncf_recommender is not None:
                            st.info("ä½¿ç”¨è®­ç»ƒå¥½çš„NCFæ·±åº¦å­¦ä¹ æ¨¡å‹...")
                            ncf_results = st.session_state.trained_ncf_recommender.recommend(selected_user, recommendation_count)
                            debug_info.append(f"NCFæ¨¡å‹è¿”å›ç»“æœç±»å‹: {type(ncf_results)}")
                            debug_info.append(f"NCFæ¨¡å‹è¿”å›ç»“æœé•¿åº¦: {len(ncf_results)}")
                            
                            if len(ncf_results) > 0:
                                recommendations = ncf_results
                                debug_info.append(f"NCFæ¨èç»“æœ: {recommendations[:3]}...")  # æ˜¾ç¤ºå‰3ä¸ª
                            else:
                                debug_info.append("NCFæ¨¡å‹è¿”å›ç©ºç»“æœ")
                        else:
                            debug_info.append("NCFæ¨¡å‹æœªè®­ç»ƒæˆ–ä¸å¯ç”¨")
                    
                    elif algorithm == "LSTMåºåˆ—é¢„æµ‹":
                        if st.session_state.trained_lstm_recommender is not None:
                            st.info("ä½¿ç”¨è®­ç»ƒå¥½çš„LSTMåºåˆ—é¢„æµ‹æ¨¡å‹...")
                            lstm_results = st.session_state.trained_lstm_recommender.recommend(selected_user, self.data, recommendation_count)
                            debug_info.append(f"LSTMæ¨¡å‹è¿”å›ç»“æœç±»å‹: {type(lstm_results)}")
                            debug_info.append(f"LSTMæ¨¡å‹è¿”å›ç»“æœé•¿åº¦: {len(lstm_results)}")
                            
                            if len(lstm_results) > 0:
                                recommendations = lstm_results
                                debug_info.append(f"LSTMæ¨èç»“æœ: {lstm_results[:3]}...")  # æ˜¾ç¤ºå‰3ä¸ª
                            else:
                                debug_info.append("LSTMæ¨¡å‹è¿”å›ç©ºç»“æœ")
                        else:
                            debug_info.append("LSTMæ¨¡å‹æœªè®­ç»ƒæˆ–ä¸å¯ç”¨")
                    
                    elif algorithm == "æ··åˆæ¨è":
                        st.info("ä½¿ç”¨æ··åˆæ¨èæ¨¡å‹...")
                        # ç»“åˆä¸¤ç§ç®—æ³•çš„ç»“æœ
                        cf_results = []
                        ncf_results = []
                        lstm_results = []
                        
                        if st.session_state.trained_cf_recommender is not None:
                            cf_res = st.session_state.trained_cf_recommender.recommend(selected_user, recommendation_count)
                            debug_info.append(f"æ··åˆæ¨è - CFç»“æœé•¿åº¦: {len(cf_res)}")
                            if len(cf_res) > 0:
                                cf_results = [(item_id, score * 0.4) for item_id, score in zip(cf_res.index, cf_res.values)]
                        
                        if st.session_state.trained_ncf_recommender is not None:
                            ncf_res = st.session_state.trained_ncf_recommender.recommend(selected_user, recommendation_count)
                            debug_info.append(f"æ··åˆæ¨è - NCFç»“æœé•¿åº¦: {len(ncf_res)}")
                            if len(ncf_res) > 0:
                                ncf_results = [(item_id, score * 0.6) for item_id, score in ncf_res]
                        
                        if st.session_state.trained_lstm_recommender is not None:
                            lstm_res = st.session_state.trained_lstm_recommender.recommend(selected_user, self.data, recommendation_count)
                            debug_info.append(f"æ··åˆæ¨è - LSTMç»“æœé•¿åº¦: {len(lstm_res)}")
                            if len(lstm_res) > 0:
                                lstm_results = [(item_id, score * 0.2) for item_id, score in lstm_res]
                        
                        # åˆå¹¶å¹¶æ’åº
                        all_results = {}
                        for item_id, score in cf_results + ncf_results + lstm_results:
                            if item_id in all_results:
                                all_results[item_id] += score
                            else:
                                all_results[item_id] = score
                        
                        recommendations = sorted(all_results.items(), key=lambda x: x[1], reverse=True)[:recommendation_count]
                        debug_info.append(f"æ··åˆæ¨èæœ€ç»ˆç»“æœæ•°é‡: {len(recommendations)}")
                    
                    # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                    with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯"):
                        for info in debug_info:
                            st.write(f"- {info}")
                    
                    if recommendations and len(recommendations) > 0:
                        # æ„å»ºæ¨èç»“æœæ•°æ®æ¡†
                        recommendations_df = pd.DataFrame(recommendations, columns=['å•†å“ID', 'æ¨èåˆ†æ•°'])
                        
                        # æ·»åŠ å•†å“é¢å¤–ä¿¡æ¯
                        item_info = []
                        for item_id in recommendations_df['å•†å“ID']:
                            item_data = self.data[self.data['item_id'] == item_id]
                            if len(item_data) > 0:
                                category = item_data['category_id'].iloc[0] if 'category_id' in item_data.columns else 'Unknown'
                                popularity = len(item_data)
                                item_info.append({
                                    'å•†å“ID': item_id,
                                    'ç±»åˆ«': category,
                                    'çƒ­åº¦': popularity
                                })
                        
                        item_info_df = pd.DataFrame(item_info)
                        if len(item_info_df) > 0:
                            recommendations_df = recommendations_df.merge(item_info_df, on='å•†å“ID', how='left')
                        
                        # æ˜¾ç¤ºæ¨èç»“æœ
                        col1, col2 = st.columns([2, 1])
                        
                        with col1:
                            st.subheader("ğŸ† æ¨èå•†å“åˆ—è¡¨")
                            
                            # æ·»åŠ æ’ååˆ—
                            recommendations_df['æ’å'] = range(1, len(recommendations_df) + 1)
                            display_df = recommendations_df[['æ’å', 'å•†å“ID', 'æ¨èåˆ†æ•°', 'ç±»åˆ«', 'çƒ­åº¦']]
                            
                            # æ ¼å¼åŒ–æ¨èåˆ†æ•°
                            display_df['æ¨èåˆ†æ•°'] = display_df['æ¨èåˆ†æ•°'].apply(lambda x: f"{x:.4f}")
                            
                            st.dataframe(display_df, use_container_width=True)
                        
                        with col2:
                            # æ¨èåˆ†æ•°åˆ†å¸ƒ
                            fig = px.bar(
                                recommendations_df.head(10),
                                x='å•†å“ID',
                                y='æ¨èåˆ†æ•°',
                                title="Top 10 æ¨èåˆ†æ•°",
                                color='æ¨èåˆ†æ•°',
                                color_continuous_scale='viridis'
                            )
                            fig.update_layout(xaxis_tickangle=45, height=300)
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # æ¨èå¤šæ ·æ€§åˆ†æ
                        if 'category_id' in self.data.columns and 'ç±»åˆ«' in recommendations_df.columns:
                            st.subheader("ğŸ¨ æ¨èå¤šæ ·æ€§åˆ†æ")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                # æ¨èå“ç±»åˆ†å¸ƒ
                                category_dist = recommendations_df['ç±»åˆ«'].value_counts()
                                fig = px.pie(
                                    values=category_dist.values,
                                    names=category_dist.index,
                                    title="æ¨èå•†å“ç±»åˆ«åˆ†å¸ƒ"
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            
                            with col2:
                                # çƒ­åº¦vsåˆ†æ•°æ•£ç‚¹å›¾
                                if 'çƒ­åº¦' in recommendations_df.columns:
                                    fig = px.scatter(
                                        recommendations_df,
                                        x='çƒ­åº¦',
                                        y='æ¨èåˆ†æ•°',
                                        color='ç±»åˆ«',
                                        title="å•†å“çƒ­åº¦ vs æ¨èåˆ†æ•°",
                                        hover_data=['å•†å“ID']
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                        
                        # æ¨èè§£é‡Š
                        st.subheader("ğŸ’¡ æ¨èè§£é‡Š")
                        
                        explanation_text = f"""
                        **æ¨èç®—æ³•:** {algorithm}
                        
                        **æ¨èä¾æ®:**
                        """
                        
                        if algorithm == "ååŒè¿‡æ»¤":
                            explanation_text += """
                            - åŸºäºä¸æ‚¨ç›¸ä¼¼çš„ç”¨æˆ·çš„è´­ä¹°è¡Œä¸º
                            - åˆ†æç”¨æˆ·ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¨¡å¼
                            - æ¨èç›¸ä¼¼ç”¨æˆ·å–œæ¬¢çš„å•†å“
                            """
                        elif algorithm == "NCFæ·±åº¦å­¦ä¹ ":
                            explanation_text += """
                            - ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ ç”¨æˆ·-å•†å“å¤æ‚å…³ç³»
                            - è€ƒè™‘ç”¨æˆ·å’Œå•†å“çš„é«˜ç»´ç‰¹å¾è¡¨ç¤º
                            - é€šè¿‡éçº¿æ€§å˜æ¢æ•è·æ½œåœ¨åå¥½
                            """
                        elif algorithm == "LSTMåºåˆ—é¢„æµ‹":
                            explanation_text += """
                            - è€ƒè™‘æ—¶é—´åºåˆ—ç‰¹å¾
                            - èƒ½æ•è·ç”¨æˆ·è¡Œä¸ºæ¨¡å¼
                            - é€‚åˆåºåˆ—æ¨è
                            """
                        elif algorithm == "æ··åˆæ¨è":
                            explanation_text += """
                            - ç»“åˆååŒè¿‡æ»¤å’Œæ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿
                            - ååŒè¿‡æ»¤æƒé‡: 40%, æ·±åº¦å­¦ä¹ æƒé‡: 60%
                            - æä¾›æ›´åŠ ç¨³å®šå’Œå‡†ç¡®çš„æ¨èç»“æœ
                            """
                        
                        explanation_text += f"""
                        
                        **ä¸ªæ€§åŒ–ç‰¹å¾:**
                        - ç”¨æˆ·å†å²è¡Œä¸º: {len(user_history)} æ¡è®°å½•
                        - è´­ä¹°è¡Œä¸º: {purchase_count} æ¬¡
                        - æµè§ˆå•†å“: {unique_items} ç§
                        - æ¨èå•†å“å‡ä¸ºç”¨æˆ·æœªæ›¾äº¤äº’è¿‡çš„å•†å“
                        """
                        
                        st.info(explanation_text)
                        
                        # æ¨èæ•ˆæœé¢„æµ‹
                        st.subheader("ğŸ“ˆ æ¨èæ•ˆæœé¢„æµ‹")
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            avg_score = recommendations_df['æ¨èåˆ†æ•°'].mean()
                            st.metric("å¹³å‡æ¨èåˆ†æ•°", f"{avg_score:.4f}")
                        
                        with col2:
                            if 'ç±»åˆ«' in recommendations_df.columns:
                                diversity_score = len(recommendations_df['ç±»åˆ«'].unique()) / len(recommendations_df)
                                st.metric("å¤šæ ·æ€§æŒ‡æ•°", f"{diversity_score:.2f}")
                        
                        with col3:
                            if 'çƒ­åº¦' in recommendations_df.columns:
                                avg_popularity = recommendations_df['çƒ­åº¦'].mean()
                                st.metric("å¹³å‡å•†å“çƒ­åº¦", f"{avg_popularity:.0f}")
                    
                    else:
                        st.error("æ— æ³•ä¸ºè¯¥ç”¨æˆ·ç”Ÿæˆæ¨èï¼Œå¯èƒ½çš„åŸå› ï¼š")
                        st.write("- ç”¨æˆ·æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®")
                        st.write("- æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰è¯¥ç”¨æˆ·")
                        st.write("- æ‰€æœ‰å€™é€‰å•†å“éƒ½å·²è¢«ç”¨æˆ·äº¤äº’è¿‡")
                
                except Exception as e:
                    st.error(f"æ¨èç”Ÿæˆå¤±è´¥: {str(e)}")
                    st.info("å»ºè®®æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹")
    
    def train_models(self):
        """è®­ç»ƒæ¨èæ¨¡å‹"""
        if self.data is None or st.session_state.models_trained:
            return
            
        st.info("ğŸ”„ æ­£åœ¨è®­ç»ƒæ¨èæ¨¡å‹ï¼Œè¯·ç¨å€™...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # ä½¿ç”¨æ•°æ®å­é›†ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
            sample_size = min(50000, len(self.data))
            df_sample = self.data.sample(n=sample_size, random_state=42)
            
            # è®­ç»ƒNCFæ¨¡å‹
            status_text.text("è®­ç»ƒNCFæ·±åº¦å­¦ä¹ æ¨¡å‹...")
            progress_bar.progress(50)
            self.ncf_recommender.fit(df_sample, epochs=3)  # å‡å°‘epochæ•°ä»¥æé«˜é€Ÿåº¦
            # ä¿å­˜åˆ°session_state
            st.session_state.trained_ncf_recommender = self.ncf_recommender
            
            # è®­ç»ƒLSTMæ¨¡å‹
            status_text.text("è®­ç»ƒLSTMåºåˆ—é¢„æµ‹æ¨¡å‹...")
            progress_bar.progress(100)
            self.lstm_recommender.fit(df_sample)
            # ä¿å­˜åˆ°session_state
            st.session_state.trained_lstm_recommender = self.lstm_recommender
            
            progress_bar.progress(100)
            status_text.text("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            # æ›´æ–°è®­ç»ƒçŠ¶æ€
            st.session_state.models_trained = True
            
            st.success("âœ… æ‰€æœ‰æ¨èæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            st.info("å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ•°æ®æ ·æœ¬è¿›è¡Œè®­ç»ƒ")
    
    def run(self):
        """è¿è¡Œä»ªè¡¨æ¿"""
        # æ¸²æŸ“ä¾§è¾¹æ 
        analysis_type = self.render_sidebar()
        
        # æ ¹æ®é€‰æ‹©æ¸²æŸ“ä¸åŒé¡µé¢
        if analysis_type == "æ•°æ®æ¦‚è§ˆ":
            self.render_data_overview()
        elif analysis_type == "ç”¨æˆ·è¡Œä¸ºåˆ†æ":
            self.render_user_behavior_analysis()
        elif analysis_type == "ç”¨æˆ·ç”»åƒåˆ†æ":
            self.render_user_segmentation()
        elif analysis_type == "æ¨èç®—æ³•æ¯”è¾ƒ":
            self.render_algorithm_comparison()
        elif analysis_type == "ä¸ªæ€§åŒ–æ¨è":
            self.render_personalized_recommendation()

def main():
    """ä¸»å‡½æ•°"""
    dashboard = RecommendationDashboard()
    dashboard.run()

if __name__ == "__main__":
    main() 