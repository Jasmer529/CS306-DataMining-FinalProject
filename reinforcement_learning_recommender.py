"""
åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨èç³»ç»Ÿ
ä½¿ç”¨æ·±åº¦Qç½‘ç»œ(DQN)å­¦ä¹ æ¨èç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import random
from collections import deque, namedtuple
import warnings
warnings.filterwarnings('ignore')

# å®šä¹‰ç»éªŒå›æ”¾çš„è½¬æ¢ç»“æ„
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class DQN(nn.Module):
    """æ·±åº¦Qç½‘ç»œ"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(DQN, self).__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # ç½‘ç»œå±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, action_dim)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity=10000):
        self.memory = deque([], maxlen=capacity)
    
    def push(self, *args):
        """ä¿å­˜è½¬æ¢"""
        self.memory.append(Transition(*args))
    
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·"""
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

class RecommendationEnvironment:
    """æ¨èç¯å¢ƒ"""
    
    def __init__(self, data, state_dim=50, max_session_length=20):
        self.data = data
        self.state_dim = state_dim
        self.max_session_length = max_session_length
        
        # åˆ›å»ºç”¨æˆ·å’Œå•†å“çš„æ˜ å°„
        self.users = sorted(data['user_id'].unique())
        self.items = sorted(data['item_id'].unique())
        
        self.user_to_idx = {user: idx for idx, user in enumerate(self.users)}
        self.item_to_idx = {item: idx for idx, item in enumerate(self.items)}
        
        self.num_users = len(self.users)
        self.num_items = len(self.items)
        
        # å‡†å¤‡ç”¨æˆ·å†å²æ•°æ®
        self.user_histories = self._prepare_user_histories()
        
        # è®¡ç®—å•†å“ç‰¹å¾
        self.item_features = self._compute_item_features()
        
        # å½“å‰çŠ¶æ€
        self.current_user = None
        self.current_session = []
        self.session_step = 0
        
        print(f"ğŸ® æ¨èç¯å¢ƒåˆå§‹åŒ–å®Œæˆ:")
        print(f"   ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"   å•†å“æ•°é‡: {self.num_items}")
        print(f"   çŠ¶æ€ç»´åº¦: {self.state_dim}")
    
    def _prepare_user_histories(self):
        """å‡†å¤‡ç”¨æˆ·å†å²è¡Œä¸ºæ•°æ®"""
        user_histories = {}
        
        for user_id in self.users:
            user_data = self.data[self.data['user_id'] == user_id].sort_values('datetime')
            
            # æå–ç”¨æˆ·è¡Œä¸ºåºåˆ—
            history = []
            for _, row in user_data.iterrows():
                item_idx = self.item_to_idx[row['item_id']]
                behavior_type = row['behavior_type']
                
                # å°†è¡Œä¸ºç±»å‹è½¬æ¢ä¸ºæ•°å€¼
                behavior_score = {
                    'pv': 1, 'fav': 2, 'cart': 3, 'buy': 4
                }.get(behavior_type, 1)
                
                history.append((item_idx, behavior_score))
            
            user_histories[user_id] = history
        
        return user_histories
    
    def _compute_item_features(self):
        """è®¡ç®—å•†å“ç‰¹å¾å‘é‡"""
        item_features = {}
        
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        item_stats = self.data.groupby('item_id').agg({
            'user_id': 'nunique',  # ç”¨æˆ·æ•°
            'behavior_type': ['count', lambda x: (x == 'buy').sum()]  # æ€»è¡Œä¸ºæ•°ï¼Œè´­ä¹°æ•°
        }).round(4)
        
        item_stats.columns = ['unique_users', 'total_behaviors', 'total_purchases']
        item_stats['purchase_rate'] = item_stats['total_purchases'] / item_stats['total_behaviors']
        item_stats['popularity'] = item_stats['total_behaviors'] / item_stats['total_behaviors'].max()
        
        # å“ç±»ç‰¹å¾
        if 'category_id' in self.data.columns:
            category_mapping = self.data.groupby('item_id')['category_id'].first()
            item_stats['category'] = category_mapping
        else:
            item_stats['category'] = 0
        
        # å½’ä¸€åŒ–ç‰¹å¾
        for col in ['unique_users', 'total_behaviors', 'total_purchases']:
            max_val = item_stats[col].max()
            if max_val > 0:
                item_stats[col] = item_stats[col] / max_val
        
        # è½¬æ¢ä¸ºç‰¹å¾å‘é‡
        for item_id in self.items:
            if item_id in item_stats.index:
                features = [
                    item_stats.loc[item_id, 'unique_users'],
                    item_stats.loc[item_id, 'total_behaviors'],
                    item_stats.loc[item_id, 'total_purchases'],
                    item_stats.loc[item_id, 'purchase_rate'],
                    item_stats.loc[item_id, 'popularity']
                ]
            else:
                features = [0.0] * 5
            
            # æ‰©å±•åˆ°å›ºå®šç»´åº¦
            while len(features) < 10:
                features.append(0.0)
            
            item_features[self.item_to_idx[item_id]] = features[:10]
        
        return item_features
    
    def reset(self, user_id=None):
        """é‡ç½®ç¯å¢ƒ"""
        if user_id is None:
            self.current_user = random.choice(self.users)
        else:
            self.current_user = user_id
        
        self.current_session = []
        self.session_step = 0
        
        return self._get_state()
    
    def _get_state(self):
        """è·å–å½“å‰çŠ¶æ€å‘é‡"""
        state = np.zeros(self.state_dim)
        
        # ç”¨æˆ·å†å²ç‰¹å¾
        user_history = self.user_histories.get(self.current_user, [])
        
        # æœ€è¿‘äº¤äº’çš„å•†å“ç‰¹å¾ (å‰20ä¸ª)
        recent_items = user_history[-20:] if len(user_history) >= 20 else user_history
        
        idx = 0
        for item_idx, behavior_score in recent_items:
            if idx + 10 < self.state_dim:
                # æ·»åŠ å•†å“ç‰¹å¾
                if item_idx in self.item_features:
                    features = self.item_features[item_idx]
                    state[idx:idx+10] = features
                
                # æ·»åŠ è¡Œä¸ºè¯„åˆ†
                if idx + 10 < self.state_dim:
                    state[idx + 10] = behavior_score / 4.0  # å½’ä¸€åŒ–
                
                idx += 11
            else:
                break
        
        # å½“å‰ä¼šè¯ç‰¹å¾
        if self.current_session:
            session_length = len(self.current_session)
            if self.state_dim > 45:
                state[-5] = min(session_length / self.max_session_length, 1.0)
                
                # æœ€è¿‘æ¨èçš„å•†å“
                last_items = self.current_session[-4:]
                for i, item_idx in enumerate(last_items):
                    if i < 4:
                        state[-4 + i] = item_idx / self.num_items
        
        return state
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # action æ˜¯æ¨èçš„å•†å“ç´¢å¼•
        recommended_item_idx = action
        recommended_item_id = self.items[recommended_item_idx]
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(recommended_item_id)
        
        # æ›´æ–°ä¼šè¯
        self.current_session.append(recommended_item_idx)
        self.session_step += 1
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = (self.session_step >= self.max_session_length)
        
        next_state = self._get_state()
        
        return next_state, reward, done
    
    def _calculate_reward(self, recommended_item_id):
        """è®¡ç®—æ¨èå¥–åŠ±"""
        user_history = self.user_histories.get(self.current_user, [])
        
        # åŸºç¡€å¥–åŠ±ï¼šå•†å“å—æ¬¢è¿ç¨‹åº¦
        item_idx = self.item_to_idx[recommended_item_id]
        base_reward = self.item_features.get(item_idx, [0] * 10)[4]  # æµè¡Œåº¦
        
        # ä¸ªæ€§åŒ–å¥–åŠ±ï¼šç”¨æˆ·æ˜¯å¦äº¤äº’è¿‡è¯¥å•†å“
        user_items = [item for item, _ in user_history]
        if item_idx in user_items:
            # æ‰¾åˆ°ç”¨æˆ·å¯¹è¯¥å•†å“çš„æœ€é«˜è¯„åˆ†
            max_score = max([score for item, score in user_history if item == item_idx])
            personalized_reward = max_score / 4.0  # å½’ä¸€åŒ–åˆ°[0,1]
        else:
            # æ–°å•†å“çš„æ¢ç´¢å¥–åŠ±
            personalized_reward = 0.1
        
        # å¤šæ ·æ€§å¥–åŠ±ï¼šé¿å…é‡å¤æ¨è
        diversity_reward = 0.0
        if item_idx not in self.current_session:
            diversity_reward = 0.2
        
        # æ€»å¥–åŠ±
        total_reward = base_reward * 0.4 + personalized_reward * 0.5 + diversity_reward * 0.1
        
        return total_reward

class RLRecommenderAgent:
    """å¼ºåŒ–å­¦ä¹ æ¨èæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim, action_dim, learning_rate=0.001, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Qç½‘ç»œ
        self.q_network = DQN(state_dim, action_dim).to(self.device)
        self.target_network = DQN(state_dim, action_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(capacity=10000)
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 64
        self.gamma = 0.95  # æŠ˜æ‰£å› å­
        self.target_update_freq = 100  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        self.step_count = 0
        
        print(f"ğŸ¤– å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
    
    def act(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œ"""
        if training and random.random() < self.epsilon:
            # Îµ-è´ªå©ªæ¢ç´¢
            return random.randrange(self.action_dim)
        
        # åˆ©ç”¨ç­–ç•¥
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
            action = q_values.max(1)[1].item()
        
        return action
    
    def remember(self, state, action, next_state, reward):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.push(state, action, next_state, reward)
    
    def replay(self):
        """ç»éªŒå›æ”¾è®­ç»ƒ"""
        if len(self.memory) < self.batch_size:
            return 0.0
        
        # é‡‡æ ·æ‰¹æ¬¡
        transitions = self.memory.sample(self.batch_size)
        batch = Transition(*zip(*transitions))
        
        # è½¬æ¢ä¸ºå¼ é‡
        state_batch = torch.FloatTensor(batch.state).to(self.device)
        action_batch = torch.LongTensor(batch.action).to(self.device)
        next_state_batch = torch.FloatTensor(batch.next_state).to(self.device)
        reward_batch = torch.FloatTensor(batch.reward).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        next_q_values = self.target_network(next_state_batch).max(1)[0].detach()
        target_q_values = reward_batch + (self.gamma * next_q_values)
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ›´æ–°Îµå€¼
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())

class RLRecommenderSystem:
    """å¼ºåŒ–å­¦ä¹ æ¨èç³»ç»Ÿ"""
    
    def __init__(self, state_dim=50, hidden_dim=128, learning_rate=0.001):
        self.state_dim = state_dim
        self.hidden_dim = hidden_dim
        self.learning_rate = learning_rate
        
        self.env = None
        self.agent = None
        self.training_history = []
        
        print("ğŸ¯ å¼ºåŒ–å­¦ä¹ æ¨èç³»ç»Ÿåˆå§‹åŒ–")
    
    def setup(self, data):
        """è®¾ç½®ç¯å¢ƒå’Œæ™ºèƒ½ä½“"""
        print("ğŸ”§ è®¾ç½®æ¨èç¯å¢ƒå’Œæ™ºèƒ½ä½“...")
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = RecommendationEnvironment(data, state_dim=self.state_dim)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = RLRecommenderAgent(
            state_dim=self.state_dim,
            action_dim=self.env.num_items,
            learning_rate=self.learning_rate
        )
        
        print("âœ… ç¯å¢ƒå’Œæ™ºèƒ½ä½“è®¾ç½®å®Œæˆ")
    
    def train(self, episodes=1000, max_steps_per_episode=20):
        """è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“"""
        if self.env is None or self.agent is None:
            print("âŒ è¯·å…ˆè®¾ç½®ç¯å¢ƒå’Œæ™ºèƒ½ä½“")
            return
        
        print(f"\nğŸš€ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ (episodes={episodes})")
        
        episode_rewards = []
        episode_losses = []
        
        for episode in range(episodes):
            # éšæœºé€‰æ‹©ç”¨æˆ·å¼€å§‹æ–°çš„å›åˆ
            state = self.env.reset()
            total_reward = 0.0
            total_loss = 0.0
            step_count = 0
            
            for step in range(max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.act(state, training=True)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done = self.env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self.agent.remember(state, action, next_state, reward)
                
                # è®­ç»ƒæ™ºèƒ½ä½“
                if len(self.agent.memory) > self.agent.batch_size:
                    loss = self.agent.replay()
                    total_loss += loss
                    step_count += 1
                
                total_reward += reward
                state = next_state
                
                if done:
                    break
                
                # æ›´æ–°ç›®æ ‡ç½‘ç»œ
                self.agent.step_count += 1
                if self.agent.step_count % self.agent.target_update_freq == 0:
                    self.agent.update_target_network()
            
            avg_loss = total_loss / max(step_count, 1)
            episode_rewards.append(total_reward)
            episode_losses.append(avg_loss)
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history.append({
                'episode': episode,
                'reward': total_reward,
                'loss': avg_loss,
                'epsilon': self.agent.epsilon
            })
            
            # æ‰“å°è¿›åº¦
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_loss = np.mean(episode_losses[-100:])
                print(f"   Episode {episode+1}/{episodes} - Avg Reward: {avg_reward:.4f}, Avg Loss: {avg_loss:.4f}, Îµ: {self.agent.epsilon:.4f}")
        
        print("âœ… å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ!")
        return episode_rewards, episode_losses
    
    def recommend_for_user(self, user_id, top_k=10):
        """ä¸ºç”¨æˆ·ç”Ÿæˆæ¨è"""
        if self.env is None or self.agent is None:
            print("âŒ è¯·å…ˆè®¾ç½®å¹¶è®­ç»ƒç³»ç»Ÿ")
            return []
        
        if user_id not in self.env.user_to_idx:
            print(f"âŒ ç”¨æˆ· {user_id} ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­")
            return []
        
        # é‡ç½®ç¯å¢ƒåˆ°æŒ‡å®šç”¨æˆ·
        state = self.env.reset(user_id)
        
        recommendations = []
        used_actions = set()
        
        # ç”Ÿæˆæ¨èåºåˆ—
        for _ in range(top_k):
            # è·å–Qå€¼
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
            
            with torch.no_grad():
                q_values = self.agent.q_network(state_tensor).squeeze()
            
            # é€‰æ‹©æœªæ¨èè¿‡çš„æœ€ä¼˜åŠ¨ä½œ
            q_values_np = q_values.cpu().numpy()
            
            # å°†å·²ä½¿ç”¨çš„åŠ¨ä½œçš„Qå€¼è®¾ä¸ºè´Ÿæ— ç©·
            for used_action in used_actions:
                q_values_np[used_action] = float('-inf')
            
            if np.all(np.isinf(q_values_np)):
                break
            
            action = np.argmax(q_values_np)
            item_id = self.env.items[action]
            
            recommendations.append(item_id)
            used_actions.add(action)
            
            # æ›´æ–°çŠ¶æ€
            state, _, _ = self.env.step(action)
        
        return recommendations
    
    def evaluate_recommendations(self, test_users, top_k=10):
        """è¯„ä¼°æ¨èæ€§èƒ½"""
        if not test_users:
            return {}
        
        print(f"\nğŸ“Š è¯„ä¼°å¼ºåŒ–å­¦ä¹ æ¨èæ€§èƒ½ (ç”¨æˆ·æ•°: {len(test_users)})")
        
        total_diversity = 0.0
        total_coverage = set()
        successful_recommendations = 0
        
        for user_id in test_users:
            try:
                recommendations = self.recommend_for_user(user_id, top_k)
                
                if recommendations:
                    # å¤šæ ·æ€§ï¼šæ¨èåˆ—è¡¨ä¸­çš„å”¯ä¸€å•†å“æ•°
                    diversity = len(set(recommendations)) / len(recommendations)
                    total_diversity += diversity
                    
                    # è¦†ç›–ç‡ï¼šæ¨èçš„å”¯ä¸€å•†å“
                    total_coverage.update(recommendations)
                    
                    successful_recommendations += 1
                    
            except Exception as e:
                continue
        
        # è®¡ç®—æŒ‡æ ‡
        results = {}
        if successful_recommendations > 0:
            results['avg_diversity'] = total_diversity / successful_recommendations
            results['coverage'] = len(total_coverage) / self.env.num_items
            results['success_rate'] = successful_recommendations / len(test_users)
        else:
            results['avg_diversity'] = 0.0
            results['coverage'] = 0.0
            results['success_rate'] = 0.0
        
        print(f"   ğŸ“ˆ å¹³å‡å¤šæ ·æ€§: {results['avg_diversity']:.4f}")
        print(f"   ğŸ“Š è¦†ç›–ç‡: {results['coverage']:.4f}")
        print(f"   âœ… æˆåŠŸç‡: {results['success_rate']:.4f}")
        
        return results
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        if self.agent is None:
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¿å­˜")
            return
        
        torch.save({
            'q_network_state_dict': self.agent.q_network.state_dict(),
            'target_network_state_dict': self.agent.target_network.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'training_history': self.training_history,
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.agent.action_dim,
                'learning_rate': self.learning_rate
            }
        }, path)
        
        print(f"ğŸ’¾ å¼ºåŒ–å­¦ä¹ æ¨¡å‹å·²ä¿å­˜è‡³: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.agent.device if self.agent else 'cpu')
        
        if self.agent is None:
            print("âŒ è¯·å…ˆè®¾ç½®ç¯å¢ƒå’Œæ™ºèƒ½ä½“")
            return
        
        self.agent.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.agent.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_history = checkpoint['training_history']
        
        print(f"ğŸ“‚ å¼ºåŒ–å­¦ä¹ æ¨¡å‹å·²ä» {path} åŠ è½½") 